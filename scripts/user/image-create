#!/bin/bash
# Create custom Docker image for DS01
# Interactive wizard for building personalized ML/DL images

set -e

BLUE='\033[0;34m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
CYAN='\033[0;36m'
BOLD='\033[1m'
DIM='\033[2m'
NC='\033[0m'

# Source context library for orchestrator detection
if [[ -f "/opt/ds01-infra/scripts/lib/ds01-context.sh" ]]; then
    source /opt/ds01-infra/scripts/lib/ds01-context.sh
fi

# Source AIME images library (detect_cuda_arch, get_base_image)
if [[ -f "/opt/ds01-infra/scripts/lib/aime-images.sh" ]]; then
    source /opt/ds01-infra/scripts/lib/aime-images.sh
fi

# Source shared Dockerfile generator library
if [[ -f "/opt/ds01-infra/scripts/lib/dockerfile-generator.sh" ]]; then
    source /opt/ds01-infra/scripts/lib/dockerfile-generator.sh
fi

USERNAME=$(whoami)
USER_ID=$(id -u)
GROUP_ID=$(id -g)
DOCKERFILES_DIR="$HOME/dockerfiles"
PROJECT_DOCKERFILE=false

# Requirements.txt import support
REQUIREMENTS_FILE=""      # Path to selected requirements.txt
REQUIREMENTS_MODE=false   # true if using requirements.txt instead of guided phases

# Source username utilities for sanitization
source "/opt/ds01-infra/scripts/lib/username-utils.sh"
SANITIZED_USERNAME=$(sanitize_username_for_slice "$USERNAME")

usage() {
    echo ""
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo -e "${BOLD}Docker Image Creator${NC}"
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo ""
    echo -e "${CYAN}Usage:${NC}"
    echo "  image-create [image-name] [options]"
    echo ""
    echo -e "${CYAN}Options:${NC}"
    echo "  -f, --framework FRAMEWORK   Base framework (pytorch, tensorflow, jax)"
    echo "  -t, --type TYPE            Use case type (cv, nlp, rl, ml, custom)"
    echo "  -p, --packages PACKAGES    Additional Python packages (space-separated)"
    echo "  -s, --system PACKAGES      System packages via apt (space-separated)"
    echo "  -r, --requirements PATH    Import packages from requirements.txt file"
    echo "  --dockerfile PATH          Use existing Dockerfile (skip creation)"
    echo "  --project-dockerfile       Store Dockerfile in project dir (~/workspace/<project>/)"
    echo "  --no-build                 Create Dockerfile only, don't build image"
    echo "  --guided                   Show detailed explanations for beginners"
    echo "  -h, --help                 Show this help message"
    echo ""
    echo -e "${CYAN}Frameworks:${NC}"
    echo "  pytorch      PyTorch 2.5.1 + CUDA 11.8 (default)"
    echo "  tensorflow   TensorFlow 2.14.0 + CUDA 11.8"
    echo "  jax          JAX + CUDA"
    echo ""
    echo -e "${CYAN}Use Case Types:${NC}"
    echo -e "  ml       General ML ${DIM}(xgboost, lightgbm, shap, optuna)${NC}"
    echo -e "  cv       Computer Vision ${DIM}(timm, ultralytics, kornia)${NC}"
    echo -e "  nlp      NLP ${DIM}(transformers, peft, safetensors)${NC}"
    echo -e "  rl       Reinforcement Learning ${DIM}(gymnasium, stable-baselines3)${NC}"
    echo -e "  audio    Audio/Speech ${DIM}(librosa, soundfile, audiomentations)${NC}"
    echo -e "  ts       Time Series ${DIM}(statsmodels, prophet, darts)${NC}"
    echo -e "  llm      LLM/GenAI ${DIM}(vllm, bitsandbytes, langchain)${NC}"
    echo -e "  custom   Specify packages manually"
    echo ""
    echo -e "${CYAN}Examples:${NC}"
    echo "  image-create my-cv-project                    # Interactive mode"
    echo "  image-create thesis-model -f pytorch -t cv    # CV project with PyTorch"
    echo "  image-create nlp-exp -f pytorch -t nlp -p \"wandb optuna\"  # With extra packages"
    echo "  image-create custom-setup -t custom --no-build  # Just create Dockerfile"
    echo "  image-create my-proj -r ~/workspace/my-proj/requirements.txt  # From requirements.txt"
    echo ""
    echo -e "${CYAN}Quick Start:${NC}"
    echo -e "  1. Run: ${GREEN}image-create my-project${NC}"
    echo "  2. Answer prompts to customise"
    echo "  3. Wait a few minutes for build"
    echo -e "  4. Then run: ${GREEN}container-deploy my-project${NC}"
    echo ""
    echo -e "${DIM}Run 'help' or 'commands' anytime to see available commands.${NC}"
    echo ""
}

extended_usage() {
    usage  # Include basic help first
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo -e "${BOLD}All Options${NC}"
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo ""
    echo -e "${BOLD}Base Image Selection:${NC}"
    echo "  --framework <name>     PyTorch, TensorFlow, or JAX"
    echo "  --usecase <name>       ml, cv, nlp, rl, audio, ts, llm, or custom"
    echo "  --skip-base-packages   Don't install use-case packages"
    echo ""
    echo -e "${BOLD}Package Installation:${NC}"
    echo "  --requirements <file>  Install from requirements.txt"
    echo "  --packages <list>      Additional pip packages"
    echo "  --system-packages <list>  Additional apt packages"
    echo ""
    echo -e "${BOLD}Jupyter Configuration:${NC}"
    echo "  --jupyter-extensions <list>  JupyterLab extensions"
    echo "  --skip-jupyter               Don't configure Jupyter"
    echo ""
    echo -e "${BOLD}Build Options:${NC}"
    echo "  --no-cache             Force fresh build"
    echo "  --no-build             Create Dockerfile only"
    echo "  --dockerfile <path>    Use existing Dockerfile"
    echo "  --project-dockerfile   Store Dockerfile in project dir"
    echo ""
    echo -e "${BOLD}More Examples:${NC}"
    echo "  image-create my-project --requirements requirements.txt"
    echo "  image-create nlp-model --framework pytorch --usecase nlp"
    echo "  image-create custom --skip-jupyter --no-build"
    echo ""
}

show_concepts() {
    echo ""
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo -e "${BOLD}Understanding Docker Images${NC}"
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo ""
    echo -e "${BOLD}What is a Docker Image?${NC}"
    echo "  A Docker image is a blueprint for your computing environment."
    echo "  Think of it like a recipe that defines all the software you need."
    echo ""
    echo -e "${BOLD}Key Terms:${NC}"
    echo -e "  â€¢ ${CYAN}Dockerfile${NC} - Recipe (text file with build instructions)"
    echo -e "  â€¢ ${CYAN}Image${NC} - Blueprint (built from Dockerfile, stored by Docker)"
    echo -e "  â€¢ ${CYAN}Container${NC} - Running instance (where you actually work)"
    echo -e "  â€¢ ${CYAN}Workspace${NC} - Your files (~/workspace/<project>/, always safe)"
    echo ""
    echo -e "${BOLD}Important:${NC} Your workspace files are separate from containers."
    echo "  Containers can be deleted and recreated; your files remain."
    echo ""
    echo -e "${CYAN}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${NC}"
    echo ""
    echo -e "${BOLD}Framework Comparison:${NC}"
    echo ""
    echo -e "  ${CYAN}PyTorch${NC} (default, recommended)"
    echo "    Best for: Research, prototyping, NLP, computer vision"
    echo "    Why: Dynamic graphs, Pythonic API, huge community"
    echo ""
    echo -e "  ${CYAN}TensorFlow${NC}"
    echo "    Best for: Production, mobile deployment, TPU training"
    echo "    Why: Mature ecosystem, TensorBoard, Keras integration"
    echo ""
    echo -e "  ${CYAN}JAX${NC}"
    echo "    Best for: High-performance computing, research, TPU"
    echo "    Why: Functional design, XLA compilation, auto-differentiation"
    echo ""
    echo -e "${CYAN}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${NC}"
    echo ""
    echo -e "${BOLD}Use Case Templates:${NC}"
    echo -e "  â€¢ ${CYAN}ml${NC} - General ML (xgboost, lightgbm, optuna)"
    echo -e "  â€¢ ${CYAN}cv${NC} - Computer Vision (opencv, albumentations, timm)"
    echo -e "  â€¢ ${CYAN}nlp${NC} - NLP/LLMs (transformers, datasets, tokenizers)"
    echo -e "  â€¢ ${CYAN}rl${NC} - Reinforcement Learning (gymnasium, stable-baselines3)"
    echo ""
    echo -e "${CYAN}â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€${NC}"
    echo ""
    echo -e "${BOLD}image-create vs image-update:${NC}"
    echo -e "  â€¢ ${GREEN}image-create${NC} - Create a new image from scratch"
    echo -e "  â€¢ ${GREEN}image-update${NC} - Rebuild existing image (after Dockerfile changes)"
    echo ""
    echo -e "${BOLD}Ready to start?${NC}"
    echo -e "  Run: ${GREEN}image-create${NC}            (interactive wizard)"
    echo -e "  Run: ${GREEN}image-create --guided${NC}   (step-by-step with explanations)"
    echo ""
}

show_base_image_packages() {
    # Display key packages from AIME base image
    local base_image="$1"

    echo -e "${CYAN}â”â”â” AIME Base Image Packages â”â”â”${NC}"
    echo ""
    echo -e "${BOLD}Pre-installed in ${CYAN}$base_image:${NC}:"
    # Key packages that are consistent across AIME images
    echo -e "  ${BOLD}Nvidia GPU libs:${NC} CUDA 12.6.3, cuDNN, NCCL, nvidia-* packages (10+ CUDA dependencies)"
    echo -e "  ${BOLD}DL Frameworks:${NC} torch, torchvision, torchaudio (PyTorch ecosystem)"
    echo -e "  ${BOLD}Core Scientific Computing:${NC} numpy & pillow"
    echo -e "  ${BOLD}Utilities:${NC}: conda, ipython, psutil, tqdm"
    echo ""
    if [ "$GUIDED" = true ]; then
        echo -e "${YELLOW}ğŸ’¡ Note:${NC} AIME base images are optimized for ML/DL workloads."
        echo "    They include essential DL frameworks and GPU libraries (133 packages total),"
        echo "    but omit heavier data science packages to keep image size manageable." 
        echo "    You can add additional packages as needed in the next steps."
        echo ""
        echo -e "${CYAN}Currently missing:${NC}"
        echo "    â€¢ Jupyter Lab (for notebooks)"
        echo "    â€¢ pandas, scipy, scikit-learn (data science)"
        echo "    â€¢ matplotlib, seaborn (visualisation)"
        echo "    â€¢ Domain-specific: opencv, transformers, etc."
        echo ""
    fi

    if [ "$GUIDED" = true ]; then
        echo -e "${DIM}To see full package list: docker run --rm $base_image pip list${NC}"
        echo ""
        read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "Press Enter to continue..." </dev/tty
        echo ""
    fi
}

# detect_cuda_arch() and get_base_image() now sourced from /opt/ds01-infra/scripts/lib/aime-images.sh

get_jupyter_packages() {
    # Jupyter and interactive tools
    echo "jupyter jupyterlab ipykernel ipywidgets notebook"
}

get_data_science_packages() {
    # Core data science packages (NOT in AIME base)
    echo "pandas scipy scikit-learn matplotlib seaborn"
}

normalize_package_name() {
    # Normalize package name: lowercase, strip version specifiers
    local pkg="$1"
    # Remove version specifiers (==, >=, <=, ~=, etc.)
    pkg=$(echo "$pkg" | sed 's/[=<>~!].*//')
    # Convert to lowercase
    echo "$pkg" | tr '[:upper:]' '[:lower:]'
}

check_duplicate_packages() {
    # Check if any input packages are already in existing packages
    # Args: $1 = new packages (space-separated), $2... = existing package lists
    local new_packages="$1"
    shift
    local existing_packages="$*"

    # Build array of existing packages (normalized)
    local existing_array=()
    for pkg in $existing_packages; do
        local normalized=$(normalize_package_name "$pkg")
        existing_array+=("$normalized")
    done

    # Check each new package
    local duplicates=()
    for new_pkg in $new_packages; do
        local new_normalized=$(normalize_package_name "$new_pkg")
        for existing in "${existing_array[@]}"; do
            if [ "$new_normalized" = "$existing" ]; then
                duplicates+=("$new_pkg")
                break
            fi
        done
    done

    # Return duplicates (space-separated)
    if [ ${#duplicates[@]} -gt 0 ]; then
        echo "${duplicates[@]}"
        return 1
    fi
    return 0
}

check_user_limits() {
    # Check user's resource limits and provide helpful information
    local limits_script="/opt/ds01-infra/scripts/docker/get_resource_limits.py"

    if [ ! -f "$limits_script" ]; then
        # Silently skip if script not available
        return 0
    fi

    # Get user's limits
    local limit_info=$(python3 "$limits_script" "$USERNAME" 2>/dev/null)

    if [ $? -eq 0 ] && [ -n "$limit_info" ]; then
        # Parse limits from the formatted output
        local group=$(echo "$limit_info" | head -1 | sed -n 's/.*group: \([^)]*\).*/\1/p')
        local max_containers=$(echo "$limit_info" | grep "Max containers:" | awk '{print $NF}')
        local max_gpus=$(echo "$limit_info" | grep "Max GPUs (simultaneous):" | awk '{print $NF}')

        # Count current containers
        local current_containers=$(docker ps -a --filter "label=maintainer=$USERNAME" --format "{{.ID}}" 2>/dev/null | wc -l)

        # Only show if we got valid data
        if [ -n "$group" ] && [ -n "$max_containers" ]; then
            echo ""
            echo -e "${CYAN}â”â”â” Your Resource Limits â”â”â”${NC}"
            echo ""
            echo -e "${BOLD}Group:${NC} $group"
            echo -e "${BOLD}Max Containers:${NC} $max_containers (currently have: $current_containers)"
            echo -e "${BOLD}Max GPUs/MIGs:${NC} $max_gpus"
            echo ""

            # Warning if approaching limits
            if [ "$max_containers" != "unlimited" ] && [ "$current_containers" -ge "$max_containers" ] 2>/dev/null; then
                echo -e "${RED}âš  WARNING: Container Limit Reached${NC}"
                echo ""
                echo "You already have $current_containers containers (limit: $max_containers)."
                echo "You won't be able to create a new container from this image"
                echo "until you stop/remove an existing container."
                echo ""
                echo -e "View containers: ${GREEN}container-list${NC}"
                echo -e "Retire container:  ${GREEN}container-retire <name>${NC}"
                echo ""
                read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
                read -p "Continue creating image anyway? [y/N]: " CONTINUE </dev/tty
                if [[ ! "$CONTINUE" =~ ^[Yy]$ ]]; then
                    echo "Cancelled."
                    exit 0
                fi
            elif [ "$max_containers" != "unlimited" ] && [ "$current_containers" -ge $((max_containers - 1)) ] 2>/dev/null; then
                echo -e "${YELLOW}âš  Note: You're at $current_containers/$max_containers containers.${NC}"
                echo "  You can create this image, but may need to stop a container"
                echo "  before creating a new one from it."
                echo ""
            fi

            # Info about GPU-based images
            if [ "$FRAMEWORK" != "pytorch-cpu" ] && [ "$FRAMEWORK" != "custom" ]; then
                echo -e "${BOLD}GPU Access:${NC}"
                echo "  This image includes CUDA/GPU support."
                if [ "$max_gpus" = "unlimited" ]; then
                    echo "  You have unlimited GPU access."
                else
                    echo "  You can allocate up to $max_gpus GPU(s) across all containers."
                fi
                echo ""
            fi
        fi
    fi
}

get_usecase_packages() {
    local usecase="$1"
    case $usecase in
        cv)
            # timm: pretrained models, albumentations: augmentation, kornia: differentiable CV
            # ultralytics: YOLOv8, opencv: image processing (torchvision already in AIME base)
            echo "timm albumentations opencv-python-headless kornia ultralytics"
            ;;
        nlp)
            # transformers: models, datasets: data loading, tokenizers: fast tokenization
            # accelerate: distributed training, sentencepiece: subword tokenization
            # peft: LoRA fine-tuning, safetensors: fast weights, evaluate: metrics
            echo "transformers datasets tokenizers accelerate sentencepiece peft safetensors evaluate"
            ;;
        rl)
            # gymnasium: environments, stable-baselines3: algorithms
            echo "gymnasium stable-baselines3"
            ;;
        ml)
            # Gradient boosting: xgboost, lightgbm, catboost
            # optuna: hyperparameter tuning, shap: model explainability
            echo "xgboost lightgbm catboost optuna shap"
            ;;
        audio)
            # librosa: audio analysis, soundfile: audio I/O, audiomentations: augmentation
            # (torchaudio already in AIME base)
            echo "librosa soundfile audiomentations"
            ;;
        ts)
            # statsmodels: classical stats, prophet: forecasting, darts: unified forecasting
            echo "statsmodels prophet darts"
            ;;
        llm)
            # vllm: fast LLM inference, bitsandbytes: quantization
            # langchain: LLM pipelines, einops: tensor operations
            echo "vllm bitsandbytes langchain einops"
            ;;
        *)
            echo ""
            ;;
    esac
}

# ============================================================================
# Requirements.txt Import Functions
# ============================================================================

scan_requirements_files() {
    # Scans ~/workspace/*/ and ~/ for requirements.txt files
    # Outputs: "path|display_name" for each entry, one per line

    local workspace_dir="$HOME/workspace"

    # Scan ~/workspace/*/requirements.txt (direct children only)
    if [ -d "$workspace_dir" ]; then
        for project_dir in "$workspace_dir"/*/; do
            if [ -d "$project_dir" ]; then
                local req_file="${project_dir}requirements.txt"
                if [ -f "$req_file" ] && [ -r "$req_file" ]; then
                    local project_name=$(basename "$project_dir")
                    echo "$req_file|$project_name"
                fi
            fi
        done
    fi

    # Check ~/requirements.txt (global)
    local global_req="$HOME/requirements.txt"
    if [ -f "$global_req" ] && [ -r "$global_req" ]; then
        echo "$global_req|Global (home directory)"
    fi
}

validate_requirements_file() {
    # Validates requirements.txt file
    # Args: $1 = file path
    # Returns: 0 on success, 1 on failure
    # Outputs warnings to stderr

    local req_file="$1"

    # Check file exists and is readable
    if [ ! -f "$req_file" ]; then
        echo -e "${RED}Error: File not found: $req_file${NC}" >&2
        return 1
    fi

    if [ ! -r "$req_file" ]; then
        echo -e "${RED}Error: Cannot read file: $req_file${NC}" >&2
        return 1
    fi

    # Check file is not empty (excluding comments and blank lines)
    local line_count
    line_count=$(grep -v '^#' "$req_file" | grep -v '^[[:space:]]*$' | wc -l)
    if [ "$line_count" -eq 0 ]; then
        echo -e "${YELLOW}Warning: requirements.txt appears to be empty${NC}" >&2
    fi

    # Warn about packages already in AIME base
    local aime_base_packages="numpy pillow tqdm conda ipython psutil torch torchvision torchaudio"
    local duplicates=""

    while IFS= read -r line; do
        # Skip comments and empty lines
        [[ "$line" =~ ^[[:space:]]*# ]] && continue
        [[ -z "${line// }" ]] && continue

        # Extract package name (strip version specifiers)
        local pkg_name
        pkg_name=$(echo "$line" | sed 's/[=<>~!].*//' | tr '[:upper:]' '[:lower:]' | tr -d '[:space:]')

        for base_pkg in $aime_base_packages; do
            if [ "$pkg_name" = "$base_pkg" ]; then
                duplicates="$duplicates $pkg_name"
            fi
        done
    done < "$req_file"

    if [ -n "$duplicates" ]; then
        echo "" >&2
        echo -e "${YELLOW}Note: These packages are already in AIME base image:${NC}" >&2
        echo -e "  ${DIM}$duplicates${NC}" >&2
        echo -e "${DIM}They will be reinstalled (safe, may update version)${NC}" >&2
    fi

    return 0
}

select_requirements_file() {
    # Interactive selection of requirements.txt file
    # Sets REQUIREMENTS_FILE global variable
    # Returns: 0 on success, 1 on cancel

    echo ""
    echo -e "${CYAN}â”â”â” Import from requirements.txt â”â”â”${NC}"
    echo ""

    # Scan for files
    local file_paths=()
    local display_names=()

    while IFS='|' read -r path name; do
        [ -z "$path" ] && continue
        file_paths+=("$path")
        display_names+=("$name")
    done < <(scan_requirements_files)

    if [ ${#file_paths[@]} -eq 0 ]; then
        echo -e "${YELLOW}No requirements.txt files found in:${NC}"
        echo "  - ~/workspace/*/"
        echo "  - ~/"
        echo ""
        # Flush any buffered stdin before reading
        read -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "Enter path to requirements.txt (or press Enter to cancel): " MANUAL_PATH </dev/tty

        if [ -z "$MANUAL_PATH" ]; then
            return 1
        fi

        # Expand ~ if present
        MANUAL_PATH="${MANUAL_PATH/#\~/$HOME}"

        if validate_requirements_file "$MANUAL_PATH"; then
            REQUIREMENTS_FILE="$MANUAL_PATH"
            return 0
        else
            return 1
        fi
    fi

    echo -e "${BOLD}Select a requirements.txt file:${NC}"
    echo ""

    local i=1
    for idx in "${!file_paths[@]}"; do
        local path="${file_paths[$idx]}"
        local name="${display_names[$idx]}"
        # Show shortened path for display
        local short_path="${path/#$HOME/~}"
        echo -e "  ${BOLD}$i)${NC} $name (${DIM}$short_path${NC})"
        ((i++))
    done
    echo -e "  ${BOLD}$i)${NC} Enter path manually..."
    echo ""

    local max_choice=$i
    # Flush any buffered stdin before reading
    read -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
    read -p "Choice [1-$max_choice]: " CHOICE </dev/tty

    # Handle manual entry (last option)
    if [ "$CHOICE" -eq "$max_choice" ] 2>/dev/null; then
        echo ""
        read -p "Enter path to requirements.txt: " MANUAL_PATH </dev/tty
        MANUAL_PATH="${MANUAL_PATH/#\~/$HOME}"

        if [ -z "$MANUAL_PATH" ]; then
            return 1
        fi

        if validate_requirements_file "$MANUAL_PATH"; then
            REQUIREMENTS_FILE="$MANUAL_PATH"
            return 0
        else
            return 1
        fi
    fi

    # Validate numeric choice
    if ! [[ "$CHOICE" =~ ^[0-9]+$ ]] || [ "$CHOICE" -lt 1 ] || [ "$CHOICE" -gt "${#file_paths[@]}" ]; then
        echo -e "${RED}Invalid selection${NC}"
        return 1
    fi

    # Get selected file
    local selected_path="${file_paths[$((CHOICE-1))]}"

    if validate_requirements_file "$selected_path"; then
        REQUIREMENTS_FILE="$selected_path"
        return 0
    else
        return 1
    fi
}

read_requirements_packages() {
    # Reads packages from requirements.txt file
    # Args: $1 = file path
    # Outputs: space-separated package list (preserving version specifiers)

    local req_file="$1"
    local packages=""

    while IFS= read -r line; do
        # Skip comments
        [[ "$line" =~ ^[[:space:]]*# ]] && continue
        # Skip empty lines
        [[ -z "${line// }" ]] && continue
        # Skip -r/-e/--requirement/--editable (recursive requirements, editable installs)
        [[ "$line" =~ ^[[:space:]]*- ]] && continue

        # Trim leading/trailing whitespace and normalize version specifiers
        # Handles "torch >= 2.0.1" â†’ "torch>=2.0.1"
        local pkg
        pkg=$(echo "$line" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | \
              sed 's/[[:space:]]*\([><=!~]\+\)[[:space:]]*/\1/g')

        if [ -n "$pkg" ]; then
            packages="$packages $pkg"
        fi
    done < "$req_file"

    # Trim leading space
    echo "${packages# }"
}

# ============================================================================
# End Requirements.txt Import Functions
# ============================================================================

interactive_mode() {
    if [ "$GUIDED" = false ]; then
        echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
        echo -e "${BOLD}Create a Custom Docker Image${NC}"
        echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
        echo ""
    fi

    # Image name
    if [ -z "$IMAGE_NAME" ]; then
        echo -e "${BOLD}Image name${NC} ${DIM}(e.g., my-project, thesis-cv)${NC}"
        echo ""
        read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "> " IMAGE_NAME </dev/tty
        # Sanitize: lowercase, spaces to hyphens, remove invalid chars, strip leading/trailing separators
        IMAGE_NAME=$(echo "$IMAGE_NAME" | tr ' ' '-' | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9._-]//g' | sed 's/^[._-]*//; s/[._-]*$//')
        if [ -z "$IMAGE_NAME" ]; then
            echo -e "${RED}Error: Image name cannot be empty or only contain special characters${NC}"
            exit 1
        fi
    fi
    echo ""
    # New naming convention: ds01-{user-id}/{project-name}:latest
    FULL_IMAGE_NAME="ds01-${USER_ID}/${IMAGE_NAME}"

    # Check if DS01 custom image exists (only check ds01-{uid}/{name}:latest format)
    # Note: We don't check AIME base image tags (aimehub/...:name._.uid) because those are
    # just tracking tags on base images, not user-created custom images
    EXISTING_DS01=$(docker images --format "{{.Repository}}:{{.Tag}}" | grep "^${FULL_IMAGE_NAME}:latest$" || true)

    if [ -n "$EXISTING_DS01" ]; then
        echo -e "${YELLOW}âš  Image with name '${IMAGE_NAME}' already exists${NC}"
        echo -e "  ${DIM}DS01 image: ${EXISTING_DS01}${NC}"
        echo ""
        read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "Overwrite? [y/N]: " OVERWRITE </dev/tty
        echo ""
        if [[ ! "$OVERWRITE" =~ ^[Yy]$ ]]; then
            echo "Cancelled."
            exit 0
        fi
    fi

    echo -e "${BOLD}Image namespace: ${CYAN}ds01-${USER_ID}${NC}"
    echo -e "${BOLD}Image name: ${CYAN}${IMAGE_NAME}${NC}"
    echo -e "${DIM}Full reference: ds01-${USER_ID}/${IMAGE_NAME}:latest${NC}"
    if [ "$GUIDED" = true ]; then
        echo -e "${DIM}Format: {namespace}/{name}:{tag} where namespace = ds01-{user-id}${NC}"
    fi
    echo ""

    # Framework
    if [ -z "$FRAMEWORK" ]; then
        if [ "$GUIDED" = true ]; then
            echo -e "${BOLD}Choosing a Framework${NC}"
            echo ""
            echo "A framework is the foundation for your ML/DL work:"
            echo ""
            echo -e "â€¢ ${CYAN}PyTorch${NC} - Most popular, great community, PyTorch + torchvision"
            echo "  Used by: Most research, computer vision, NLP"
            echo ""
            echo -e "â€¢ ${CYAN}TensorFlow${NC} - Google's framework, production-ready"
            echo "  Used by: Industry applications, deployment"
            echo ""
            echo -e "â€¢ ${CYAN}JAX${NC} - High-performance, functional approach"
            echo "  Used by: Research, custom algorithms"
            echo ""
            echo -e "â€¢ ${CYAN}PyTorch CPU${NC} - No GPU, for testing or CPU-only work"
            echo ""
            echo -e "â€¢ ${CYAN}Custom${NC} - Specify your own base image (advanced)"
            echo "  Use any Docker image as base (e.g., ubuntu:22.04, python:3.11)"
            echo ""
            echo -e "${YELLOW}ğŸ’¡ Recommendation:${NC} Choose PyTorch unless you have specific needs"
            echo ""
        fi

        echo -e "${BOLD}Select base framework:${NC}"
        echo -e "  ${BOLD}1)${NC} PyTorch (latest from AIME catalog) ${GREEN}(recommended)${NC}"
        echo -e "  ${BOLD}2)${NC} TensorFlow (latest from AIME catalog)"
        echo -e "  ${BOLD}3)${NC} JAX + CUDA (if available)"
        echo -e "  ${BOLD}4)${NC} PyTorch CPU-only"
        echo -e "  ${BOLD}5)${NC} Custom (specify base image)"
        echo -e ""
        read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "Choice [1-5, default: 1]: " FW_CHOICE </dev/tty

        case $FW_CHOICE in
            2) FRAMEWORK="tensorflow" ;;
            3) FRAMEWORK="jax" ;;
            4) FRAMEWORK="pytorch-cpu" ;;
            5)
                FRAMEWORK="custom"
                echo ""
                echo -e "${BOLD}Custom Base Image${NC}"
                echo "Enter Docker image (e.g., ubuntu:22.04, python:3.11-slim, nvidia/cuda:12.0-runtime)"
                read -p "> " CUSTOM_BASE_IMAGE </dev/tty
                if [ -z "$CUSTOM_BASE_IMAGE" ]; then
                    echo -e "${RED}Error: Base image cannot be empty${NC}"
                    exit 1
                fi
                echo -e "${YELLOW}âš  Custom base: No default packages will be installed${NC}"
                SKIP_BASE_PACKAGES=true
                ;;
            *) FRAMEWORK="pytorch" ;;
        esac

        # Get actual base image from AIME catalog
        if [ "$FRAMEWORK" != "custom" ]; then
            SELECTED_BASE_IMAGE=$(get_base_image "$FRAMEWORK" "$CUSTOM_BASE_IMAGE")
            echo ""
            echo -e "${GREEN}âœ“${NC} Selected base image: ${CYAN}$SELECTED_BASE_IMAGE${NC}"
            echo ""

            # Show what's already in the AIME base image
            if [ "$GUIDED" = true ] || [ "$SKIP_BASE_PACKAGES" != true ]; then
                show_base_image_packages "$SELECTED_BASE_IMAGE"
            fi
        fi
    fi

    # === Package Source Selection (after Phase 1, before Phase 2) ===
    if [ "$SKIP_BASE_PACKAGES" != true ] && [ "$REQUIREMENTS_MODE" != true ]; then
        echo -e "${CYAN}â”â”â” Package Installation Method â”â”â”${NC}"
        echo ""

        if [ "$GUIDED" = true ]; then
            echo "You can install Python packages in two ways:"
            echo ""
            echo -e "  ${CYAN}1) Import requirements.txt${NC} - Use existing file"
            echo "     Best for: Reproducing existing environments"
            echo ""
            echo -e "  ${CYAN}2) Guided Selection${NC} - Choose from curated package sets"
            echo "     Best for: New users, exploring options"
            echo ""
        fi

        echo -e "${BOLD}How would you like to specify packages?${NC}"
        echo -e "  ${BOLD}1)${NC} Import from requirements.txt file ${GREEN}(default)${NC}"
        echo -e "  ${BOLD}2)${NC} Guided selection (Jupyter, Data Science, Use Case)"
        echo ""
        read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "Choice [1-2, default: 1]: " PKG_SOURCE_CHOICE </dev/tty

        case $PKG_SOURCE_CHOICE in
            2)
                REQUIREMENTS_MODE=false
                ;;
            *)
                if select_requirements_file; then
                    REQUIREMENTS_MODE=true
                    # Don't skip Jupyter/DS phases - user can still add those
                    # Only skip use-case since requirements.txt handles domain packages
                    USECASE="custom"
                    CUSTOM_USECASE_PACKAGES=""
                    echo ""
                    local short_req="${REQUIREMENTS_FILE/#$HOME/~}"
                    echo -e "${GREEN}âœ“${NC} Will install packages from: ${CYAN}$short_req${NC}"
                    echo -e "${DIM}You can still add Jupyter and Data Science packages below${NC}"
                else
                    echo -e "${YELLOW}Falling back to guided selection${NC}"
                    REQUIREMENTS_MODE=false
                fi
                ;;
        esac
        echo ""
    fi
    # === End Package Source Selection ===

    # Phase 2: Core Python & Jupyter
    if [ -z "$JUPYTER_CHOICE" ] && [ "$SKIP_BASE_PACKAGES" != true ]; then
        if [ "$GUIDED" = true ]; then
            echo ""
            echo -e "${BOLD}Phase 2: Core Python & Interactive (Jupyter)${NC}"
            echo ""
            echo "These enable notebook-based development:"
            echo ""
            echo -e "  ${CYAN}Default packages:${NC}"
            echo "  â€¢ jupyter - Jupyter ecosystem"
            echo "  â€¢ jupyterlab - Web-based IDE"
            echo "  â€¢ ipykernel - Python kernel for notebooks"
            echo "  â€¢ ipywidgets - Interactive widgets"
            echo ""
            echo -e "${YELLOW}ğŸ’¡ Package Versions:${NC}"
            echo "  â€¢ Without version: installs latest (e.g., 'pandas')"
            echo "  â€¢ With version: installs specific (e.g., 'pandas==1.5.3', 'transformers>=4.30.0')"
            echo ""
            echo -e "${YELLOW}ğŸ’¡ Recommendation:${NC} Install if you use notebooks"
            echo ""
            read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
            read -p "Press Enter to continue..." </dev/tty
            echo ""
        fi

        if [ "$GUIDED" = false ]; then
            echo -e "${CYAN}â”â”â”â” Choose Defaults â”â”â”â”${NC}"
            echo ""
        fi

        echo -e "${BOLD}Install Jupyter Lab & interactive tools?${NC}"
        echo -e "  ${BOLD}1)${NC} Yes - Install defaults (${GREEN}recommended${NC})"
        echo -e "  ${BOLD}2)${NC} No - Skip (use terminal/IDE only)"
        echo -e "  ${BOLD}3)${NC} Custom - Specify packages manually"
        echo ""
        read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "Choice [1-3, default: 1]: " JUPYTER_PKG_CHOICE </dev/tty

        case $JUPYTER_PKG_CHOICE in
            2)
                JUPYTER_CHOICE="skip"
                echo -e "${YELLOW}Skipped Jupyter packages${NC}"
                ;;
            3)
                JUPYTER_CHOICE="custom"
                # Loop until no duplicates
                while true; do
                    echo ""
                    echo -e "${BOLD}Specify Jupyter packages:${NC} (space-separated)"
                    echo -e "${DIM}Examples: jupyterlab ipywidgets ipykernel==6.25.0${NC}"
                    read -p "> " CUSTOM_JUPYTER_PACKAGES </dev/tty

                    if [ -z "$CUSTOM_JUPYTER_PACKAGES" ]; then
                        echo -e "${YELLOW}No packages entered${NC}"
                        break
                    fi

                    # Check for duplicates (against AIME base packages)
                    local aime_base="numpy pillow tqdm conda ipython psutil torch torchvision torchaudio"
                    local duplicates=$(check_duplicate_packages "$CUSTOM_JUPYTER_PACKAGES" "$aime_base")

                    if [ $? -eq 1 ]; then
                        echo ""
                        echo -e "${YELLOW}âš   Warning: These packages are already in AIME base:${NC}"
                        echo -e "   ${YELLOW}$duplicates${NC}"
                        echo ""
                        echo "Please enter different packages (or press Enter to go back):"
                    else
                        break
                    fi
                done
                ;;
            *)
                JUPYTER_CHOICE="default"
                ;;
        esac
        echo ""
    fi

    # Phase 3: Core Data Science
    if [ -z "$DATA_SCIENCE_CHOICE" ] && [ "$SKIP_BASE_PACKAGES" != true ]; then
        if [ "$GUIDED" = true ]; then
            echo ""
            echo -e "${BOLD}Phase 3: Core Data Science${NC}"
            echo ""
            echo "Essential libraries for data analysis:"
            echo ""
            echo -e "  ${CYAN}Default packages:${NC}"
            echo "  â€¢ pandas - DataFrames & data manipulation"
            echo "  â€¢ scipy - Scientific computing"
            echo "  â€¢ scikit-learn - Traditional ML algorithms"
            echo "  â€¢ matplotlib, seaborn - Visualisation"
            echo ""
            echo -e "${YELLOW}âš  Note:${NC} These are NOT in AIME base (only numpy included)"
            echo -e "${YELLOW}ğŸ’¡ Recommendation:${NC} Install defaults (needed for most DS work)"
            echo ""
            read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
            read -p "Press Enter to continue..." </dev/tty
            echo ""
        fi
        echo -e "${BOLD}Install core data science packages?${NC}"
        echo -e "  ${BOLD}1)${NC} Yes - Install defaults (${GREEN}recommended${NC})"
        echo -e "  ${BOLD}2)${NC} No - Skip (framework-only setup)"
        echo -e "  ${BOLD}3)${NC} Custom - Specify packages manually"
        echo ""
        read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "Choice [1-3, default: 1]: " DS_PKG_CHOICE </dev/tty

        case $DS_PKG_CHOICE in
            2)
                DATA_SCIENCE_CHOICE="skip"
                echo -e "${YELLOW}Skipped data science packages${NC}"
                ;;
            3)
                DATA_SCIENCE_CHOICE="custom"
                # Loop until no duplicates
                while true; do
                    echo ""
                    echo -e "${BOLD}Specify data science packages:${NC} (space-separated)"
                    read -p "> " CUSTOM_DS_PACKAGES </dev/tty

                    if [ -z "$CUSTOM_DS_PACKAGES" ]; then
                        echo -e "${YELLOW}No packages entered${NC}"
                        break
                    fi

                    # Check for duplicates (against AIME base + Jupyter packages)
                    local aime_base="numpy pillow tqdm conda ipython psutil torch torchvision torchaudio"
                    local jupyter_pkgs=""
                    if [ "$JUPYTER_CHOICE" = "default" ]; then
                        jupyter_pkgs=$(get_jupyter_packages)
                    elif [ "$JUPYTER_CHOICE" = "custom" ]; then
                        jupyter_pkgs="$CUSTOM_JUPYTER_PACKAGES"
                    fi

                    local duplicates=$(check_duplicate_packages "$CUSTOM_DS_PACKAGES" "$aime_base" "$jupyter_pkgs")

                    if [ $? -eq 1 ]; then
                        echo ""
                        echo -e "${YELLOW}âš   Warning: These packages are already included:${NC}"
                        echo -e "   ${YELLOW}$duplicates${NC}"
                        echo ""
                        echo "Please enter different packages (or press Enter to go back):"
                    else
                        break
                    fi
                done
                ;;
            *)
                DATA_SCIENCE_CHOICE="default"
                ;;
        esac
        echo ""
    fi

    # Phase 4: Use-Case Specific
    if [ -z "$USECASE" ] && [ "$SKIP_BASE_PACKAGES" != true ]; then
        if [ "$GUIDED" = true ]; then
            echo ""
            echo -e "${BOLD}Phase 4: Use-Case Specific Packages${NC}"
            echo ""
            echo "This determines which specialised packages get installed:"
            echo ""
            echo -e "â€¢ ${CYAN}General ML${NC} - Gradient boosting & explainability"
            echo "  Packages: xgboost, lightgbm, catboost, optuna, shap"
            echo "  Example: Classification, regression, hyperparameter tuning"
            echo ""
            echo -e "â€¢ ${CYAN}Computer Vision${NC} - Images, videos, object detection"
            echo "  Packages: timm, albumentations, opencv, kornia, ultralytics"
            echo "  Example: Image classification, YOLOv8, segmentation"
            echo ""
            echo -e "â€¢ ${CYAN}NLP${NC} - Text, language models, transformers"
            echo "  Packages: transformers, datasets, peft, safetensors, evaluate"
            echo "  Example: Text classification, LoRA fine-tuning, chatbots"
            echo ""
            echo -e "â€¢ ${CYAN}Reinforcement Learning${NC} - Agents, environments"
            echo "  Packages: gymnasium, stable-baselines3"
            echo "  Example: Game AI, robotics, optimization"
            echo ""
            echo -e "â€¢ ${CYAN}Audio/Speech${NC} - Audio processing & analysis"
            echo "  Packages: librosa, soundfile, audiomentations"
            echo "  Example: Speech recognition, music analysis, audio classification"
            echo ""
            echo -e "â€¢ ${CYAN}Time Series${NC} - Forecasting & temporal data"
            echo "  Packages: statsmodels, prophet, darts"
            echo "  Example: Demand forecasting, stock prediction, anomaly detection"
            echo ""
            echo -e "â€¢ ${CYAN}LLM/GenAI${NC} - Large language model inference & fine-tuning"
            echo "  Packages: vllm, bitsandbytes, langchain, einops"
            echo "  Example: LLM serving, quantization, RAG pipelines"
            echo ""
            echo -e "â€¢ ${CYAN}Custom${NC} - Skip use case packages or specify manually"
            echo ""
            read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
            read -p "Press Enter to continue..." </dev/tty
            echo ""
        fi

        echo -e "${BOLD}Select use case (additional domain-specific packages):${NC}"
        echo -e "  ${BOLD}1)${NC} General ML ${DIM}(xgboost, lightgbm, shap, optuna)${NC} (${GREEN}default${NC})"
        echo -e "  ${BOLD}2)${NC} Computer Vision ${DIM}(timm, ultralytics, kornia)${NC}"
        echo -e "  ${BOLD}3)${NC} NLP ${DIM}(transformers, peft, safetensors)${NC}"
        echo -e "  ${BOLD}4)${NC} Reinforcement Learning ${DIM}(gymnasium, stable-baselines3)${NC}"
        echo -e "  ${BOLD}5)${NC} Audio/Speech ${DIM}(librosa, soundfile, audiomentations)${NC}"
        echo -e "  ${BOLD}6)${NC} Time Series ${DIM}(statsmodels, prophet, darts)${NC}"
        echo -e "  ${BOLD}7)${NC} LLM/GenAI ${DIM}(vllm, bitsandbytes, langchain)${NC}"
        echo -e "  ${BOLD}8)${NC} Custom ${DIM}(specify packages manually)${NC}"
        echo ""
        read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
        read -p "Choice [1-8, default: 1]: " UC_CHOICE </dev/tty

        case $UC_CHOICE in
            2) USECASE="cv" ;;
            3) USECASE="nlp" ;;
            4) USECASE="rl" ;;
            5) USECASE="audio" ;;
            6) USECASE="ts" ;;
            7) USECASE="llm" ;;
            8)
                USECASE="custom"
                echo ""
                echo -e "${BOLD}Custom Use Case Packages${NC}"
                echo "Specify packages (space-separated), or press Enter to skip:"
                read -p "> " CUSTOM_USECASE_PACKAGES </dev/tty
                ;;
            1|"") USECASE="ml" ;;
            *)
                echo "Invalid choice. Setting to default: General ML."
                USECASE="ml"
                ;;
        esac
    fi

    # Display included packages before asking for additional
    if [ -n "$USECASE" ] && [ "$SKIP_BASE_PACKAGES" != "true" ]; then
        echo ""
        echo -e "${CYAN}â”â”â” Packages To Be Installed â”â”â”${NC}"
        echo ""

        # Jupyter packages
        if [ "$JUPYTER_CHOICE" = "default" ]; then
            echo -e "${BOLD}Jupyter & Interactive:${NC}"
            echo "  jupyter, jupyterlab, ipykernel, ipywidgets, notebook"
            echo ""
        elif [ "$JUPYTER_CHOICE" = "custom" ]; then
            echo -e "${BOLD}Jupyter (custom):${NC}"
            echo "  $CUSTOM_JUPYTER_PACKAGES"
            echo ""
        elif [ "$JUPYTER_CHOICE" = "skip" ]; then
            echo -e "${BOLD}Jupyter:${NC} Skipped"
            echo ""
        fi

        # Data Science packages
        if [ "$DATA_SCIENCE_CHOICE" = "default" ]; then
            echo -e "${BOLD}Data Science:${NC}"
            echo "  pandas, scipy, scikit-learn, matplotlib, seaborn"
            echo ""
        elif [ "$DATA_SCIENCE_CHOICE" = "custom" ]; then
            echo -e "${BOLD}Data Science (custom):${NC}"
            echo "  $CUSTOM_DS_PACKAGES"
            echo ""
        elif [ "$DATA_SCIENCE_CHOICE" = "skip" ]; then
            echo -e "${BOLD}Data Science:${NC} Skipped"
            echo ""
        fi

        # Use case packages
        if [ "$USECASE" != "custom" ] && [ -n "$USECASE" ]; then
            local uc_pkgs=$(get_usecase_packages "$USECASE")
            if [ -n "$uc_pkgs" ]; then
                echo -e "${BOLD}Use Case Packages ($USECASE):${NC}"
                echo "  $uc_pkgs"
                echo ""
            fi
        elif [ "$USECASE" = "custom" ] && [ -n "$CUSTOM_USECASE_PACKAGES" ]; then
            echo -e "${BOLD}Use Case Packages (custom):${NC}"
            echo "  $CUSTOM_USECASE_PACKAGES"
            echo ""
        fi
    fi

    # Additional packages
    if [ -z "$ADDITIONAL_PACKAGES" ]; then
        while true; do
            echo ""
            echo -e "${CYAN}â”â”â” Add Custom-specified packages? â”â”â”${NC}"
            echo ""
            echo -e "${BOLD}Space-separated${NC} (or press Enter to skip)"
            echo -e "${DIM}Examples: wandb pytorch-lightning==1.5.3${NC}"
            read -p "> " ADDITIONAL_PACKAGES </dev/tty

            if [ -z "$ADDITIONAL_PACKAGES" ]; then
                break
            fi

            # Check for duplicates (against ALL previous selections)
            local aime_base="numpy pillow tqdm conda ipython psutil torch torchvision torchaudio"
            local jupyter_pkgs=""
            local ds_pkgs=""
            local usecase_pkgs=""

            if [ "$JUPYTER_CHOICE" = "default" ]; then
                jupyter_pkgs=$(get_jupyter_packages)
            elif [ "$JUPYTER_CHOICE" = "custom" ]; then
                jupyter_pkgs="$CUSTOM_JUPYTER_PACKAGES"
            fi

            if [ "$DATA_SCIENCE_CHOICE" = "default" ]; then
                ds_pkgs=$(get_data_science_packages)
            elif [ "$DATA_SCIENCE_CHOICE" = "custom" ]; then
                ds_pkgs="$CUSTOM_DS_PACKAGES"
            fi

            if [ -n "$USECASE" ] && [ "$USECASE" != "custom" ]; then
                usecase_pkgs=$(get_usecase_packages "$USECASE")
            elif [ "$USECASE" = "custom" ] && [ -n "$CUSTOM_USECASE_PACKAGES" ]; then
                usecase_pkgs="$CUSTOM_USECASE_PACKAGES"
            fi

            local duplicates=$(check_duplicate_packages "$ADDITIONAL_PACKAGES" "$aime_base" "$jupyter_pkgs" "$ds_pkgs" "$usecase_pkgs")

            if [ $? -eq 1 ]; then
                echo ""
                echo -e "${YELLOW}âš   Warning: These packages are already included:${NC}"
                echo -e "   ${YELLOW}$duplicates${NC}"
                echo ""
                echo "Please enter different packages (or press Enter to skip):"
            else
                break
            fi
        done
    fi

    # System packages
    if [ -z "$SYSTEM_PACKAGES" ]; then
        echo -e "\n${BOLD}System packages (apt)?${NC} (or press Enter to skip)"
        echo -e "${DIM}Examples: htop tmux vim git-lfs${NC}"
        read -p "> " SYSTEM_PACKAGES </dev/tty
        echo ""
    fi
}

create_dockerfile() {
    local image_name="$1"
    local framework="$2"
    local usecase="$3"
    local additional="$4"
    local system_pkgs="$5"
    local custom_base="$6"
    local jupyter_choice="$7"          # NEW: jupyter/interactive packages
    local custom_jupyter_pkgs="$8"     # NEW: custom jupyter packages
    local data_science_choice="$9"     # NEW: data science packages
    local custom_ds_pkgs="${10}"       # NEW: custom DS packages
    local custom_usecase_pkgs="${11}"
    local skip_base="${12}"
    local requirements_file="${13}"     # Path to requirements.txt (optional)

    local base_image=$(get_base_image "$framework" "$custom_base")
    local usecase_packages=$(get_usecase_packages "$usecase")

    # Handle custom use case packages
    if [ "$usecase" = "custom" ] && [ -n "$custom_usecase_pkgs" ]; then
        usecase_packages="$custom_usecase_pkgs"
    elif [ "$usecase" = "custom" ]; then
        usecase_packages=""
    fi

    # Extract project name from full image name (ds01-1001/test-project -> test-project)
    local project_name="${image_name##*/}"  # Remove everything before the last /

    # Determine Dockerfile location - prefer project workspace
    local dockerfile_dir=""
    local dockerfile=""
    local workspace_dir="$HOME/workspace/${project_name}"

    if [ -n "$PROJECT_DIR" ]; then
        # Explicit project dir specified
        dockerfile_dir="$PROJECT_DIR"
        dockerfile="$dockerfile_dir/Dockerfile"
    elif [ -d "$workspace_dir" ]; then
        # Project workspace exists - use it
        dockerfile_dir="$workspace_dir"
        dockerfile="$dockerfile_dir/Dockerfile"
    else
        # Create new project workspace (consistent with project-init)
        dockerfile_dir="$workspace_dir"
        dockerfile="$dockerfile_dir/Dockerfile"
    fi

    mkdir -p "$dockerfile_dir"

    cat > "$dockerfile" << DOCKERFILEEOF
# DS01 Custom Image: $project_name
# Created: $(date)
# Framework: $framework
# Use case: $usecase
# Author: $USERNAME

FROM $base_image

# DS01 metadata labels
LABEL maintainer="$USERNAME"
LABEL maintainer.id="$USER_ID"
LABEL ds01.project="$project_name"
LABEL ds01.framework="$framework"
LABEL ds01.created="$(date -Iseconds)"
LABEL ds01.managed="true"

# Build arguments (set automatically by DS01)
ARG DS01_USER_ID=${USER_ID}
ARG DS01_GROUP_ID=${GROUP_ID}
ARG DS01_USERNAME=${USERNAME}

WORKDIR /workspace

DOCKERFILEEOF

    # Only add system packages and Python packages if not using fully custom base
    if [ "$skip_base" != true ]; then
        cat >> "$dockerfile" << 'DOCKERFILEEOF'
# System packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    curl \
    wget \
    vim \
    htop \
DOCKERFILEEOF

        # Add user's system packages if provided
        if [ -n "$system_pkgs" ]; then
            # Split packages and add each on its own line
            for pkg in $system_pkgs; do
                echo "    $pkg \\" >> "$dockerfile"
            done
        fi

        cat >> "$dockerfile" << 'DOCKERFILEEOF'
    && rm -rf /var/lib/apt/lists/*

DOCKERFILEEOF

        # === Requirements.txt installation (if provided) ===
        if [ -n "$requirements_file" ] && [ -f "$requirements_file" ]; then
            local req_packages
            req_packages=$(read_requirements_packages "$requirements_file")

            if [ -n "$req_packages" ]; then
                local short_req="${requirements_file/#$HOME/~}"
                echo "# Packages from requirements.txt" >> "$dockerfile"
                echo "# Source: $short_req" >> "$dockerfile"
                echo "RUN pip install --no-cache-dir \\" >> "$dockerfile"

                local pkg_array=($req_packages)
                local pkg_count=${#pkg_array[@]}
                local i=0
                for pkg in "${pkg_array[@]}"; do
                    ((i++))
                    if [ $i -lt $pkg_count ]; then
                        echo "    $pkg \\" >> "$dockerfile"
                    else
                        echo "    $pkg" >> "$dockerfile"
                    fi
                done
                echo "" >> "$dockerfile"
            fi
        fi
        # === End Requirements.txt installation ===

        # Phase 2: Jupyter & Interactive packages (only if not using requirements.txt)
        if [ -z "$requirements_file" ]; then
        if [ "$jupyter_choice" = "skip" ]; then
            echo "# Jupyter packages skipped by user" >> "$dockerfile"
            echo "" >> "$dockerfile"
        elif [ "$jupyter_choice" = "custom" ] && [ -n "$custom_jupyter_pkgs" ]; then
            echo "# Jupyter & Interactive (Custom)" >> "$dockerfile"
            echo "RUN pip install --no-cache-dir \\" >> "$dockerfile"
            local pkg_array=($custom_jupyter_pkgs)
            local pkg_count=${#pkg_array[@]}
            local i=0
            for pkg in "${pkg_array[@]}"; do
                ((i++))
                if [ $i -lt $pkg_count ]; then
                    echo "    $pkg \\" >> "$dockerfile"
                else
                    echo "    $pkg" >> "$dockerfile"
                fi
            done
            echo "" >> "$dockerfile"
        elif [ "$jupyter_choice" = "default" ]; then
            local jupyter_packages=$(get_jupyter_packages)
            echo "# Jupyter & Interactive" >> "$dockerfile"
            echo "RUN pip install --no-cache-dir \\" >> "$dockerfile"
            local pkg_array=($jupyter_packages)
            local pkg_count=${#pkg_array[@]}
            local i=0
            for pkg in "${pkg_array[@]}"; do
                ((i++))
                if [ $i -lt $pkg_count ]; then
                    echo "    $pkg \\" >> "$dockerfile"
                else
                    echo "    $pkg" >> "$dockerfile"
                fi
            done
            echo "" >> "$dockerfile"
        fi

        # Phase 3: Data Science packages
        if [ "$data_science_choice" = "skip" ]; then
            echo "# Data science packages skipped by user" >> "$dockerfile"
            echo "" >> "$dockerfile"
        elif [ "$data_science_choice" = "custom" ] && [ -n "$custom_ds_pkgs" ]; then
            echo "# Core Data Science (Custom)" >> "$dockerfile"
            echo "RUN pip install --no-cache-dir \\" >> "$dockerfile"
            local pkg_array=($custom_ds_pkgs)
            local pkg_count=${#pkg_array[@]}
            local i=0
            for pkg in "${pkg_array[@]}"; do
                ((i++))
                if [ $i -lt $pkg_count ]; then
                    echo "    $pkg \\" >> "$dockerfile"
                else
                    echo "    $pkg" >> "$dockerfile"
                fi
            done
            echo "" >> "$dockerfile"
        elif [ "$data_science_choice" = "default" ]; then
            local ds_packages=$(get_data_science_packages)
            echo "# Core Data Science" >> "$dockerfile"
            echo "RUN pip install --no-cache-dir \\" >> "$dockerfile"
            local pkg_array=($ds_packages)
            local pkg_count=${#pkg_array[@]}
            local i=0
            for pkg in "${pkg_array[@]}"; do
                ((i++))
                if [ $i -lt $pkg_count ]; then
                    echo "    $pkg \\" >> "$dockerfile"
                else
                    echo "    $pkg" >> "$dockerfile"
                fi
            done
            echo "" >> "$dockerfile"
        fi

        # Use case specific packages
        if [ -n "$usecase_packages" ]; then
            echo "# Use case specific packages" >> "$dockerfile"
            echo "RUN pip install --no-cache-dir \\" >> "$dockerfile"
            local pkg_array=($usecase_packages)
            local pkg_count=${#pkg_array[@]}
            local i=0
            for pkg in "${pkg_array[@]}"; do
                ((i++))
                if [ $i -lt $pkg_count ]; then
                    echo "    $pkg \\" >> "$dockerfile"
                else
                    echo "    $pkg" >> "$dockerfile"
                fi
            done
            echo "" >> "$dockerfile"
        fi
        fi  # End: if [ -z "$requirements_file" ] - Phases 2-4

        # Additional user packages (always available, even with requirements.txt)
        if [ -n "$additional" ]; then
            echo "# Additional user packages" >> "$dockerfile"
            echo "RUN pip install --no-cache-dir \\" >> "$dockerfile"
            local pkg_array=($additional)
            local pkg_count=${#pkg_array[@]}
            local i=0
            for pkg in "${pkg_array[@]}"; do
                ((i++))
                if [ $i -lt $pkg_count ]; then
                    echo "    $pkg \\" >> "$dockerfile"
                else
                    echo "    $pkg" >> "$dockerfile"
                fi
            done
            echo "" >> "$dockerfile"
        fi

        # Always add custom section for future additions
        cat >> "$dockerfile" << DOCKERFILEEOF
# Custom additional packages

DOCKERFILEEOF
    else
        echo "# Custom base image - no default packages installed" >> "$dockerfile"
        echo "" >> "$dockerfile"
    fi

    # Jupyter configuration (only if Jupyter was installed)
    if [ "$skip_base" != true ] && [ "$jupyter_choice" = "default" ] || [ "$jupyter_choice" = "custom" ]; then
        cat >> "$dockerfile" << DOCKERFILEEOF
# Configure Jupyter Lab
RUN jupyter lab --generate-config && \\
    echo "c.ServerApp.ip = '0.0.0.0'" >> /root/.jupyter/jupyter_lab_config.py && \\
    echo "c.ServerApp.allow_root = True" >> /root/.jupyter/jupyter_lab_config.py && \\
    echo "c.ServerApp.open_browser = False" >> /root/.jupyter/jupyter_lab_config.py && \\
    echo "c.ServerApp.token = ''" >> /root/.jupyter/jupyter_lab_config.py && \\
    echo "c.ServerApp.password = ''" >> /root/.jupyter/jupyter_lab_config.py

# IPython kernel
RUN python -m ipykernel install --user \\
    --name=$project_name \\
    --display-name="$project_name ($framework)"

DOCKERFILEEOF
    fi

    # User/Group Setup (DS01 optimization - avoids slow docker commit at container creation)
    # These ARGs are passed at build time via --build-arg
    cat >> "$dockerfile" << 'DOCKERFILEEOF'
# User/Group Setup (DS01 - baked into image to avoid docker commit at container creation)
ARG DS01_USER_ID
ARG DS01_GROUP_ID
ARG DS01_USERNAME

# Create user/group with specific UID:GID matching host user
RUN set -e && \
    if [ -n "$DS01_USER_ID" ] && [ -n "$DS01_GROUP_ID" ] && [ -n "$DS01_USERNAME" ]; then \
        echo "DS01: Setting up user $DS01_USERNAME (UID=$DS01_USER_ID, GID=$DS01_GROUP_ID)" && \
        # Step 1: Remove conflicting group if exists with different name
        EXISTING_GROUP=$(getent group $DS01_GROUP_ID 2>/dev/null | cut -d: -f1 || true) && \
        if [ -n "$EXISTING_GROUP" ] && [ "$EXISTING_GROUP" != "$DS01_USERNAME" ]; then \
            echo "DS01: Removing conflicting group $EXISTING_GROUP (has GID $DS01_GROUP_ID)" && \
            groupdel "$EXISTING_GROUP" 2>/dev/null || true; \
        fi && \
        # Step 2: Create group with specific GID
        if ! getent group $DS01_USERNAME >/dev/null 2>&1; then \
            addgroup --gid $DS01_GROUP_ID $DS01_USERNAME; \
        fi && \
        # Step 3: Remove conflicting user if exists with different name
        EXISTING_USER=$(getent passwd $DS01_USER_ID 2>/dev/null | cut -d: -f1 || true) && \
        if [ -n "$EXISTING_USER" ] && [ "$EXISTING_USER" != "$DS01_USERNAME" ]; then \
            echo "DS01: Removing conflicting user $EXISTING_USER (has UID $DS01_USER_ID)" && \
            userdel -r "$EXISTING_USER" 2>/dev/null || true; \
        fi && \
        # Step 4: Create user with specific UID and GID
        if ! getent passwd $DS01_USERNAME >/dev/null 2>&1; then \
            adduser --disabled-password --gecos "" --uid $DS01_USER_ID --gid $DS01_GROUP_ID $DS01_USERNAME; \
        fi && \
        # Step 4b: CRITICAL - Truncate lastlog/faillog to prevent huge sparse files
        # High UIDs (e.g., 1722830498) cause these files to grow to 300GB+
        # which breaks Docker overlay2 layer export
        : > /var/log/lastlog && \
        : > /var/log/faillog && \
        # Step 5: Configure sudo access (NOPASSWD)
        usermod -aG sudo $DS01_USERNAME && \
        echo "$DS01_USERNAME ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers && \
        # Step 6: Create .local/bin directory for pip --user installs
        mkdir -p /home/$DS01_USERNAME/.local/bin && \
        chown -R $DS01_USER_ID:$DS01_GROUP_ID /home/$DS01_USERNAME && \
        # Step 7: Configure PATH in bashrc (matches AIME docker run setup)
        # Add ~/.local/bin to PATH for pip install --user packages
        echo 'export PATH="$HOME/.local/bin:$PATH"' >> /home/$DS01_USERNAME/.bashrc && \
        # Step 8: Configure HOME export (fixes LDAP username issues)
        echo "export HOME=/home/$DS01_USERNAME" >> /home/$DS01_USERNAME/.bashrc && \
        # Step 9: Configure PS1 prompt (matches AIME docker run setup)
        # Uses container hostname which Docker sets from --name or container ID
        # Format: [container-name] user@host:path$
        echo 'export PS1='"'"'[\h] \u@ds01:\w\$ '"'"'' >> /home/$DS01_USERNAME/.bashrc && \
        chown $DS01_USER_ID:$DS01_GROUP_ID /home/$DS01_USERNAME/.bashrc && \
        echo "DS01: User setup complete"; \
    else \
        echo "DS01: Skipping user setup (build args not provided)"; \
    fi

DOCKERFILEEOF

    # Store user info in labels for mlc-patched.py to verify and skip commit
    cat >> "$dockerfile" << DOCKERFILEEOF
# DS01 Labels for container creation optimization
LABEL aime.mlc.DS01_HAS_USER_SETUP="true"
LABEL aime.mlc.DS01_USER_ID="$USER_ID"
LABEL aime.mlc.DS01_GROUP_ID="$GROUP_ID"
LABEL aime.mlc.DS01_USERNAME="$SANITIZED_USERNAME"

DOCKERFILEEOF

    # Final environment variables and CMD
    cat >> "$dockerfile" << DOCKERFILEEOF
# Environment variables
ENV PYTHONUNBUFFERED=1
ENV CUDA_DEVICE_ORDER=PCI_BUS_ID
ENV HF_HOME=/workspace/.cache/huggingface
ENV TORCH_HOME=/workspace/.cache/torch
ENV MPLCONFIGDIR=/workspace/.cache/matplotlib

CMD ["/bin/bash"]
DOCKERFILEEOF

    echo "$dockerfile"
}

# Parse arguments
IMAGE_NAME=""
FRAMEWORK=""
USECASE=""
ADDITIONAL_PACKAGES=""
SYSTEM_PACKAGES=""
CUSTOM_BASE_IMAGE=""
JUPYTER_CHOICE=""
CUSTOM_JUPYTER_PACKAGES=""
DATA_SCIENCE_CHOICE=""
CUSTOM_DS_PACKAGES=""
CUSTOM_USECASE_PACKAGES=""
SKIP_BASE_PACKAGES=false
NO_BUILD=false
GUIDED=false
EXISTING_DOCKERFILE=""

while [[ $# -gt 0 ]]; do
    case $1 in
        -f|--framework)
            FRAMEWORK="$2"
            shift 2
            ;;
        -t|--type|--type=*)
            if [[ "$1" == --type=* ]]; then
                USECASE="${1#*=}"
                shift
            else
                USECASE="$2"
                shift 2
            fi
            ;;
        -p|--packages)
            ADDITIONAL_PACKAGES="$2"
            shift 2
            ;;
        -s|--system)
            SYSTEM_PACKAGES="$2"
            shift 2
            ;;
        -r|--requirements)
            REQUIREMENTS_FILE="$2"
            REQUIREMENTS_MODE=true
            shift 2
            ;;
        --project-dockerfile)
            PROJECT_DOCKERFILE=true
            shift
            ;;
        --dockerfile|--dockerfile=*)
            if [[ "$1" == --dockerfile=* ]]; then
                EXISTING_DOCKERFILE="${1#*=}"
                shift
            else
                EXISTING_DOCKERFILE="$2"
                shift 2
            fi
            ;;
        --no-build)
            NO_BUILD=true
            shift
            ;;
        --guided)
            GUIDED=true
            shift
            ;;
        -h|--help)
            usage
            exit 0
            ;;
        --info)
            extended_usage
            exit 0
            ;;
        --concepts)
            show_concepts
            exit 0
            ;;
        -*)
            echo -e "${RED}Unknown option: $1${NC}\n"
            usage
            exit 1
            ;;
        *)
            if [ -z "$IMAGE_NAME" ]; then
                IMAGE_NAME="$1"
            else
                echo -e "${RED}Multiple image names specified${NC}\n"
                usage
                exit 1
            fi
            shift
            ;;
    esac
done

# Validate --requirements flag if provided
if [ "$REQUIREMENTS_MODE" = true ] && [ -n "$REQUIREMENTS_FILE" ]; then
    # Expand ~ if present
    REQUIREMENTS_FILE="${REQUIREMENTS_FILE/#\~/$HOME}"

    if ! validate_requirements_file "$REQUIREMENTS_FILE"; then
        echo -e "${RED}Error: Invalid requirements file${NC}"
        exit 1
    fi

    SHORT_REQ="${REQUIREMENTS_FILE/#$HOME/~}"
    echo -e "${GREEN}âœ“${NC} Using requirements file: ${CYAN}$SHORT_REQ${NC}"

    # Still include Jupyter and Data Science by default (user can override with --skip-jupyter etc)
    # Only skip use-case since requirements.txt handles domain packages
    USECASE="custom"
    CUSTOM_USECASE_PACKAGES=""
fi

# Handle --project-dockerfile flag
if [ "$PROJECT_DOCKERFILE" = true ]; then
    # Try to determine project name from image name
    if [ -n "$IMAGE_NAME" ]; then
        # Remove -image suffix if present
        PROJECT_NAME="${IMAGE_NAME%-image}"
        PROJECT_DIR="$HOME/workspace/$PROJECT_NAME"

        # Create project directory if it doesn't exist
        if [ ! -d "$PROJECT_DIR" ]; then
            echo -e "${YELLOW}âš  Project directory does not exist: $PROJECT_DIR${NC}"
            read -p "Create it now? [Y/n]: " CREATE_DIR </dev/tty
            CREATE_DIR=${CREATE_DIR:-Y}
            if [[ "$CREATE_DIR" =~ ^[Yy]$ ]]; then
                mkdir -p "$PROJECT_DIR"
                echo -e "${GREEN}âœ“ Created $PROJECT_DIR${NC}"
            else
                echo -e "${RED}Cannot use --project-dockerfile without project directory${NC}"
                echo "Falling back to centralized location: ~/dockerfiles/"
                PROJECT_DOCKERFILE=false
                PROJECT_DIR=""
            fi
        fi
    else
        echo -e "${YELLOW}âš  --project-dockerfile requires an image name${NC}"
        PROJECT_DOCKERFILE=false
    fi
fi

# Show guided introduction
if [ "$GUIDED" = true ]; then
    echo ""
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo -e "${BOLD}Docker Image Creation - Guided Mode${NC}"
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo ""
    echo -e "${BOLD}What is a Docker Image?${NC}"
    echo ""
    echo "  A Docker image is a blueprint for your computing environment:"
    echo "  â€¢ Contains operating system (Ubuntu Linux)"
    echo "  â€¢ Pre-installed software (Python, PyTorch, TensorFlow, etc.)"
    echo "  â€¢ All libraries and dependencies"
    echo "  â€¢ Configuration and settings"
    echo ""
    echo -e "${BOLD}Why Create a Custom Image?${NC}"
    echo ""
    echo "  You create an image to:"
    echo "  â€¢ Install specific Python packages you need"
    echo "  â€¢ Set up your preferred ML/DL framework version"
    echo "  â€¢ Include domain-specific libraries (CV, NLP, RL)"
    echo "  â€¢ Save time - install once, use many times"
    echo ""
    echo -e "${YELLOW}ğŸ’¡ Pro Tip: ${BOLD}Image vs Container:${NC}"
    echo ""
    echo -e "  â€¢ ${CYAN}Image${NC} = Blueprint (like a recipe)"
    echo -e "  â€¢ ${CYAN}Container${NC} = Running instance (like the cooked dish)"
    echo "  â€¢ One image can create many containers"
    echo "  â€¢ Containers inherit everything from the image"
    echo ""
    echo -e "${BOLD}The Process:${NC}"
    echo ""
    echo "  1. Choose base framework (PyTorch, TensorFlow, JAX)"
    echo "  2. Select use case (CV, NLP, RL, or general ML)"
    echo "  3. Add extra packages if needed"
    echo "  4. Build image (takes 3-5 minutes, depending on existing cache)"
    echo "  5. Create containers from this image"
    echo ""
    read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
    read -p "Press Enter to start creating your image..." </dev/tty
    echo ""
fi

# Interactive mode if missing info (skip framework/usecase prompts if using existing Dockerfile)
if [ -n "$EXISTING_DOCKERFILE" ]; then
    # Using existing Dockerfile - only need image name
    if [ -z "$IMAGE_NAME" ]; then
        echo -e "${RED}Error:${NC} Image name required when using --dockerfile"
        echo "Usage: image-create <image-name> --dockerfile <path>"
        exit 1
    fi
    # Sanitize image name
    IMAGE_NAME=$(echo "$IMAGE_NAME" | tr ' ' '-' | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9._-]//g' | sed 's/^[._-]*//; s/[._-]*$//')
    FULL_IMAGE_NAME="ds01-${USER_ID}/${IMAGE_NAME}"
    # Set defaults for metadata (not used for build but needed for info file)
    [ -z "$FRAMEWORK" ] && FRAMEWORK="custom"
    [ -z "$USECASE" ] && USECASE="custom"
elif [ -z "$IMAGE_NAME" ] || [ -z "$FRAMEWORK" ] || [ -z "$USECASE" ]; then
    interactive_mode
else
    # Non-interactive: sanitize image name - lowercase, spaces to hyphens, remove invalid chars, strip leading/trailing separators
    IMAGE_NAME=$(echo "$IMAGE_NAME" | tr ' ' '-' | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9._-]//g' | sed 's/^[._-]*//; s/[._-]*$//')
    if [ -z "$IMAGE_NAME" ]; then
        echo -e "${RED}Error: Image name cannot be empty or only contain special characters${NC}"
        exit 1
    fi
    # Use same naming convention as interactive mode: ds01-{user-id}/{project-name}
    FULL_IMAGE_NAME="ds01-${USER_ID}/${IMAGE_NAME}"

    # In non-interactive mode, default to including Jupyter and Data Science packages
    # unless using requirements.txt mode (which sets these to "skip")
    if [ "$REQUIREMENTS_MODE" != true ]; then
        [ -z "$JUPYTER_CHOICE" ] && JUPYTER_CHOICE="default"
        [ -z "$DATA_SCIENCE_CHOICE" ] && DATA_SCIENCE_CHOICE="default"
    fi
fi

# Check user's resource limits and provide helpful info
check_user_limits

# Create or use existing Dockerfile
if [ -n "$EXISTING_DOCKERFILE" ]; then
    # Expand ~ if present
    EXISTING_DOCKERFILE="${EXISTING_DOCKERFILE/#\~/$HOME}"

    if [ ! -f "$EXISTING_DOCKERFILE" ]; then
        echo -e "${RED}Error:${NC} Dockerfile not found: $EXISTING_DOCKERFILE"
        exit 1
    fi

    DOCKERFILE="$EXISTING_DOCKERFILE"
    echo -e "${GREEN}âœ“${NC} Using existing Dockerfile: ${BLUE}$DOCKERFILE${NC}\n"
else
    echo -e "${CYAN}Creating Dockerfile...${NC}\n"

    echo -e "${GREEN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo -e "${GREEN}${BOLD}âœ“ Phase 1/2: Complete${NC}"
    echo -e "${GREEN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo ""

    DOCKERFILE=$(create_dockerfile "$FULL_IMAGE_NAME" "$FRAMEWORK" "$USECASE" "$ADDITIONAL_PACKAGES" "$SYSTEM_PACKAGES" "$CUSTOM_BASE_IMAGE" "$JUPYTER_CHOICE" "$CUSTOM_JUPYTER_PACKAGES" "$DATA_SCIENCE_CHOICE" "$CUSTOM_DS_PACKAGES" "$CUSTOM_USECASE_PACKAGES" "$SKIP_BASE_PACKAGES" "$REQUIREMENTS_FILE")
    echo -e "${GREEN}âœ“${NC} Dockerfile created: ${BLUE}$DOCKERFILE${NC}\n"
fi


# Guided explanation of Dockerfile
if [ "$GUIDED" = true ]; then
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo -e "${BOLD}What is a Dockerfile?${NC}"
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo ""
    echo "A Dockerfile is a recipe for building your image:"
    echo ""
    echo -e "  â€¢ ${CYAN}FROM${NC} - Start with a base image (PyTorch, TensorFlow, etc.)"
    echo -e "  â€¢ ${CYAN}RUN${NC} - Execute commands (install packages, configure)"
    echo -e "  â€¢ ${CYAN}ENV${NC} - Set environment variables"
    echo -e "  â€¢ ${CYAN}WORKDIR${NC} - Set working directory"
    echo ""
    echo "Your Dockerfile is saved at:"
    echo -e "  ${BLUE}$DOCKERFILE${NC}"
    echo ""
    echo "You can edit it later to:"
    echo "  â€¢ Add more packages"
    echo "  â€¢ Change configurations"
    echo "  â€¢ Install system tools"
    echo ""
    echo -e "Then rebuild with: ${GREEN}image-update $FULL_IMAGE_NAME${NC}"
    echo ""
fi

# Phase 2: Build Image?
BUILD_IMAGE=true
if [ "$NO_BUILD" = true ]; then
    BUILD_IMAGE=false
    echo -e "${YELLOW}Build skipped (--no-build flag)${NC}"
    echo ""
    build_context=$(dirname "$DOCKERFILE")
    echo "Build later with:"
    echo -e "  ${GREEN}docker build --build-arg DS01_USER_ID=$USER_ID --build-arg DS01_GROUP_ID=$GROUP_ID --build-arg DS01_USERNAME=$SANITIZED_USERNAME -t $FULL_IMAGE_NAME:latest -f $DOCKERFILE $build_context/${NC}"
    echo ""
else
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    echo -e "${BOLD}Phase 2/2: Build Docker Image${NC}"
    echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
    if [ "$GUIDED" = true ]; then
        echo ""
        echo "This will:"
        echo "  â€¢ Download base image"
        echo "  â€¢ Install all packages"
        echo "  â€¢ Configure Jupyter and Python"
        echo "  â€¢ Save final image (~18 GB)"
        echo ""
    fi
    echo -e "${DIM}This can take a few minutes${NC}"
    echo ""
    read -r -t 0.1 -n 10000 discard </dev/tty 2>/dev/null || true
    read -p "Build image now? [Y/n]: " BUILD_CONFIRM </dev/tty
    BUILD_CONFIRM=${BUILD_CONFIRM:-Y}
    echo ""

    if [[ ! "$BUILD_CONFIRM" =~ ^[Yy]$ ]]; then
        BUILD_IMAGE=false
        echo "Build skipped."
        echo ""
        build_context=$(dirname "$DOCKERFILE")
        echo "Build later with:"
        echo -e "  ${GREEN}docker build --build-arg DS01_USER_ID=$USER_ID --build-arg DS01_GROUP_ID=$GROUP_ID --build-arg DS01_USERNAME=$SANITIZED_USERNAME -t $FULL_IMAGE_NAME:latest -f $DOCKERFILE $build_context/${NC}"
        echo ""
    fi
fi

# Build image if confirmed
IMAGE_BUILT=false
if [ "$BUILD_IMAGE" = true ]; then
    echo -e "${CYAN}Building image... (this takes 3-5 minutes, depending on existing cache)${NC}\n"

    # Use the directory containing the Dockerfile as build context
    build_context=$(dirname "$DOCKERFILE")
    # Pass user info as build args for user/group setup inside image
    if docker build \
        --build-arg DS01_USER_ID="$USER_ID" \
        --build-arg DS01_GROUP_ID="$GROUP_ID" \
        --build-arg DS01_USERNAME="$SANITIZED_USERNAME" \
        -t "$FULL_IMAGE_NAME:latest" \
        -f "$DOCKERFILE" \
        "$build_context/"; then
        IMAGE_BUILT=true

        # Save metadata (sanitize filename - replace / with _)
        mkdir -p "$HOME/ds01-config/images"
        METADATA_FILE=$(echo "$FULL_IMAGE_NAME" | tr '/' '_')
        cat > "$HOME/ds01-config/images/${METADATA_FILE}.info" << INFOEOF
Image: $FULL_IMAGE_NAME
Framework: $FRAMEWORK
Use Case: $USECASE
Created: $(date)
Dockerfile: $DOCKERFILE

Packages:
$([ -n "$(get_usecase_packages $USECASE)" ] && echo "- Use case: $(get_usecase_packages $USECASE)")
$([ -n "$ADDITIONAL_PACKAGES" ] && echo "- Additional: $ADDITIONAL_PACKAGES")
$([ -n "$SYSTEM_PACKAGES" ] && echo "- System: $SYSTEM_PACKAGES")
INFOEOF

        # Show success output (suppress banner in orchestration context)
        if type is_atomic_context &>/dev/null && is_atomic_context && [[ -z "$DS01_ORCHESTRATOR" ]]; then
            echo ""
            echo -e "${GREEN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
            echo -e "${GREEN}${BOLD}âœ“ Image Built Successfully${NC}"
            echo -e "${GREEN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
            echo ""
            echo -e "${BOLD}Image:${NC} ${CYAN}$FULL_IMAGE_NAME:latest${NC}"
            echo -e "${DIM}Dockerfile: $DOCKERFILE${NC}"

            if [ "$GUIDED" = true ]; then
                echo ""
                echo -e "${BOLD}What Just Happened?${NC}"
                echo "  âœ“ Downloaded base image with $FRAMEWORK"
                echo "  âœ“ Installed packages"
                echo "  âœ“ Configured Jupyter Lab"
                echo ""
                echo -e "${BOLD}Key Concepts:${NC}"
                echo -e "  â€¢ ${CYAN}Dockerfile${NC} = Recipe (text instructions)"
                echo -e "  â€¢ ${CYAN}Image${NC} = Blueprint (stored by Docker)"
                echo -e "  â€¢ ${CYAN}Container${NC} = Running instance"
                echo ""
                echo -e "${BOLD}Next:${NC} ${GREEN}container-deploy $IMAGE_NAME${NC}"
                echo ""
                echo -e "${BOLD}Other Commands:${NC}"
                echo -e "  ${GREEN}image-list${NC}     - List your images"
                echo -e "  ${GREEN}image-update${NC}   - Modify and rebuild"
            else
                echo ""
                echo -e "Next: ${GREEN}container-deploy $IMAGE_NAME${NC}"
            fi
        elif [[ -z "$DS01_ORCHESTRATOR" ]]; then
            # Fallback if context library not loaded - still show output
            echo ""
            echo -e "${GREEN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
            echo -e "${GREEN}${BOLD}âœ“ Image Built Successfully${NC}"
            echo -e "${GREEN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
            echo ""
            echo -e "${BOLD}Image:${NC} ${CYAN}$FULL_IMAGE_NAME:latest${NC}"
            echo ""
            echo -e "Next: ${GREEN}container-deploy $IMAGE_NAME${NC}"
        fi
        # In orchestration context (DS01_ORCHESTRATOR set), output is suppressed
        # Orchestrator handles its own success messages
    else
        echo ""
        echo -e "${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
        echo -e "${RED}${BOLD}âœ— Phase 2/2: Build Failed${NC}"
        echo -e "${RED}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
        echo ""
        echo "Check Dockerfile: $DOCKERFILE"
        echo ""
        exit 1
    fi
fi
