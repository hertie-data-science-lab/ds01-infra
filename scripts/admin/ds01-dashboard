#!/bin/bash
# DS01 Admin Dashboard - System overview for administrators

set -e

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
CYAN='\033[0;36m'
BLUE='\033[0;34m'
BOLD='\033[1m'
DIM='\033[2m'
NC='\033[0m'

echo -e "${CYAN}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo -e "${BOLD}              DS01 GPU SERVER ADMIN DASHBOARD${NC}"
echo -e "${CYAN}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo ""

# System Info
echo -e "${BOLD}SYSTEM INFORMATION${NC}"
echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"
echo -e "  Hostname:     $(hostname)"
echo -e "  Uptime:       $(uptime -p)"
echo -e "  Kernel:       $(uname -r)"
echo -e "  Date:         $(date)"
echo ""

# GPU Status
echo -e "${BOLD}GPU STATUS${NC}"
echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"
if command -v nvidia-smi &>/dev/null; then
    # Show physical GPUs with MIG mode
    nvidia-smi --query-gpu=index,name,mig.mode.current,memory.total,temperature.gpu --format=csv,noheader | \
        awk -F', ' 'BEGIN {printf "  %-5s %-25s %-12s %-12s %-6s\n", "GPU", "Name", "MIG Mode", "Mem Total", "Temp"}
                    BEGIN {printf "  %-5s %-25s %-12s %-12s %-6s\n", "---", "----", "--------", "---------", "----"}
                    {
                        mig_status = ($3 == "Enabled") ? "MIG ✓" : "Full GPU"
                        printf "  %-5s %-25s %-12s %-12s %-6s\n", $1, $2, mig_status, $4, $5
                    }'

    # Show utilization only for non-MIG GPUs (MIG GPUs show [N/A])
    echo ""
    echo -e "  ${BOLD}Full GPU Utilization:${NC}"
    nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv,noheader | \
        awk -F', ' '{
            if ($2 != "[N/A]" && $2 != "N/A") {
                printf "    GPU %s: %s (Memory: %s)\n", $1, $2, $3
            }
        }' | head -n 10

    # If no full GPUs, show message
    if ! nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader | grep -qv "\[N/A\]"; then
        echo "    (All GPUs have MIG enabled - see MIG instances below)"
    fi
else
    echo -e "${YELLOW}  nvidia-smi not available${NC}"
fi
echo ""

# GPU Allocation Status (MIG-aware)
if [ -f /opt/ds01-infra/scripts/docker/gpu_allocator.py ]; then
    echo -e "${BOLD}GPU ALLOCATION STATUS${NC}"
    echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"

    # Use Python to format GPU status with MIG utilization
    python3 << 'PYEOF'
import sys
import subprocess
import re
sys.path.insert(0, '/opt/ds01-infra/scripts/docker')

def get_mig_process_memory():
    """Get per-MIG GPU memory usage from running processes"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-compute-apps=pid,used_memory,gpu_uuid', '--format=csv,noheader'],
            capture_output=True, text=True, check=True
        )

        mig_memory = {}
        for line in result.stdout.strip().split('\n'):
            if not line.strip():
                continue
            parts = [p.strip() for p in line.split(',')]
            if len(parts) >= 3:
                pid, mem_str, uuid = parts[0], parts[1], parts[2]
                # Parse memory (e.g., "1024 MiB" -> 1024)
                mem_mb = int(re.search(r'\d+', mem_str).group())
                if uuid.startswith('MIG-'):
                    mig_memory[uuid] = mig_memory.get(uuid, 0) + mem_mb

        return mig_memory
    except:
        return {}

def get_physical_gpu_util():
    """Get physical GPU utilization percentages"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=index,utilization.gpu', '--format=csv,noheader,nounits'],
            capture_output=True, text=True, check=True
        )

        gpu_util = {}
        for line in result.stdout.strip().split('\n'):
            if not line.strip():
                continue
            parts = line.split(',')
            if len(parts) >= 2:
                gpu_id = parts[0].strip()
                util = parts[1].strip()
                if util != '[N/A]' and util != 'N/A':
                    gpu_util[gpu_id] = int(util)

        return gpu_util
    except:
        return {}

try:
    from gpu_allocator import GPUAllocationManager

    manager = GPUAllocationManager()
    status = manager.get_status()

    # Get MIG process memory and physical GPU utilization
    mig_memory = get_mig_process_memory()
    gpu_util = get_physical_gpu_util()

    if status['mig_enabled']:
        print(f"  Total MIG Instances: {status['total_gpus']} ({status['total_allocated_containers']} with containers)")
        print()

        # Group by physical GPU
        by_gpu = {}
        for gpu in status['gpus']:
            physical = gpu.get('physical_gpu', '0')
            if physical not in by_gpu:
                by_gpu[physical] = []
            by_gpu[physical].append(gpu)

        # Display grouped by GPU
        for gpu_id in sorted(by_gpu.keys(), key=lambda x: int(x) if x.isdigit() else 0):
            instances = by_gpu[gpu_id]

            # Check if this is a full GPU or MIG GPU
            is_full_gpu = instances and instances[0].get('type') == 'physical_gpu'

            if is_full_gpu:
                # Full GPU (no MIG)
                inst = instances[0]
                util = gpu_util.get(gpu_id, 0)
                mem_used = inst.get('memory_used', 0)
                mem_total = inst.get('memory_total', 1)
                mem_pct = (mem_used / mem_total * 100) if mem_total > 0 else 0

                # Convert MB to GB for display
                mem_used_gb = mem_used / 1024
                mem_total_gb = mem_total / 1024

                print(f"  GPU {gpu_id}: Full GPU (MIG disabled)")
                print(f"    Utilization: {util}% | Memory: {mem_used_gb:.1f}GB / {mem_total_gb:.1f}GB ({mem_pct:.0f}%)")

                count = inst['container_count']
                if count > 0:
                    print(f"    Containers: \033[1m{count}\033[0m")
                    for container in inst['containers']:
                        # Extract username from container name (format: name._.uid)
                        user = container.split('._.')[0] if '._.' in container else 'unknown'
                        print(f"       └─ \033[0;36m{container}\033[0m ({user})")
                else:
                    print(f"    \033[1;33mavailable\033[0m")
                print()
            else:
                # MIG GPU
                physical_util = gpu_util.get(gpu_id, 0)
                print(f"  GPU {gpu_id}: {len(instances)} MIG instances (Physical GPU util: {physical_util}%)")

                for inst in instances:
                    mig_id = inst['id']
                    count = inst['container_count']
                    profile = inst.get('profile', 'unknown')
                    uuid = inst.get('uuid', '')

                    # Get MIG-specific memory usage if available
                    mig_mem_mb = mig_memory.get(uuid, 0)
                    mig_mem_gb = mig_mem_mb / 1024

                    # Parse profile memory (e.g., "1g.10gb" -> 10GB)
                    profile_mem = 10  # default
                    if 'gb' in profile.lower():
                        try:
                            profile_mem = int(re.search(r'(\d+)gb', profile.lower()).group(1))
                        except:
                            pass

                    # Calculate memory percentage
                    mem_pct = (mig_mem_gb / profile_mem * 100) if profile_mem > 0 else 0

                    # Format MIG utilization display
                    # Note: Per-MIG utilization not available from nvidia-smi (shows physical GPU util instead)
                    if mig_mem_mb > 0:
                        mig_util_str = f"Util: {physical_util}% | Mem: {mig_mem_gb:.1f}GB/{profile_mem}GB ({mem_pct:.0f}%)"
                    else:
                        mig_util_str = f"Util: {physical_util}% | Mem: 0.0GB/{profile_mem}GB (0%)"

                    if count > 0:
                        print(f"    • MIG {mig_id} ({profile}): {mig_util_str}")
                        print(f"      Containers: \033[1m{count}\033[0m")
                        for container in inst['containers']:
                            user = container.split('._.')[0] if '._.' in container else 'unknown'
                            print(f"         └─ \033[0;36m{container}\033[0m ({user})")
                    else:
                        print(f"    • MIG {mig_id} ({profile}): \033[1;33mavailable\033[0m")
                print()
    else:
        # Non-MIG mode
        print(f"  Physical GPUs: {status['total_gpus']}")
        print(f"  Containers allocated: {status['total_allocated_containers']}")
        print()

        for gpu in status['gpus']:
            gpu_id = gpu['id']
            count = gpu['container_count']
            util = gpu_util.get(gpu_id, 0)
            mem_used = gpu.get('memory_used', 0)
            mem_total = gpu.get('memory_total', 1)
            mem_pct = (mem_used / mem_total * 100) if mem_total > 0 else 0

            print(f"  GPU {gpu_id}: {util}% util | {mem_used}MB/{mem_total}MB ({mem_pct:.0f}%)")

            if count > 0:
                print(f"    Containers: {count}")
                for container in gpu['containers']:
                    user = container.split('._.')[0] if '._.' in container else 'unknown'
                    print(f"      └─ {container} ({user})")
            else:
                print(f"    \033[1;33mavailable\033[0m")
            print()

except Exception as e:
    import traceback
    print(f"  Error reading GPU allocation status: {e}")
    traceback.print_exc()
    print(f"  Run: python3 /opt/ds01-infra/scripts/docker/gpu_allocator.py status")
    print()
PYEOF

    # Check for containers with GPUs not tracked by allocator
    UNTRACKED_WITH_GPU=$(docker ps --filter "label=aime.mlc.DS01_MANAGED=true" --format '{{.Labels}}' 2>/dev/null | \
        grep "aime.mlc.GPUS=device=" | wc -l)
    UNTRACKED_WITH_GPU=${UNTRACKED_WITH_GPU:-0}

    if [ "$UNTRACKED_WITH_GPU" -gt 0 ] 2>/dev/null; then
        echo -e "  ${DIM}Note: $UNTRACKED_WITH_GPU container(s) with legacy GPU assignments${NC}"
        echo ""
    fi

elif nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | grep -q "Enabled"; then
    echo -e "${BOLD}MIG STATUS${NC}"
    echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"
    echo -e "  ${YELLOW}MIG enabled but allocation tracking not initialized${NC}"
    echo -e "  ${CYAN}Run:${NC} sudo ds01-mig-partition"
    echo ""
fi

# Container Stats
echo -e "${BOLD}CONTAINER OVERVIEW (System-Wide)${NC}"
echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"
TOTAL_CONTAINERS=$(docker ps -a | wc -l)
RUNNING_CONTAINERS=$(docker ps | wc -l)
STOPPED_CONTAINERS=$((TOTAL_CONTAINERS - RUNNING_CONTAINERS - 1))

# DS01-managed containers (subtract 1 for header line)
DS01_TOTAL=$(($(docker ps -a --filter "label=aime.mlc.DS01_MANAGED=true" | wc -l) - 1))
DS01_RUNNING=$(($(docker ps --filter "label=aime.mlc.DS01_MANAGED=true" | wc -l) - 1))
DS01_STOPPED=$((DS01_TOTAL - DS01_RUNNING))

echo -e "  ${BOLD}All Docker Containers:${NC}"
echo -e "    Total:      ${BOLD}$TOTAL_CONTAINERS${NC}  (all users, all types)"
echo -e "    Running:    ${GREEN}$RUNNING_CONTAINERS${NC}"
echo -e "    Stopped:    ${YELLOW}$STOPPED_CONTAINERS${NC}"
echo ""
echo -e "  ${BOLD}DS01-Managed Containers:${NC}"
echo -e "    Total:      ${BOLD}$DS01_TOTAL${NC}  (created via container-create)"
echo -e "    Running:    ${GREEN}$DS01_RUNNING${NC}"
echo -e "    Stopped:    ${YELLOW}$DS01_STOPPED${NC}"

echo ""
echo -e "  ${DIM}Tip: Run 'container-list' to see YOUR containers only${NC}"
echo ""

# Active Users - Show ALL users on system, indicate container status
echo -e "${BOLD}SYSTEM USERS${NC}"
echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"

# Get all human users (UID >= 1000, < 65534) with their UIDs
ALL_USERS=$(getent passwd | awk -F: '$3 >= 1000 && $3 < 65534 {print $1":"$3}' | sort)

if [ -z "$ALL_USERS" ]; then
    echo -e "  ${DIM}No regular users found on system${NC}"
else
    # For each user, check if they have running containers
    while IFS=: read -r username uid; do
        # Get list of container names for this user
        CONTAINER_NAMES=$(docker ps --format '{{.Names}}' 2>/dev/null | grep "\._.${uid}$" || true)

        # Count non-empty lines
        if [ -z "$CONTAINER_NAMES" ]; then
            CONTAINER_COUNT=0
        else
            CONTAINER_COUNT=$(echo "$CONTAINER_NAMES" | wc -l)
        fi

        if [ "$CONTAINER_COUNT" -gt 0 ]; then
            # Show user with green indicator and list containers
            echo -e "  ${BOLD}${GREEN}●${NC} ${BOLD}$username${NC} (uid:$uid): $CONTAINER_COUNT container(s) running"
            # List each container with its status
            echo "$CONTAINER_NAMES" | while read -r container_name; do
                if [ -n "$container_name" ]; then
                    # Extract project name (remove .<username>.<uid> suffix)
                    project_name=$(echo "$container_name" | sed 's/\._.*//')
                    # Get container status
                    container_status=$(docker inspect --format '{{.State.Status}}' "$container_name" 2>/dev/null || echo "unknown")
                    echo -e "      └─ ${GREEN}$project_name${NC} ($container_status)"
                fi
            done
        else
            echo -e "  ${DIM}○ $username (uid:$uid): no containers${NC}"
        fi
    done <<< "$ALL_USERS"
fi

echo ""

# Resource Usage
echo -e "${BOLD}SYSTEM RESOURCES${NC}"
echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"

# CPU
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d% -f1)
echo -e "  CPU Usage:    ${BOLD}${CPU_USAGE}%${NC}"

# Memory
MEM_INFO=$(free -h | grep Mem:)
MEM_TOTAL=$(echo $MEM_INFO | awk '{print $2}')
MEM_USED=$(echo $MEM_INFO | awk '{print $3}')
MEM_PERCENT=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')
echo -e "  Memory:       ${BOLD}${MEM_USED}${NC} / ${MEM_TOTAL} (${MEM_PERCENT}%)"

# Disk
DISK_INFO=$(df -h / | tail -1)
DISK_USED=$(echo $DISK_INFO | awk '{print $3}')
DISK_TOTAL=$(echo $DISK_INFO | awk '{print $2}')
DISK_PERCENT=$(echo $DISK_INFO | awk '{print $5}')
echo -e "  Disk (root):  ${BOLD}${DISK_USED}${NC} / ${DISK_TOTAL} (${DISK_PERCENT})"

DOCKER_DISK=$(df -h /var/lib/docker 2>/dev/null | tail -1 || df -h / | tail -1)
DOCKER_USED=$(echo $DOCKER_DISK | awk '{print $3}')
DOCKER_AVAIL=$(echo $DOCKER_DISK | awk '{print $4}')
echo -e "  Disk (docker): ${BOLD}${DOCKER_USED}${NC} used, ${DOCKER_AVAIL} available"
echo ""

# Systemd Slice Status
if systemctl status ds01.slice &>/dev/null; then
    echo -e "${BOLD}CGROUP SLICE STATUS${NC}"
    echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"
    systemd-cgtop -n 1 --depth=2 2>/dev/null | grep ds01 || echo "  No active cgroup activity"
    echo ""
fi

# Recent Allocation Logs
if [ -f /var/log/ds01/gpu-allocations.log ]; then
    echo -e "${BOLD}RECENT GPU ALLOCATIONS (Last 5)${NC}"
    echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"
    tail -5 /var/log/ds01/gpu-allocations.log | while read -r line; do
        echo -e "  ${line}"
    done
    echo ""
fi

# Warnings
echo -e "${BOLD}SYSTEM HEALTH${NC}"
echo -e "${CYAN}─────────────────────────────────────────────────────────────${NC}"

WARNINGS=0

# Check disk space
if [ "${DISK_PERCENT%\%}" -gt 85 ]; then
    echo -e "  ${RED}⚠${NC}  Disk usage high: ${DISK_PERCENT}"
    WARNINGS=$((WARNINGS + 1))
fi

# Check memory
if (( $(echo "$MEM_PERCENT > 90" | bc -l) )); then
    echo -e "  ${RED}⚠${NC}  Memory usage high: ${MEM_PERCENT}%"
    WARNINGS=$((WARNINGS + 1))
fi

# Check for many stopped containers
if [ "$STOPPED_CONTAINERS" -gt 10 ]; then
    echo -e "  ${YELLOW}⚠${NC}  Many stopped containers: ${STOPPED_CONTAINERS}"
    WARNINGS=$((WARNINGS + 1))
fi

if [ $WARNINGS -eq 0 ]; then
    echo -e "  ${GREEN}✓${NC} All systems nominal"
fi

echo ""
echo -e "${CYAN}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
echo ""
echo -e "${BOLD}Quick Actions:${NC}"
echo "  ds01-logs           - View infrastructure logs"
echo "  ds01-users          - See detailed user activity"
echo "  ds01-container-remove - Clean up stopped containers"
echo ""
