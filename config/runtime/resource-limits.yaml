# DS01 GPU Server - Resource Limits Configuration
# /opt/ds01-infra/config/resource-limits.yaml
#
# Related files:
# - config/groups/*.members       - Group member lists (one file per group)
# - config/group-overrides.txt    - Deterministic group assignment overrides
# - config/user-overrides.yaml    - Per-user resource limit exceptions
# - scripts/lib/error-messages.sh - Error message templates
#
# DATA FLOW (Single Source of Truth):
#   /home/ scan + group-overrides.txt
#       ↓ MERGE (add only)
#   config/groups/*.members ← SINGLE SOURCE OF TRUTH
#       ↓ reads
#   This file (resource-limits.yaml) parser
#       ↓ applies
#   user-overrides.yaml (custom limits)
#
# 4-TIER GROUP MODEL:
#   student    → Full GPU (MIG disabled), 32 CPUs, 32GB (default)
#   researcher → Full GPU access, 64 CPUs, 128GB (PhD students, staff)
#   faculty    → Full GPU access, 64 CPUs, 128GB (professors)
#   admin      → Unlimited (datasciencelab only)

# Default limits for all users (unless overridden by group or user-specific settings)
defaults:
  # === GPU limits ===
  max_mig_instances: 3              # Max MIG/GPU instances user can have allocated (across all containers)
  max_mig_per_container: 3          # Max MIG-equivalents per single container
  allow_full_gpu: false             # Can user request full (non-MIG) GPU? Default: no

  # TODO-NOT-IMPLEMENTED: gpu_memory - redundant with MIG memory limits
  # gpu_memory: null                # GPU memory limit in GB (null = no limit, already enforced by MIG)

  # === Compute limits (per container) ===
  max_cpus: 32                      # CPU cores per container
  memory: 32g                       # RAM limit per container
  memory_swap: 32g                  # Swap limit (should match memory to prevent swap)
  shm_size: 16g                     # Shared memory (important for PyTorch DataLoader)
  pids_limit: 4096                  # Max processes (prevents fork bombs)

  # TODO-NOT-IMPLEMENTED: I/O limits - no enforcement code exists
  # io_read_bps: 500M               # Max read bandwidth (bytes/sec)
  # io_write_bps: 200M              # Max write bandwidth (bytes/sec)
  # ulimit_nofile: 65535            # Max open files

  # TODO-NOT-IMPLEMENTED: Storage quotas - requires manual setup
  # storage_workspace: 100G         # Max storage in /workspace/$USER
  # storage_data: 200G              # Max storage in /data/$USER
  # storage_tmp: 50G                # Max storage in /tmp (inside container)

  # === Container lifecycle ===
  max_containers_per_user: 3        # Max simultaneous containers
  max_runtime: 24h                  # Max walltime (null = no limit)
  idle_timeout: 0.5h                # Auto-stop after X hours of GPU inactivity
  gpu_hold_after_stop: 0.25h        # How long to hold GPU after container stopped (null = indefinite)
  container_hold_after_stop: 0.5h   # Auto-remove container after X hours stopped (null = never auto-remove)
  max_tasks: 32768                  # Max tasks for systemd slice (fixes libgomp thread creation)

default_group: student

# === Enforcement settings ===
enforcement:
  aggregate_limits: true           # Enable per-user aggregate enforcement via systemd slices
  cgroup_driver: systemd           # Required Docker cgroup driver (must match /etc/docker/daemon.json)

# === Group definitions ===
# Member lists are maintained in config/groups/{group}.members files
# File format: one username per line, # comments allowed
groups:
  student:
    # Members: config/groups/student.members
    # Note: Users not in any group file default to 'student' via default_group
    allow_full_gpu: true            # MIG disabled on hardware — students need full GPU access
    max_mig_per_container: 3
    # Per-user aggregate limits (systemd slice enforcement)
    aggregate:
      cpu_quota: "9600%"            # 32 CPUs × 3 containers = 96 CPUs (systemd format: 100% = 1 CPU)
      memory_max: "96G"             # 32GB × 3 = 96GB
      memory_high: "86G"            # 90% of 96GB (soft limit, triggers throttling)
      tasks_max: 12288              # 4096 × 3
      gpu_limit: 3                  # Max GPU/MIG slots across all containers (matches max_mig_instances)
    # Per-group lifecycle policies
    policies:
      gpu_idle_threshold: 5          # GPU util % below which GPU is idle
      cpu_idle_threshold: 2.0        # CPU util % below which CPU is idle (bursty but shorter jobs)
      network_idle_threshold: 1048576  # Network I/O bytes below which network is idle (1MB)
      idle_detection_window: 3       # Consecutive idle checks before action

  researcher:
    # Members: config/groups/researcher.members
    # PhD students, research assistants, staff with research needs
    allow_full_gpu: true            # Full GPU access for PhD work
    max_mig_per_container: 4        # Up to 4 MIG-equivalents per container (= 1 full GPU)
    max_mig_instances: 6            # Can use up to 6 MIG instances or equivalent
    max_cpus: 48                    # Increased for CPU-heavy workloads
    memory: 64g                     # Increased for large datasets
    memory_swap: 64g
    shm_size: 32g
    max_runtime: 48h                # Longer runtime for research jobs
    idle_timeout: 1h
    max_tasks: 65536                # High for parallel workloads
    max_containers_per_user: 5
    # Per-user aggregate limits (systemd slice enforcement)
    aggregate:
      cpu_quota: "24000%"           # 48 CPUs × 5 containers = 240 CPUs
      memory_max: "320G"            # 64GB × 5 = 320GB
      memory_high: "288G"           # 90% of 320GB (soft limit)
      tasks_max: 327680             # 65536 × 5
      gpu_limit: 6                  # Max GPU/MIG slots across all containers (matches max_mig_instances)
    # Per-group lifecycle policies
    policies:
      gpu_idle_threshold: 5
      cpu_idle_threshold: 3.0        # More lenient for data-heavy workflows
      network_idle_threshold: 1048576
      idle_detection_window: 4       # More patient — 4 consecutive checks
    # TODO-NOT-IMPLEMENTED: Storage quotas
    # storage_workspace: 500G
    # storage_data: 1T

  faculty:
    # Members: config/groups/faculty.members
    # Professors and senior researchers - teaching + research needs
    allow_full_gpu: true            # Full GPU access
    max_mig_per_container: 4        # Up to 4 MIG-equivalents per container
    max_mig_instances: 8            # Can use up to 8 MIG instances
    max_cpus: 64                    # High for demanding workloads
    memory: 128g                    # Large memory for big models
    memory_swap: 128g
    shm_size: 64g
    max_runtime: 72h                # Extended runtime for faculty
    idle_timeout: 2h                # Longer idle timeout (teaching demos, etc.)
    max_tasks: 65536
    max_containers_per_user: 5
    # Per-user aggregate limits (systemd slice enforcement)
    aggregate:
      cpu_quota: "32000%"           # 64 CPUs × 5 containers = 320 CPUs
      memory_max: "640G"            # 128GB × 5 = 640GB
      memory_high: "576G"           # 90% of 640GB (soft limit)
      tasks_max: 327680             # 65536 × 5
      gpu_limit: 8                  # Max GPU/MIG slots across all containers (matches max_mig_instances)
    # Per-group lifecycle policies
    policies:
      gpu_idle_threshold: 5
      cpu_idle_threshold: 3.0
      network_idle_threshold: 1048576
      idle_detection_window: 4
    # TODO-NOT-IMPLEMENTED: Storage quotas
    # storage_workspace: 1T
    # storage_data: 2T

  admin:
    # Members: config/groups/admin.members
    allow_full_gpu: true            # Admins CAN request full GPUs
    max_mig_per_container: null     # Unlimited MIG-equivalents per container
    max_mig_instances: null         # Unlimited (can use all GPUs)
    max_cpus: 64
    memory: 128g
    memory_swap: 128g
    shm_size: 64g
    idle_timeout: 1h
    max_runtime: 24h
    gpu_hold_after_stop: 1h         # Hold GPU for 1h after stop
    max_containers_per_user: 10
    max_tasks: null
    # NOTE: No aggregate section — admin users have no aggregate cap
    # TODO-NOT-IMPLEMENTED: Storage quotas
    # storage_workspace: 2T
    # storage_data: 5T

# Per-user overrides are maintained in: config/user-overrides.yaml
# (removed from this file - see that file for format and examples)

# === GPU allocation ===
gpu_allocation:
  # DEPLOYED: This value is used by gpu_allocator_v2.py
  mig_instances_per_gpu: 4          # How many MIG instances per physical GPU (1 full GPU = 4 MIG-equivalents)

  # TODO-NOT-IMPLEMENTED: Strategy settings - hardcoded to least_allocated/dynamic
  # strategy: least_allocated       # Options: least_allocated, round_robin
  # allocation_method: dynamic      # Allocate on container start, not permanent assignment

  # TODO-NOT-IMPLEMENTED: MIG auto-detected via nvidia-smi, these are informational only
  # enable_mig: true                # Enable MIG partitioning
  # mig_strategy: auto              # auto, manual, disabled
  # mig_profile: 2g.20gb            # MIG profile (options: 1g.10gb, 2g.20gb, 3g.40gb)

  # TODO-NOT-IMPLEMENTED: MIG Hardware Configuration - auto-detected
  # mig_gpus:
  #   0: { enable: false, profile: null }         # Full GPU only
  #   1: { enable: true, profile: 1g.10gb, instances: 4 }
  #   2: { enable: true, profile: 1g.10gb, instances: 4 }
  #   3: { enable: true, profile: 1g.10gb, instances: 4 }

  # TODO-NOT-IMPLEMENTED: Reservation system
  # respect_reservations: true      # Honor user_override reservations

# === Container lifecycle policies ===
policies:
  # DEPLOYED: Used by check-idle-containers.sh and high-demand detection
  high_demand_threshold: 0.8       # System is "high demand" at 80% GPU allocation
  high_demand_idle_reduction: 0.5  # Reduce idle timeout by 50% during high demand
  gpu_idle_threshold: 5            # GPU utilisation % threshold for idle detection
  cpu_idle_threshold: 2.0          # CPU utilisation % threshold for idle detection (was hardcoded 1.0)
  network_idle_threshold: 1048576  # Network I/O bytes threshold for idle detection (1MB)
  idle_detection_window: 3         # Consecutive idle checks before action
  grace_period: 30m                # Startup grace period before idle detection begins
  keepalive_max_duration: 24h      # Max time .keep-alive file is respected
  sigterm_grace_seconds: 60        # SIGTERM grace period for GPU containers
  gpu_hold_after_manual_stop: 15m  # GPU hold duration when user manually stops
  created_container_timeout: 30m   # Cleanup timeout for created-never-started containers

  # TODO-NOT-IMPLEMENTED: Policy enforcement
  # auto_stop_idle: true
  # checkpoint_warning: 1h         # Warn users X hours before auto-stop
  # require_workspace_mount: true  # Force all containers to mount /workspace
  # enforce_storage_quotas: true   # Enable disk quotas
  # soft_limit_threshold: 0.8      # Warn at 80% of hard limit
  # graceful_errors: true          # Show helpful error messages when limits exceeded
  # suggest_alternatives: true     # Suggest alternatives when resource unavailable

# === Container type-specific settings (external containers) ===
# These apply to containers created outside ds01 CLI (dev containers, compose, direct docker run)
# Core principle: GPU access = ephemeral enforcement, No GPU = permanent OK
container_types:
  # Dev containers (VS Code, Cursor, etc.) - development work
  devcontainer:
    idle_timeout: null             # Exempt from idle timeout (only subject to max_runtime)
    max_runtime: 168h              # 1 week max
    sigterm_grace_seconds: 30       # Dev containers: K8s default
    default_mig_count:             # Group-based MIG allocation for external containers
      student: 1                   # Students get 1 MIG for dev containers
      researcher: 2                # Researchers can have 2 MIGs in dev containers
      faculty: 2                   # Faculty can have 2 MIGs in dev containers
      admin: null                  # Unlimited for admins

  # Docker Compose containers
  compose:
    idle_timeout: 30m
    max_runtime: 72h               # 3 days max
    sigterm_grace_seconds: 45       # Compose: middle ground
    default_mig_count:
      student: 1
      researcher: 2
      faculty: 2
      admin: null

  # Direct docker run commands
  docker:
    idle_timeout: 30m
    max_runtime: 48h               # 2 days max
    sigterm_grace_seconds: 60       # Direct docker: full minute (likely GPU)
    default_mig_count:
      student: 1
      researcher: 1                # Slightly stricter for raw docker
      faculty: 2
      admin: null

  # API-created containers (Portainer, SDK, etc.) - stricter limits
  unknown:
    idle_timeout: 15m              # Short timeout for unidentified containers
    max_runtime: 24h               # 1 day max
    sigterm_grace_seconds: 30       # Unknown: K8s default
    default_mig_count:
      student: 1
      researcher: 1
      faculty: 1
      admin: null

# === Monitoring configuration ===
# Infrastructure containers excluded from unmanaged container detection
monitoring:
  # Containers matching these prefixes are considered DS01 infrastructure
  # and are excluded from unmanaged container alerts
  infrastructure_containers:
    - ds01-prometheus
    - ds01-grafana
    - ds01-alertmanager
    - ds01-dcgm-exporter
    - ds01-node-exporter

  # Label that marks a container as protected infrastructure
  # Containers with ds01.protected=true are excluded from unmanaged detection
  infrastructure_label: "ds01.protected"

# === Bare metal GPU access ===
# Controls who can use GPUs directly on the host (outside containers)
# See: config/deploy/profile.d/ds01-gpu-awareness.sh
bare_metal_access:
  exempt_users:                     # Users permanently exempt from CUDA_VISIBLE_DEVICES="" block
    - datasciencelab
  admin_group: ds01-admin           # Group with admin privileges for bare-metal-access CLI
  default_grant_duration: 24h       # Default duration for temporary grants
  state_dir: /var/lib/ds01/bare-metal-grants  # Grant state files (JSON, one per user)

# === Access control ===
# Docker wrapper user isolation and admin bypass settings
# See: scripts/docker/docker-wrapper.sh
access_control:
  admin_users:                      # Users who bypass all isolation checks
    - datasciencelab
  admin_group: ds01-admin           # Group that bypasses all isolation checks
  wrapper:
    enforcement_mode: full          # disabled, monitoring, full
    debug_level: 0                  # 0=off, 1=interceptions, 2=all invocations
    unknown_commands: allow          # allow or deny unrecognised Docker commands

# REMOVED SECTIONS:
# - advanced: None of these settings are implemented
# - wizard: Error messages moved to scripts/lib/error-messages.sh
