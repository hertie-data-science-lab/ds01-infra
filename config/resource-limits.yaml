# DS01 GPU Server - Resource Limits Configuration
# /opt/ds01-infra/config/resource-limits.yaml
#
# Related files:
# - config/groups/*.members       - Group member lists (one file per group)
# - config/group-overrides.txt    - Deterministic group assignment overrides
# - config/user-overrides.yaml    - Per-user resource limit exceptions
# - scripts/lib/error-messages.sh - Error message templates
#
# DATA FLOW (Single Source of Truth):
#   /home/ scan + group-overrides.txt
#       ↓ MERGE (add only)
#   config/groups/*.members ← SINGLE SOURCE OF TRUTH
#       ↓ reads
#   This file (resource-limits.yaml) parser
#       ↓ applies
#   user-overrides.yaml (custom limits)
#
# 4-TIER GROUP MODEL:
#   student    → MIG only, 32 CPUs, 32GB (default)
#   researcher → Full GPU access, 64 CPUs, 128GB (PhD students, staff)
#   faculty    → Full GPU access, 64 CPUs, 128GB (professors)
#   admin      → Unlimited (datasciencelab only)

# Default limits for all users (unless overridden by group or user-specific settings)
defaults:
  # === GPU limits ===
  max_mig_instances: 3              # Max MIG/GPU instances user can have allocated (across all containers)
  max_mig_per_container: 3          # Max MIG-equivalents per single container
  allow_full_gpu: false             # Can user request full (non-MIG) GPU? Default: no

  # TODO-NOT-IMPLEMENTED: gpu_memory - redundant with MIG memory limits
  # gpu_memory: null                # GPU memory limit in GB (null = no limit, already enforced by MIG)

  # === Compute limits (per container) ===
  max_cpus: 32                      # CPU cores per container
  memory: 32g                       # RAM limit per container
  memory_swap: 32g                  # Swap limit (should match memory to prevent swap)
  shm_size: 16g                     # Shared memory (important for PyTorch DataLoader)
  pids_limit: 4096                  # Max processes (prevents fork bombs)

  # TODO-NOT-IMPLEMENTED: I/O limits - no enforcement code exists
  # io_read_bps: 500M               # Max read bandwidth (bytes/sec)
  # io_write_bps: 200M              # Max write bandwidth (bytes/sec)
  # ulimit_nofile: 65535            # Max open files

  # TODO-NOT-IMPLEMENTED: Storage quotas - requires manual setup
  # storage_workspace: 100G         # Max storage in /workspace/$USER
  # storage_data: 200G              # Max storage in /data/$USER
  # storage_tmp: 50G                # Max storage in /tmp (inside container)

  # === Container lifecycle ===
  max_containers_per_user: 3        # Max simultaneous containers
  max_runtime: 24h                  # Max walltime (null = no limit)
  idle_timeout: 0.5h                # Auto-stop after X hours of GPU inactivity
  gpu_hold_after_stop: 0.25h        # How long to hold GPU after container stopped (null = indefinite)
  container_hold_after_stop: 0.5h   # Auto-remove container after X hours stopped (null = never auto-remove)
  max_tasks: 32768                  # Max tasks for systemd slice (fixes libgomp thread creation)

default_group: student

# === Group definitions ===
# Member lists are maintained in config/groups/{group}.members files
# File format: one username per line, # comments allowed
groups:
  student:
    # Members: config/groups/student.members
    # Note: Users not in any group file default to 'student' via default_group
    allow_full_gpu: false
    max_mig_per_container: 3

  researcher:
    # Members: config/groups/researcher.members
    # PhD students, research assistants, staff with research needs
    allow_full_gpu: true            # Full GPU access for PhD work
    max_mig_per_container: 4        # Up to 4 MIG-equivalents per container (= 1 full GPU)
    max_mig_instances: 6            # Can use up to 6 MIG instances or equivalent
    max_cpus: 48                    # Increased for CPU-heavy workloads
    memory: 64g                     # Increased for large datasets
    memory_swap: 64g
    shm_size: 32g
    max_runtime: 48h                # Longer runtime for research jobs
    idle_timeout: 1h
    max_tasks: 65536                # High for parallel workloads
    max_containers_per_user: 5
    # TODO-NOT-IMPLEMENTED: Storage quotas
    # storage_workspace: 500G
    # storage_data: 1T

  faculty:
    # Members: config/groups/faculty.members
    # Professors and senior researchers - teaching + research needs
    allow_full_gpu: true            # Full GPU access
    max_mig_per_container: 4        # Up to 4 MIG-equivalents per container
    max_mig_instances: 8            # Can use up to 8 MIG instances
    max_cpus: 64                    # High for demanding workloads
    memory: 128g                    # Large memory for big models
    memory_swap: 128g
    shm_size: 64g
    max_runtime: 72h                # Extended runtime for faculty
    idle_timeout: 2h                # Longer idle timeout (teaching demos, etc.)
    max_tasks: 65536
    max_containers_per_user: 5
    # TODO-NOT-IMPLEMENTED: Storage quotas
    # storage_workspace: 1T
    # storage_data: 2T

  admin:
    # Members: config/groups/admin.members
    allow_full_gpu: true            # Admins CAN request full GPUs
    max_mig_per_container: null     # Unlimited MIG-equivalents per container
    max_mig_instances: null         # Unlimited (can use all GPUs)
    max_cpus: 64
    memory: 128g
    memory_swap: 128g
    shm_size: 64g
    idle_timeout: 1h
    max_runtime: 24h
    gpu_hold_after_stop: 1h         # Hold GPU for 1h after stop
    max_containers_per_user: 10
    max_tasks: null
    # TODO-NOT-IMPLEMENTED: Storage quotas
    # storage_workspace: 2T
    # storage_data: 5T

# Per-user overrides are maintained in: config/user-overrides.yaml
# (removed from this file - see that file for format and examples)

# === GPU allocation ===
gpu_allocation:
  # DEPLOYED: This value is used by gpu_allocator_v2.py
  mig_instances_per_gpu: 4          # How many MIG instances per physical GPU (1 full GPU = 4 MIG-equivalents)

  # TODO-NOT-IMPLEMENTED: Strategy settings - hardcoded to least_allocated/dynamic
  # strategy: least_allocated       # Options: least_allocated, round_robin
  # allocation_method: dynamic      # Allocate on container start, not permanent assignment

  # TODO-NOT-IMPLEMENTED: MIG auto-detected via nvidia-smi, these are informational only
  # enable_mig: true                # Enable MIG partitioning
  # mig_strategy: auto              # auto, manual, disabled
  # mig_profile: 2g.20gb            # MIG profile (options: 1g.10gb, 2g.20gb, 3g.40gb)

  # TODO-NOT-IMPLEMENTED: MIG Hardware Configuration - auto-detected
  # mig_gpus:
  #   0: { enable: false, profile: null }         # Full GPU only
  #   1: { enable: true, profile: 1g.10gb, instances: 4 }
  #   2: { enable: true, profile: 1g.10gb, instances: 4 }
  #   3: { enable: true, profile: 1g.10gb, instances: 4 }

  # TODO-NOT-IMPLEMENTED: Reservation system
  # respect_reservations: true      # Honor user_override reservations

# === Container lifecycle policies ===
policies:
  # DEPLOYED: Used by check-idle-containers.sh and high-demand detection
  high_demand_threshold: 0.8       # System is "high demand" at 80% GPU allocation
  high_demand_idle_reduction: 0.5  # Reduce idle timeout by 50% during high demand

  # TODO-NOT-IMPLEMENTED: Policy enforcement
  # auto_stop_idle: true
  # checkpoint_warning: 1h         # Warn users X hours before auto-stop
  # require_workspace_mount: true  # Force all containers to mount /workspace
  # enforce_storage_quotas: true   # Enable disk quotas
  # soft_limit_threshold: 0.8      # Warn at 80% of hard limit
  # graceful_errors: true          # Show helpful error messages when limits exceeded
  # suggest_alternatives: true     # Suggest alternatives when resource unavailable

# === Container type-specific settings (external containers) ===
# These apply to containers created outside ds01 CLI (dev containers, compose, direct docker run)
# Core principle: GPU access = ephemeral enforcement, No GPU = permanent OK
container_types:
  # Dev containers (VS Code, Cursor, etc.) - development work
  devcontainer:
    idle_timeout: 30m              # Shorter than ds01 default - ephemeral nature
    max_runtime: 168h              # 1 week max
    default_mig_count:             # Group-based MIG allocation for external containers
      student: 1                   # Students get 1 MIG for dev containers
      researcher: 2                # Researchers can have 2 MIGs in dev containers
      faculty: 2                   # Faculty can have 2 MIGs in dev containers
      admin: null                  # Unlimited for admins

  # Docker Compose containers
  compose:
    idle_timeout: 30m
    max_runtime: 72h               # 3 days max
    default_mig_count:
      student: 1
      researcher: 2
      faculty: 2
      admin: null

  # Direct docker run commands
  docker:
    idle_timeout: 30m
    max_runtime: 48h               # 2 days max
    default_mig_count:
      student: 1
      researcher: 1                # Slightly stricter for raw docker
      faculty: 2
      admin: null

  # API-created containers (Portainer, SDK, etc.) - stricter limits
  unknown:
    idle_timeout: 15m              # Short timeout for unidentified containers
    max_runtime: 24h               # 1 day max
    default_mig_count:
      student: 1
      researcher: 1
      faculty: 1
      admin: null

# REMOVED SECTIONS:
# - advanced: None of these settings are implemented
# - wizard: Error messages moved to scripts/lib/error-messages.sh
