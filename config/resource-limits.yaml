# DS01 GPU Server - Resource Limits Configuration
# /opt/ds01-infra/config/resource-limits.yaml

# This file controls default and per-user/group resource allocations

# Default limits for all users (unless overridden below)
defaults:
  # GPU limits
  max_mig_instances: 2              # Max MIG/GPU instances user can have allocated (across all containers)
  max_gpus_per_container: 1         # Max MIG/GPU instances per single container
  allow_full_gpu: false             # Can user request full (non-MIG) GPU? Default: no
  gpu_memory: null                  # GPU memory limit in GB (null = no limit, already enforced by MIG)

  # Compute limits (per container)
  max_cpus: 16                      # CPU cores per container
  memory: 32g                       # RAM limit per container
  memory_swap: 32g                  # Swap limit (should match memory to prevent swap)
  shm_size: 16g                     # Shared memory (important for PyTorch DataLoader)
  pids_limit: 4096                  # Max processes (prevents fork bombs)

  # I/O limits (per container) - for storage fairness
  io_read_bps: 500M                 # Max read bandwidth (bytes/sec)
  io_write_bps: 200M                # Max write bandwidth (bytes/sec)
  ulimit_nofile: 65535              # Max open files

  # Storage limits (enforced via disk quotas)
  storage_workspace: 100G           # Max storage in /workspace/$USER
  storage_data: 200G                # Max storage in /data/$USER
  storage_tmp: 50G                  # Max storage in /tmp (inside container)

  # Container lifecycle
  max_containers_per_user: 3        # Max simultaneous containers
  max_runtime: 24h                  # Max walltime (null = no limit) - increased from 12h for overnight training
  idle_timeout: 2h                  # Auto-stop after X hours of GPU inactivity - increased from 0.5h
  gpu_hold_after_stop: 0.25h        # How long to hold GPU after container stopped (null = indefinite)
  container_hold_after_stop: 0.5h   # Auto-remove container after X hours stopped (null = never auto-remove)
  max_tasks: 512                    # Max tasks for systemd slice

default_group: student

# User group definitions
groups:
  student:
    members: []  # TO DO: autopopulate from LDAP group 'ds01-students'
    allow_full_gpu: false           # Students can ONLY use MIG instances
    max_gpus_per_container: 1       # 1 MIG per container

  researcher:
    members: []
    allow_full_gpu: true            # Researchers CAN request full GPUs
    max_gpus_per_container: 2       # Up to 2 MIG/GPU per container
    max_mig_instances: 8            # Can use up to 8 MIG instances or equivalent
    max_cpus: 32
    memory: 64g
    memory_swap: 64g
    shm_size: 32g
    storage_workspace: 500G
    storage_data: 1T
    max_runtime: 72h                # 3 days for LLM fine-tuning
    idle_timeout: 4h                # More lenient
    max_tasks: 2048
    max_containers_per_user: 5

  admin:
    members: [datasciencelab]
    allow_full_gpu: true            # Admins CAN request full GPUs
    max_gpus_per_container: null    # Unlimited GPUs per container
    max_mig_instances: null         # Unlimited (can use all GPUs)
    max_cpus: 64
    memory: 128g
    memory_swap: 128g
    shm_size: 64g
    storage_workspace: 2T
    storage_data: 5T
    idle_timeout: null              # No timeout
    max_runtime: null               # No limit
    gpu_hold_after_stop: 1h         # Hold GPU for 1h after stop
    container_hold_after_stop: null # Never auto-remove
    max_containers_per_user: 10
    max_tasks: null

# Per-user overrides (takes precedence over group settings)
# These override any group settings for specific users
user_overrides:
  # Example time-based reservation:
  # john_doe:
  #   max_mig_instances: 4
  #   allow_full_gpu: true          # Grant full GPU access temporarily
  #   memory: 64g
  #   reservation_start: "2025-11-01T00:00:00"
  #   reservation_end: "2025-11-08T00:00:00"
  #   reserved_gpus: [0]            # Reserve specific GPU IDs
  #   reason: "Thesis deadline - needs dedicated GPU for 1 week"

# GPU allocation strategy
gpu_allocation:
  strategy: least_allocated         # Options: least_allocated, round_robin
  allocation_method: dynamic        # Allocate on container start, not permanent assignment

  # MIG (Multi-Instance GPU) configuration for A100s
  enable_mig: true                  # Enable MIG partitioning
  mig_strategy: auto                # auto, manual, disabled
  mig_profile: 2g.20gb              # MIG profile (3 instances per A100)
                                    # Options: 1g.10gb (7 instances), 2g.20gb (3 instances), 3g.40gb (2 instances)

  # MIG Hardware Configuration (which GPUs to partition)
  mig_gpus:
    # GPU 0: Leave as full GPU (no MIG) - only accessible to users with allow_full_gpu=true
    0:
      enable: false
      profile: null
    # GPUs 1-3: Enable MIG with 4 instances each (using 1g.10gb profile)
    1:
      enable: true
      profile: 1g.10gb              # 7 instances per GPU (we'll create 4)
      instances: 4                  # Number of instances to create
    2:
      enable: true
      profile: 1g.10gb
      instances: 4
    3:
      enable: true
      profile: 1g.10gb
      instances: 4

  # Full GPU vs MIG access control:
  # - allow_full_gpu: false (default, students): Can ONLY get MIG instances
  # - allow_full_gpu: true (researchers, admins): Can request MIG OR full GPU
  # - GPU 0 (full GPU) only allocated to users with allow_full_gpu=true
  # - GPUs 1-3 (MIG) available to all users

  respect_reservations: true        # Honor user_override reservations
  
# Container lifecycle policies
policies:
  auto_stop_idle: true
  checkpoint_warning: 1h           # Warn users X hours before auto-stop
  require_workspace_mount: true     # Force all containers to mount /workspace
  enforce_storage_quotas: true      # Enable disk quotas
  
  # Error handling
  graceful_errors: true             # Show helpful error messages when limits exceeded
  suggest_alternatives: true        # Suggest alternatives when resource unavailable
  
# Advanced settings
advanced:
  enable_mps: false                 # MPS (Multi-Process Service) - disabled when MIG enabled
  network_mode: host                # or 'bridge'
  restart_policy: unless-stopped

# Wizard defaults (for interactive setup) TODO: move this to a script
wizard:
  default_mig_count: 1              # Default MIG request in wizard
  recommended_mig_count: 1          # Recommended MIG count shown to users
  max_mig_warning_threshold: 4      # Warn if requesting more than this
  
  # Error messages for common scenarios
  error_messages:
    mig_limit_exceeded: |
      ❌ GPU Limit Exceeded
      
      You requested {requested} MIG instances, but your limit is {max} MIGs.
      You currently have {current} MIGs allocated.
      
      Options:
      1. Reduce your MIG request to {available} or fewer
      2. Stop an existing container to free up GPUs
      3. Launch this container without GPU (CPU-only)
      
      Check your current allocations: ds01-gpu-status
      
    container_limit_exceeded: |
      ❌ Container Limit Exceeded
      
      You already have {current} containers running.
      Your limit is {max} containers.
      
      Please stop a container before creating a new one:
        mlc-stop <container-name>
      
    no_gpu_available: |
      ⚠️ No GPUs Available
      
      All GPUs are currently allocated to other users.
      
      Options:
      1. Wait for a GPU to become available
      2. Launch as CPU-only container for now
      3. Check GPU status: ds01-gpu-status
      
    reservation_conflict: |
      ❌ GPU Reserved
      
      The requested GPU is reserved for {reserved_user} until {end_time}.
      Reason: {reason}
      
      Please try a different GPU or wait for the reservation to end.
