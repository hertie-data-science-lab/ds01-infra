---
phase: 02-awareness-layer
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/monitoring/detect-workloads.py
autonomous: true

must_haves:
  truths:
    - "Scanner detects all running containers and classifies by origin (ds01-managed, devcontainer, compose, raw-docker)"
    - "Scanner detects host GPU processes and attributes them to a user via /proc"
    - "Scanner persists unified inventory to /var/lib/ds01/workload-inventory.json"
    - "Scanner emits events via ds01_events on state transitions (new workload, exited workload)"
    - "Transient GPU processes (< 2 scans) do not generate events"
    - "System GPU processes (nvidia-persistenced, DCGM, Xorg) are excluded from user-facing inventory"
    - "Inventory reflects near-real-time state (max 30s lag from systemd timer polling interval, within 60s detection success window)"
  artifacts:
    - path: "scripts/monitoring/detect-workloads.py"
      provides: "Core workload detection scanner"
      exports: ["main"]
      min_lines: 200
  key_links:
    - from: "scripts/monitoring/detect-workloads.py"
      to: "scripts/lib/ds01_events.py"
      via: "import log_event"
      pattern: "from ds01_events import log_event"
    - from: "scripts/monitoring/detect-workloads.py"
      to: "/var/lib/ds01/workload-inventory.json"
      via: "JSON read/write"
      pattern: "workload-inventory\\.json"
    - from: "scripts/monitoring/detect-workloads.py"
      to: "docker Python SDK"
      via: "import docker"
      pattern: "docker\\.from_env"
---

<objective>
Create the core workload detection scanner that discovers all GPU workloads on the system.

Purpose: This is the detection engine — the heart of Phase 2. It scans Docker API for all containers (classifying by origin), queries nvidia-smi + /proc for host GPU processes, persists state to an inventory file, and emits events on transitions. It must run as a oneshot script suitable for systemd timer invocation.

Note: The inventory is near-real-time, not real-time. The systemd timer polls every 30 seconds, so the inventory may be up to 30 seconds stale. This is acceptable for the detection success criteria (new workloads detected within 60 seconds). "Real-time inventory" (DETECT-04) means "current state at last scan" with max 30s lag.

Output: `scripts/monitoring/detect-workloads.py` — a standalone Python script that performs one complete scan cycle when executed.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-awareness-layer/02-CONTEXT.md
@.planning/phases/02-awareness-layer/02-RESEARCH.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/CONVENTIONS.md
@scripts/lib/ds01_events.py
@scripts/docker/docker-wrapper.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Container detection, classification, and GPU access check</name>
  <files>scripts/monitoring/detect-workloads.py</files>
  <action>
Create `scripts/monitoring/detect-workloads.py` with the following structure:

**File structure:**
- Shebang `#!/usr/bin/env python3`, module docstring, `from __future__ import annotations`
- Imports: `docker`, `subprocess`, `json`, `sys`, `logging` from stdlib, `Path` from pathlib
- Add `scripts/lib` to `sys.path` for `ds01_events` import (follow existing pattern from gpu_allocator_v2.py)
- Safe import of `log_event` with try/except fallback to no-op (follow pattern from 01-06)

**Constants:**
- `INVENTORY_FILE = Path("/var/lib/ds01/workload-inventory.json")`
- `SYSTEM_GPU_PROCESSES`: set of known system process names to exclude: `{"nvidia-persistenced", "nv-hostengine", "dcgm", "dcgmi", "nvidia-smi", "Xorg", "X"}`
- `SCAN_TIMEOUT = 25` seconds (must be shorter than 30s timer interval)

**Container detection functions:**

`classify_container(container) -> str`:
- Priority 1: Check for `ds01.managed` label -> return `"ds01-managed"`
- Priority 2: Check for `devcontainer.*` labels (any key starting with `devcontainer.`) -> return `"devcontainer"`
- Priority 3: Check for `com.docker.compose.project` label -> return `"compose"`
- Priority 4: Check container name starts with `vsc-` -> return `"devcontainer"`
- Priority 5: Default -> return `"raw-docker"`

`has_gpu_access(container) -> bool`:
- Check `HostConfig.Runtime == "nvidia"`
- Check `HostConfig.DeviceRequests` for any entry with `"nvidia"` in capabilities (nested list — capabilities is list of list of strings)
- Check `HostConfig.Devices` for paths containing `"nvidia"`
- Return True if any check passes

`get_container_user(container) -> str`:
- Check labels in priority order: `ds01.user`, `aime.mlc.USER`
- For devcontainers: parse `devcontainer.local_folder` label to extract username from path (e.g., `/home/alice/project` -> `alice`)
- Fallback: try to get process owner of container PID via `/proc/{pid}/status` Uid field + getent
- Final fallback: return `"unknown"`

`get_container_gpu_devices(container) -> list[str]`:
- Parse `ds01.gpu.allocated` label if present (format: `"0:1"` or `"0"`)
- Otherwise check DeviceRequests for device IDs
- Return list of device strings, empty list if no GPU

`scan_containers(client) -> dict`:
- Call `client.containers.list(all=True)` to get ALL containers (including stopped)
- For each container, build entry dict with: `id` (first 12 chars), `name`, `origin` (from classify), `user` (from get_container_user), `has_gpu` (from has_gpu_access), `gpu_devices`, `status` (container.status: running/exited/created/etc), `image` (container.image.tags[0] if available, else image.id[:12])
- Return dict keyed by container ID (first 12 chars)
- Wrap in try/except for docker.errors.DockerException — log warning, return empty dict

**Host GPU process detection functions:**

`is_container_process(pid: int) -> bool`:
- Read `/proc/{pid}/cgroup` and check for `docker` or `containerd` in content
- Return True if containerised, False if host process
- Handle FileNotFoundError/PermissionError gracefully (process may have exited)

`get_process_user(pid: int) -> str`:
- Read `/proc/{pid}/status`, find `Uid:` line, extract first (real) UID
- Resolve UID to username via `getent passwd {uid}` subprocess call with 1s timeout
- Return username string or `"unknown"` on any failure

`get_process_cmdline(pid: int) -> str`:
- Read `/proc/{pid}/cmdline` as BYTES (critical: null-byte separators)
- Split on `b"\0"`, decode each part as utf-8 with errors='replace', join with spaces
- Return command string or `""` on failure

`scan_host_gpu_processes() -> dict`:
- Run `nvidia-smi --query-compute-apps=pid,used_memory,gpu_uuid --format=csv,noheader,nounits` with 5s timeout
- Parse CSV output into list of (pid, gpu_memory_mb, gpu_uuid)
- For each PID:
  - Skip if `is_container_process(pid)` returns True
  - Get user via `get_process_user(pid)`
  - Get cmdline via `get_process_cmdline(pid)`
  - Get process name from cmdline (first element)
  - Skip if process name (basename) is in `SYSTEM_GPU_PROCESSES`
  - Build entry: `pid`, `user`, `cmdline`, `gpu_memory_mb`, `gpu_uuid`
- Return dict keyed by str(pid)
- Handle subprocess.TimeoutExpired and other errors — log warning, return empty dict

**Main scan function:**

`run_scan() -> dict`:
- Create docker client via `docker.from_env()`
- Call `scan_containers(client)`
- Call `scan_host_gpu_processes()`
- Build inventory dict: `{"last_scan": <UTC ISO timestamp>, "containers": <containers>, "host_processes": <processes>}`
- Return inventory

This task creates the file with detection logic only. Inventory persistence and event emission added in Task 2.

Follow DS01 Python conventions: type hints on all public functions, Google-style docstrings, `main()` entry point with argparse (add `--dry-run` flag that prints inventory to stdout without saving).
  </action>
  <verify>
Run `python3 -c "import ast; ast.parse(open('scripts/monitoring/detect-workloads.py').read()); print('Syntax OK')"` to verify no syntax errors.

Run `ruff check scripts/monitoring/detect-workloads.py` to verify linting passes.

Verify the file has all required functions: `classify_container`, `has_gpu_access`, `get_container_user`, `scan_containers`, `is_container_process`, `get_process_user`, `get_process_cmdline`, `scan_host_gpu_processes`, `run_scan`.
  </verify>
  <done>
`scripts/monitoring/detect-workloads.py` exists with complete container detection (classify by origin, GPU access check, user attribution) and host GPU process detection (nvidia-smi + /proc attribution, system process exclusion). All functions have type hints and docstrings. Syntax and linting pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Inventory persistence, state diffing, event emission, and transient filtering</name>
  <files>scripts/monitoring/detect-workloads.py</files>
  <action>
Add inventory persistence and event emission to the scanner created in Task 1.

**Inventory persistence functions:**

`load_inventory() -> dict`:
- Read `INVENTORY_FILE` if it exists, parse JSON
- Return parsed dict or default: `{"last_scan": None, "containers": {}, "host_processes": {}, "_pending_processes": {}}`
- `_pending_processes` tracks processes seen only once (transient filter)
- Handle JSONDecodeError gracefully (return default, log warning)

`save_inventory(inventory: dict) -> None`:
- Write to `INVENTORY_FILE` with `json.dumps(indent=2)`
- Ensure parent directory exists (`INVENTORY_FILE.parent.mkdir(parents=True, exist_ok=True)`)
- Write atomically: write to `.tmp` file first, then `os.rename()` to final path
- Handle OSError (log warning, don't crash)

**State diffing and event emission:**

`detect_transitions(old: dict, new: dict) -> None`:
- Compare old and new container dicts:
  - New containers (in new but not old): emit `detection.container_discovered` event with details: name, origin, has_gpu, user, image
  - Exited containers (in old but not new): emit `detection.container_exited` event with details: name, origin, user
  - Status changes (same container, different status): emit `detection.container_status_changed` with old_status, new_status
- Compare old and new host_processes dicts:
  - New processes: emit `detection.host_gpu_process_discovered` with user, cmdline, gpu_memory_mb
  - Exited processes: emit `detection.host_gpu_process_exited` with user, cmdline
- All events use the Phase 1 log_event() function with appropriate user attribution

**Transient process filtering:**

Implement 2-scan persistence for host GPU processes to avoid event noise:

- When a new host GPU process is detected (PID not in old inventory):
  - Add to `_pending_processes` dict (not to main `host_processes`)
  - Do NOT emit event yet
- On next scan, if PID is still present AND was in `_pending_processes`:
  - Move from `_pending_processes` to `host_processes`
  - NOW emit `detection.host_gpu_process_discovered` event
- If PID was in `_pending_processes` but is now gone:
  - Remove from `_pending_processes` silently (transient, no event)
- If PID was in `host_processes` and is now gone:
  - Remove from `host_processes`
  - Emit `detection.host_gpu_process_exited` event

Container processes do NOT need transient filtering (containers are stable, not transient).

**Timestamp tracking for inventory entries:**

Add `detected_at` field (UTC ISO timestamp) when a container or process is first detected. Preserve this field across scans (don't overwrite with new scan data — copy from old inventory entry if same ID exists).

**Update main() function:**

The `main()` function should:
1. Parse args (`--dry-run` prints to stdout, `--verbose` enables debug logging)
2. Load previous inventory
3. Run scan (get new inventory)
4. Apply transient filtering for host processes
5. Detect transitions and emit events (skip if `--dry-run`)
6. Merge `detected_at` timestamps from old inventory
7. Save new inventory (skip if `--dry-run`)
8. Log summary to stderr: "Scan complete: {n} containers ({m} with GPU), {p} host GPU processes"

**Make executable:** Add `chmod +x` — the systemd service will call this directly.
  </action>
  <verify>
Run `python3 -c "import ast; ast.parse(open('scripts/monitoring/detect-workloads.py').read()); print('Syntax OK')"`.

Run `ruff format scripts/monitoring/detect-workloads.py && ruff check --fix scripts/monitoring/detect-workloads.py`.

Verify the file has persistence functions: `load_inventory`, `save_inventory`, `detect_transitions`.

Run `python3 scripts/monitoring/detect-workloads.py --help` to verify argparse works (should show help text without errors, even if docker isn't available in this context).
  </verify>
  <done>
`detect-workloads.py` is complete with: inventory persistence (atomic JSON writes), state diffing (container and process transitions), event emission via ds01_events, transient process filtering (2-scan threshold), and detected_at timestamp tracking. The scanner can be invoked as a oneshot script suitable for systemd timer.
  </done>
</task>

</tasks>

<verification>
1. `python3 scripts/monitoring/detect-workloads.py --help` shows usage without errors
2. `python3 scripts/monitoring/detect-workloads.py --dry-run` performs a scan and prints inventory to stdout (requires docker access)
3. `ruff check scripts/monitoring/detect-workloads.py` passes with no errors
4. File is executable (`ls -la scripts/monitoring/detect-workloads.py` shows execute bit)
5. Verify event types used match Phase 1 schema (detection.container_discovered, detection.container_exited, etc.)
</verification>

<success_criteria>
- Scanner detects all containers (running and stopped) and classifies each by origin
- Scanner detects host GPU processes via nvidia-smi and attributes to users via /proc
- System GPU processes (nvidia-persistenced, DCGM, Xorg) excluded
- Inventory persisted atomically to /var/lib/ds01/workload-inventory.json
- Inventory is near-real-time: reflects state at last scan with max 30s lag (acceptable for 60s detection window)
- Events emitted only for confirmed state transitions (not transient processes)
- Script runs as oneshot (no daemon loop), exits cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/02-awareness-layer/02-01-SUMMARY.md`
</output>
