---
phase: 02.1-gpu-access-control-research
plan: 02
subsystem: access-control
tags: [gpu, cuda, profile.d, video-group, exemptions, phase-3-revision]

# Dependency graph
requires:
  - phase: 02.1-01
    provides: Research-informed design document for GPU access control
provides:
  - Updated ds01-gpu-awareness.sh with video group exemption logic
  - Revised 03-03 deployment plan removing device permission anti-patterns
  - Research-aligned Phase 3 plans consistent with three-layer architecture
affects: [03-03-deployment, phase-3-execution]

# Tech tracking
tech-stack:
  added: []
  patterns:
    - "Profile.d exemption checking via group membership"
    - "Interactive shell filtering for profile.d scripts"

key-files:
  created: []
  modified:
    - config/deploy/profile.d/ds01-gpu-awareness.sh
    - .planning/phases/03-access-control/03-03-PLAN.md

key-decisions:
  - "Video group membership + profile.d exemption check is Layer 3 exemption mechanism"
  - "Profile.d scripts must check for interactive shell context to avoid breaking system services"
  - "Video group sync simplified - only add exempt users, bare-metal-access revoke handles removal"
  - "03-03 plan removes all udev rule deployment and device permission enforcement"

patterns-established:
  - "Profile.d scripts use 'return' not 'exit' (sourced, not executed)"
  - "Non-interactive shell check: [[ $- == *i* ]] || return 0"
  - "Group membership check: groups | grep -qw groupname"

# Metrics
duration: 3min
completed: 2026-01-31
---

# Phase 2.1 Plan 02: GPU Access Control Plan Revision Summary

**Profile.d exemption logic for video group and revised Phase 3 deployment plan without device permission anti-patterns**

## Performance

- **Duration:** 3 min
- **Started:** 2026-01-31T21:08:46Z
- **Completed:** 2026-01-31T21:12:00Z
- **Tasks:** 2
- **Files modified:** 2

## Accomplishments
- Updated ds01-gpu-awareness.sh to check video group membership and skip CUDA_VISIBLE_DEVICES for exempt users
- Added interactive shell check to avoid affecting system services and cron jobs
- Completely revised 03-03-PLAN.md to remove udev rules and device permission enforcement
- Aligned Phase 3 deployment with research-informed three-layer architecture

## Task Commits

Each task was committed atomically:

1. **Task 1: Update ds01-gpu-awareness.sh with video group exemption logic** - `811d260` (feat)
2. **Task 2: Rewrite 03-03-PLAN.md to remove device permission anti-patterns** - `9460c3e` (refactor)

## Files Created/Modified

**config/deploy/profile.d/ds01-gpu-awareness.sh**
- Added interactive shell check (`[[ $- == *i* ]]`) to skip non-interactive contexts
- Added video group membership check before setting CUDA_VISIBLE_DEVICES
- Users in video group (bare-metal access granted) bypass GPU hiding
- Uses `return` not `exit` (sourced script)

**.planning/phases/03-access-control/03-03-PLAN.md**
- Removed config/deploy/udev/99-ds01-nvidia.rules from files_modified
- Added config/deploy/profile.d/ds01-gpu-awareness.sh to files_modified
- Removed all udev rule deployment sections
- Removed device permission enforcement (MODE="0660")
- Simplified video group sync to only add exempt users (no removal logic)
- Added profile.d deployment of updated ds01-gpu-awareness.sh
- Added architecture context referencing 02.1-DESIGN.md
- Clarified nvidia-* wrappers are UX tools, not security enforcement
- Clarified CUDA_VISIBLE_DEVICES is Layer 1 deterrent, Docker device mapping is Layer 2 security boundary
- Updated verification sections to check NO udev rules deployed, device permissions remain 0666

## Decisions Made

**1. Profile.d exemption via video group is the Layer 3 mechanism**
- Rationale: Without udev rules enforcing device permissions 0660, video group membership alone doesn't control GPU access. The profile.d script checks video group to decide whether to set CUDA_VISIBLE_DEVICES. This is the actual exemption mechanism.

**2. Interactive shell check prevents breaking system services**
- Rationale: Non-interactive shells (cron jobs, systemd services, scripts run by root) should not have CUDA_VISIBLE_DEVICES forced on them. This would break monitoring tools, system automation, and admin operations.

**3. Simplified video group sync in deploy.sh**
- Rationale: No need to remove non-exempt users from video group during sync. Without udev rules, video group membership is harmless unless combined with profile.d exemption. Users with temporary grants will be removed by `bare-metal-access revoke` when grants expire.

**4. Complete removal of udev rules and device permission enforcement from 03-03**
- Rationale: Research findings (02.1-RESEARCH.md, 02.1-DESIGN.md) identified device permission manipulation as anti-pattern. Breaks nvidia-smi, conflicts with NVIDIA driver, fragile race conditions. CUDA_VISIBLE_DEVICES + Docker device mapping is the proven HPC pattern.

## Deviations from Plan

None - plan executed exactly as written.

## Issues Encountered

None

## Next Phase Readiness

Phase 3 plan 03-03 is now research-aligned and ready for execution:
- Removes fragile udev rule deployment
- Uses profile.d exemption logic for video group members
- Device permissions remain at defaults (0666)
- Deployment integrates all three layers of GPU access control architecture

**Blockers:** None

**Dependencies satisfied:**
- Plan 02.1-01 design document provided architecture and revision guidance
- 03-01 and 03-02 remain valid (no changes needed per design audit)

**Next step:** Execute revised 03-03 plan to deploy access control components with correct architecture

---
*Phase: 02.1-gpu-access-control-research*
*Completed: 2026-01-31*
