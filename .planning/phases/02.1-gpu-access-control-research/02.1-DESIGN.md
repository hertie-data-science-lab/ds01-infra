# Phase 2.1: GPU Access Control Design Decision

**Decision Date:** 2026-01-31
**Context:** DS01 Phase 3 implementation (udev + device permissions) broke GPU allocation pipeline after three debug attempts. Research into HPC/container best practices conducted to ground architecture in proven patterns.

## Executive Summary

DS01 adopts a **three-layer GPU access control architecture** combining CUDA_VISIBLE_DEVICES environment variable for host-level deterrence, Docker device mapping for kernel-level container isolation, and video group membership for opt-in bare-metal exemptions. This approach aligns with HPC standards (SLURM, Kubernetes) and resolves the Phase 3 failures caused by device permission manipulation. The design validates Phase 3 plans 03-01 and 03-02 as-is, but requires major revision of 03-03 to remove udev rules and device permission enforcement.

## Design Decision: Layered GPU Access Control

**Chosen approach:** Layered GPU Access Control using CUDA_VISIBLE_DEVICES for host-level hiding + Docker device mapping for container-level enforcement + video group for opt-in exemptions.

This is Pattern 1 from the research findings (02.1-RESEARCH.md).

## Why This Approach

1. **CUDA_VISIBLE_DEVICES="" is the HPC standard** — SLURM, PBS, LSF all use environment variable isolation for host-level GPU hiding. Proven at scale across thousands of HPC installations.

2. **Container-level enforcement is the security boundary** — Docker `--gpus device=UUID` provides kernel-level access control via cgroups. NVIDIA Container Toolkit handles device mounting, capability setup, and environment variable injection automatically.

3. **Device permissions stay at defaults (0666)** — No manipulation needed. nvidia-smi and monitoring tools work without restriction. NVIDIA driver parameters don't interfere. nvidia-persistenced has no conflicts.

4. **Simple to deploy, simple to understand** — Profile.d script exports one environment variable. Docker wrapper adds one flag. Video group is standard Linux convention.

5. **Proven at scale** — Kubernetes GPU Operator, SLURM GPU allocation, and HPC centres worldwide use this pattern. Not experimental.

## The Three Layers

### Layer 1: Host-Level Deterrent (CUDA_VISIBLE_DEVICES="")

**Purpose:** UX layer preventing accidental bare-metal GPU usage

**Mechanism:** `/etc/profile.d/ds01-gpu-awareness.sh` exports `CUDA_VISIBLE_DEVICES=""`

**Effect:**
- `torch.cuda.is_available()` returns False
- CUDA applications see zero GPUs
- nvidia-smi still works (doesn't use CUDA runtime)
- Phase 2 awareness layer detects bare-metal GPU processes

**Limitation:** NOT a security boundary — users can override via `os.environ['CUDA_VISIBLE_DEVICES'] = '0'` before importing torch. This is acceptable because:
- Container enforcement is the actual security layer
- Phase 2 awareness layer detects bare-metal GPU processes regardless
- UX barrier prevents accidental violations
- Determined users monitored, not prevented

**Status:** DEPLOYED (Phase 2 awareness layer, config/deploy/profile.d/ds01-gpu-awareness.sh)

### Layer 2: Container Enforcement (Docker device mapping)

**Purpose:** Actual security boundary — kernel-level GPU access control

**Mechanism:** Docker `--gpus device=UUID` maps specific GPU into container via cgroups devices controller. NVIDIA Container Toolkit translates UUID to device files (/dev/nvidia0, /dev/nvidiactl, etc.) and sets CUDA_VISIBLE_DEVICES inside container.

**Effect:**
- Container sees only allocated GPU(s)
- Cannot access other GPUs (kernel blocks device access)
- CUDA enumeration returns correct device count
- Memory and fault isolation (especially with MIG instances)

**Limitation:** Requires Docker — not applicable to bare-metal processes. This is the intended architecture: containers are the enforcement point.

**Status:** DEPLOYED (scripts/docker/docker-wrapper.sh + gpu_allocator_v2.py)

**Current implementation:**
- gpu_allocator_v2.py tracks allocations via Docker labels (ds01.gpu.allocated)
- mlc-patched.py adds `--gpus device=UUID` to container creation
- Docker wrapper intercepts all container operations
- Allocations survive container stop (gpu_hold_after_stop timeout)

### Layer 3: Exemption Management (grant files + config)

**Purpose:** Opt-in bare-metal GPU access for approved users

**Mechanism:** Grant files (`/var/lib/ds01/bare-metal-grants/<user>.json`) and config (`resource-limits.yaml bare_metal_access.exempt_users`). Profile.d script checks these on every shell start — no re-login needed.

**Effect:**
- Exempted users bypass Layer 1 (CUDA_VISIBLE_DEVICES="" not set)
- Non-exempted users deterred by Layer 1
- Exemptions explicit and auditable (grant files + config)
- Video group is separate — all docker users get it for nvidia-smi query access

**Limitation:** Exempted users bypass Layer 1 deterrent. They must be monitored via Phase 2 awareness layer to prevent conflicts with container GPU allocation.

**Status:** DEPLOYED
- bare-metal-access CLI (grant/revoke/status)
- Profile.d checks grant files and exempt_users config
- Video group managed separately by add-user-to-docker.sh (all docker users)

**Implementation note (divergence from original design):** The original design proposed video group membership as the exemption check. The implementation uses grant files + config exempt_users instead, which is better because:
- Grant file changes take effect on next shell start (no re-login needed)
- Temporary grants auto-expire via `at` scheduler
- Video group stays simple: all docker users get it for nvidia-smi

## Rejected Alternatives

### Alternative 1: Device file permissions (0660, video group as default enforcement)

**What:** Restrict `/dev/nvidia*` to mode 0660, group video. Non-video-group users cannot access device files.

**Why considered:** Standard Linux device access control pattern. Appears to provide kernel-level enforcement.

**Why rejected:**
- **Breaks nvidia-smi:** nvidia-smi requires read access to `/dev/nvidiactl` and `/dev/nvidia-uvm` for all operations, even listing GPUs. No read-only query mode exists.
- **Breaks DCGM and monitoring:** System monitoring tools fail when device permissions restricted.
- **NVIDIA driver overrides:** Driver kernel module parameters (`NVreg_DeviceFileGID`, `NVreg_DeviceFileMode`) override udev rules. Requires modprobe.d configuration and initramfs rebuild.
- **Race conditions:** nvidia-persistenced may recreate devices with default permissions.
- **DS01 experienced this failure directly:** Three debug sessions (gpu-alloc-silent-exit-after-awareness-layer.md) attempted workarounds. All failed or became fragile.

**Research evidence:** HPC centres explicitly avoid device permission manipulation. Anti-pattern per research findings.

### Alternative 2: udev rules for device permission enforcement

**What:** Use udev rules (`99-ds01-nvidia.rules`) to set `/dev/nvidia*` permissions to 0660, video group.

**Why considered:** Appears cleaner than modprobe.d kernel parameters. Automatic on device creation.

**Why rejected:**
- **NVIDIA driver parameters override udev rules:** Even when udev rules trigger, driver recreates devices with module parameter settings.
- **Race conditions on boot:** Timing between udev trigger and nvidia-persistenced device creation.
- **Same problems as Alternative 1:** Still breaks nvidia-smi and monitoring tools.
- **Plan 03-03 originally included this:** Must be removed.

**Research evidence:** Forums and mailing lists document udev rule failures with NVIDIA devices. Kernel module parameters take precedence.

### Alternative 3: Hand-rolled cgroups v2 BPF device controller

**What:** Implement custom BPF programs for cgroups v2 device access control to restrict GPU device access per-user or per-process.

**Why considered:** Modern replacement for deprecated cgroups v1 devices controller. Kernel-level enforcement.

**Why rejected:**
- **Complex and fragile:** Requires BPF programming, cgroup hierarchy management, systemd integration.
- **Not portable:** BPF device controller implementation varies across kernel versions.
- **Docker already handles this:** NVIDIA Container Toolkit + Docker device mapping transparently manage cgroups for containers. Reimplementing is error-prone.
- **"Don't hand-roll":** Research recommendation — let Docker/NVIDIA Container Toolkit handle cgroups.

**Research evidence:** SLURM cgroups v2 plugin documents complexity. Kubernetes uses device plugins, not hand-rolled BPF.

### Alternative 4: nvidia-* command wrappers as primary security enforcement

**What:** Wrap nvidia-smi and other nvidia-* commands to block non-authorized users. Rely on wrapper as security boundary.

**Why considered:** Already implemented in Plan 03-01. Simple to understand.

**Why rejected AS SECURITY:**
- **Wrapping nvidia-smi doesn't prevent CUDA access:** CUDA libraries use `/dev/nvidia*` directly, not nvidia-smi. User can run arbitrary CUDA code without ever invoking nvidia-smi.
- **Bypass via direct device access:** `python -c "import torch; torch.cuda.is_available()"` doesn't call nvidia-smi.
- **Not a security boundary:** Wrappers are a UX layer, not kernel enforcement.

**REMOVED:** nvidia-wrapper.sh was deployed but broke the GPU allocation chain (nvidia-smi is required by allocator, monitors, and dashboard). Wrapper removed; nvidia-smi access ensured via device permissions 0666 for all users.

## Audit of Existing Phase 3 Implementation

### Plan 03-01 (Bare metal restriction) — VALID, KEEP

**What was implemented:**
- nvidia-wrapper.sh: Wrapper for nvidia-* commands with video group check
- bare-metal-access CLI: Admin tool for temporary/permanent grants
- resource-limits.yaml config: bare_metal_access and access_control sections
- Rate-limited denial logging

**Alignment with research:**
- ✅ **nvidia-wrapper as UX tool:** Valid — provides contextual error messages directing users to containers
- ✅ **Video group for exemptions:** Valid — opt-in bare-metal access for approved users
- ✅ **Rate limiting:** Valid — prevents log flooding
- ✅ **Temporary grants via at command:** Valid — POSIX standard, simpler than systemd timers

**Caveats:**
- nvidia-wrapper checks video group membership, but the actual bare-metal deterrent is CUDA_VISIBLE_DEVICES (Layer 1).
- If user is NOT in video group AND overrides CUDA_VISIBLE_DEVICES, they CAN still access GPUs on bare metal.
- nvidia-wrapper only intercepts nvidia-* commands, not arbitrary CUDA programs.
- **This is acceptable:** Phase 2 awareness layer detects bare-metal GPU processes regardless of how they were launched.

**Conclusion:** Keep Plan 03-01 implementation. nvidia-wrapper is a UX enhancement, not security enforcement.

### Plan 03-02 (Container isolation) — VALID, KEEP, INDEPENDENT

**What was implemented:**
- Docker wrapper container isolation: `filter_container_list()` for docker ps filtering, `verify_container_ownership()` for operation authorization
- Admin bypass: root, datasciencelab, ds01-admin group
- Rate-limited denial logging
- Fail-open modes: monitoring, disabled, emergency bypass

**Alignment with research:**
- ✅ **Container isolation independent of GPU access control:** Docker wrapper enforces user-to-container ownership, not GPU allocation.
- ✅ **Label-based filtering:** Efficient, leverages Docker daemon.
- ✅ **Fail-open safety:** Monitoring mode for rollout, bypass for emergencies.

**No GPU access control changes needed:** Container isolation is orthogonal to GPU allocation.

**Conclusion:** Keep Plan 03-02 implementation unchanged.

### Plan 03-03 (Deployment integration) — NEEDS MAJOR REVISION

**Original plan:**
- Deploy udev rules (99-ds01-nvidia.rules) to restrict /dev/nvidia* to mode 0660, video group
- Deploy nvidia-* wrappers to /usr/local/bin
- Deploy bare-metal-access CLI
- Sync video group membership from resource-limits.yaml exempt_users
- Remove non-exempt users from video group
- Deploy MOTD
- Install at/atd prerequisite

**Problems with original plan:**
- ❌ **udev rules:** Anti-pattern per research. Breaks nvidia-smi. NVIDIA driver overrides. Remove entirely.
- ❌ **Device permission enforcement (0660):** Conflicts with GPU allocator. Remove.
- ⚠️ **Video group sync:** Current plan removes non-exempt users from video group. This implies video group membership alone grants access, but without udev rules enforcing 0660, video group membership doesn't control device access. The actual mechanism is CUDA_VISIBLE_DEVICES.

**Required changes for 03-03:**

1. **REMOVE:** udev rules file creation (config/deploy/udev/99-ds01-nvidia.rules)
2. **REMOVE:** udev rule deployment from deploy.sh (no `cp ... /etc/udev/rules.d/`)
3. **REMOVE:** udevadm reload and trigger commands
4. **REMOVE:** All references to MODE="0660" device permissions
5. **REVISE:** Video group sync becomes simpler — just ensure exempt users are in video group. No need to remove non-exempt users because video group membership alone doesn't grant GPU access without profile.d exemption.
6. **ADD:** Update ds01-gpu-awareness.sh to check video group membership — if user is in video group, don't export CUDA_VISIBLE_DEVICES="" (allow bare-metal access). This is the actual enforcement mechanism for exemptions.
7. **KEEP:** at/atd prerequisite (required for temporary grant auto-revocation)
8. **KEEP:** nvidia-* wrapper deployment (UX tool)
9. **KEEP:** bare-metal-access deployment (admin CLI)
10. **KEEP:** MOTD deployment

**Simplified video group sync logic:**
```bash
# Add exempt users to video group (for profile.d exemption)
for user in $EXEMPT_USERS; do
    if id "$user" &>/dev/null; then
        if ! groups "$user" 2>/dev/null | grep -q '\bvideo\b'; then
            usermod -aG video "$user"
        fi
    fi
done

# NOTE: We don't remove non-exempt users from video group because video group
# alone doesn't grant GPU access. The actual mechanism is profile.d exemption.
# Users with active temporary grants stay in video group until revocation.
```

**Profile.d update (new file or update existing):**
```bash
#!/bin/bash
# DS01 GPU Awareness Layer
# Skip CUDA_VISIBLE_DEVICES="" for video group members (bare-metal access exemption)
if groups | grep -qw video; then
    return  # User exempt — allow bare-metal GPU access
fi
export CUDA_VISIBLE_DEVICES=""
```

**Conclusion:** Plan 03-03 requires major revision to remove udev/device-permission enforcement and add profile.d exemption logic.

## What Needs to Change in 03-03

Specific changes for updated 03-03-PLAN.md:

### Remove (Anti-patterns):
- ❌ config/deploy/udev/99-ds01-nvidia.rules file
- ❌ udev rule deployment section in deploy.sh
- ❌ udevadm control --reload-rules and udevadm trigger commands
- ❌ All references to MODE="0660" or device permissions
- ❌ Logic to "remove non-exempt users from video group" (video group membership is for exemptions only, not default enforcement)

### Revise (Simplify):
- ✏️ Video group sync: Just ADD exempt users to video group. Don't remove others (unless they have no active temporary grants AND we want to clean up).
- ✏️ Temporary grant preservation: Keep users with active grants in video group (current logic correct, just remove device permission rationale).

### Add (Missing exemption mechanism):
- ➕ Update config/deploy/profile.d/ds01-gpu-awareness.sh to check video group membership
- ➕ If user is in video group: don't set CUDA_VISIBLE_DEVICES="" (allow bare-metal)
- ➕ If user NOT in video group: set CUDA_VISIBLE_DEVICES="" (hide GPUs)
- ➕ Profile.d exemption logic is the ACTUAL bare-metal access control, not device permissions

### Keep (Valid components):
- ✅ at/atd prerequisite check and installation
- ✅ nvidia-* wrapper deployment (UX tool)
- ✅ bare-metal-access CLI deployment
- ✅ MOTD deployment
- ✅ State directory creation (/var/lib/ds01/bare-metal-grants, /var/lib/ds01/rate-limits)
- ✅ Video group sync for exempt users (but simplified — no removal logic needed)

## Open Items

### 1. CVE-2025-23266: NVIDIA Container Toolkit Privilege Escalation

**What:** Container escape vulnerability in NVIDIA Container Toolkit versions < 1.16.2.

**Impact:** Containers with GPU access may escalate to host root.

**Mitigation:** Upgrade to nvidia-container-toolkit >= 1.16.2 or apply config.toml workaround.

**Action required:** Verify DS01 server version before Phase 3 deployment.
```bash
nvidia-ctk --version  # Should be >= 1.16.2
```

### 2. Profile.d Exemption Logic

**What:** ds01-gpu-awareness.sh currently sets CUDA_VISIBLE_DEVICES="" for all users. Video group members should be exempt.

**Impact:** Users with bare-metal access grants (video group) currently still get CUDA_VISIBLE_DEVICES="", defeating the exemption.

**Action required:** Update profile.d script in 03-03 revised plan.

### 3. CUDA_VISIBLE_DEVICES is NOT a Security Boundary

**What:** Environment variables can be overridden by user code before CUDA initialization.

**Impact:** Determined users can bypass Layer 1 deterrent by setting `os.environ['CUDA_VISIBLE_DEVICES'] = '0'` before importing torch.

**Mitigation:** Documented and accepted. Container enforcement (Layer 2) is the security boundary. Phase 2 awareness layer detects bare-metal GPU processes regardless.

**Action required:** None — this is by design.

### 4. Video Group Semantics Change

**Original assumption (Plan 03-03):** Video group + device permissions = enforcement.

**New understanding:** Video group + profile.d exemption = opt-in bare-metal access.

**Impact:** Video group membership alone doesn't grant or deny GPU access. Profile.d script must check video group to implement exemption.

**Action required:** Update 03-03 plan and deployment logic.

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│ Host (DS01 Server)                                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  User Login                                                     │
│      ↓                                                          │
│  /etc/profile.d/ds01-gpu-awareness.sh                           │
│      ↓                                                          │
│  Check: grant file exists OR user in exempt_users config?       │
│      YES → Allow bare-metal (skip CUDA_VISIBLE_DEVICES)         │
│      NO  → Set CUDA_VISIBLE_DEVICES="" (Layer 1: Deterrent)     │
│                                                                 │
│  User attempts bare-metal GPU usage:                            │
│      python -c "import torch; torch.cuda.is_available()"        │
│      ↓                                                          │
│  Layer 1 (if not exempt): Returns False (no GPUs visible)       │
│      ↓                                                          │
│  Phase 2 Awareness Layer: Detects if user overrides and uses GPU│
│                                                                 │
│  User runs nvidia-smi:                                          │
│      ↓                                                          │
│  /usr/bin/nvidia-smi (direct, device perms 0666)                │
│      → All users can query GPU state (required for allocation)  │
│                                                                 │
│  User runs container deploy:                                    │
│      ↓                                                          │
│  gpu_allocator_v2.py: Allocate GPU (track via Docker labels)    │
│      ↓                                                          │
│  Docker run --gpus device=GPU-UUID-abc123 ...                   │
│      ↓                                                          │
│  Layer 2: Kernel cgroups device controller                      │
│      → Container sees ONLY allocated GPU                        │
│      → CUDA_VISIBLE_DEVICES set by NVIDIA Container Toolkit     │
│      → Security boundary (cannot escape to other GPUs)          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## References

- **Research document:** .planning/phases/02.1-gpu-access-control-research/02.1-RESEARCH.md
- **Phase 3 summaries:** 03-01-SUMMARY.md (bare metal restriction), 03-02-SUMMARY.md (container isolation)
- **Current profile.d:** config/deploy/profile.d/ds01-gpu-awareness.sh
- **GPU allocator:** scripts/docker/gpu_allocator_v2.py
- **Docker wrapper:** scripts/docker/docker-wrapper.sh

## Conclusion

The three-layer architecture (CUDA_VISIBLE_DEVICES + Docker device mapping + video group exemptions) aligns with HPC best practices and resolves Phase 3's device permission failures. Plans 03-01 and 03-02 are valid and should remain unchanged. Plan 03-03 requires major revision to remove udev rules and device permission enforcement, replacing them with profile.d exemption logic for video group members.

This design is production-ready and proven at scale.
