---
phase: 02.1-gpu-access-control-research
plan: 02
type: execute
wave: 2
depends_on: ["02.1-01"]
files_modified:
  - .planning/phases/03-access-control/03-03-PLAN.md
  - config/deploy/profile.d/ds01-gpu-awareness.sh
autonomous: true

must_haves:
  truths:
    - "03-03-PLAN.md no longer references udev rules or device permission manipulation"
    - "03-03-PLAN.md no longer creates 99-ds01-nvidia.rules"
    - "03-03-PLAN.md video group sync adds exempt users only (does not remove non-exempt users from video group based on device permissions)"
    - "03-03-PLAN.md includes profile.d exemption logic (video group members skip CUDA_VISIBLE_DEVICES)"
    - "03-03-PLAN.md deployment section deploys updated ds01-gpu-awareness.sh with exemption check"
    - "ds01-gpu-awareness.sh checks video group membership before setting CUDA_VISIBLE_DEVICES"
    - "Phase 3 plans are consistent with design document decisions"
  artifacts:
    - path: ".planning/phases/03-access-control/03-03-PLAN.md"
      provides: "Updated deployment plan without device permission anti-patterns"
      contains: "ds01-gpu-awareness.sh"
    - path: "config/deploy/profile.d/ds01-gpu-awareness.sh"
      provides: "Updated GPU awareness script with video group exemption"
      contains: "video"
  key_links:
    - from: "03-03-PLAN.md"
      to: "02.1-DESIGN.md"
      via: "Plan revised based on design document"
      pattern: "design"
    - from: "ds01-gpu-awareness.sh"
      to: "/etc/profile.d/ds01-gpu-awareness.sh"
      via: "deploy.sh copies to /etc/profile.d/"
      pattern: "profile.d"
---

<objective>
Update Phase 3 plan 03-03 to remove device permission anti-patterns and align with the research-informed design. Update ds01-gpu-awareness.sh to support video group exemptions.

Purpose: Apply the design decisions from 02.1-DESIGN.md to the remaining unexecuted Phase 3 plan (03-03). This ensures the deployment integration plan uses CUDA_VISIBLE_DEVICES as the host-level mechanism and video group for exemptions only, without udev rules or device permission manipulation. Also update the profile.d script to respect video group exemptions.

Output: Revised 03-03-PLAN.md and updated ds01-gpu-awareness.sh with exemption logic.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md
@.planning/phases/02.1-gpu-access-control-research/02.1-01-SUMMARY.md
@.planning/phases/03-access-control/03-03-PLAN.md
@.planning/phases/03-access-control/03-01-SUMMARY.md
@.planning/phases/03-access-control/03-02-SUMMARY.md
@config/deploy/profile.d/ds01-gpu-awareness.sh
@scripts/system/deploy.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update ds01-gpu-awareness.sh with video group exemption logic</name>
  <files>config/deploy/profile.d/ds01-gpu-awareness.sh</files>
  <action>
    Update the existing ds01-gpu-awareness.sh to add video group exemption checking. Currently the script unconditionally sets `export CUDA_VISIBLE_DEVICES=""`. It needs to check if the current user is in the video group first — if they are, they have been granted bare-metal access and should NOT have CUDA_VISIBLE_DEVICES set.

    **Updated script:**
    ```bash
    #!/bin/bash
    # DS01 GPU Awareness Layer
    # Prevents direct host GPU access from PyTorch/CUDA applications
    # Use containers for GPU workloads instead
    #
    # Sets CUDA_VISIBLE_DEVICES="" which makes torch.cuda.is_available() return False
    # while keeping nvidia-smi functional for system tools and GPU allocation.
    #
    # Users in the 'video' group are exempt (granted bare-metal access via bare-metal-access CLI).
    #
    # NVIDIA Container Runtime automatically sets CUDA_VISIBLE_DEVICES inside containers,
    # so this host-level setting does not affect containerised workloads.
    #
    # Standard HPC approach used by SLURM and other cluster managers.

    # Skip for non-interactive shells (cron jobs, systemd services, scripts)
    [[ $- == *i* ]] || return 0

    # Exempt users in the video group (bare-metal GPU access granted)
    if groups 2>/dev/null | grep -qw video; then
        return 0
    fi

    export CUDA_VISIBLE_DEVICES=""
    ```

    **Key changes from current version:**
    - Added interactive shell check (`[[ $- == *i* ]]`) — non-interactive shells (cron, systemd services, scripts called from system context) should not have CUDA_VISIBLE_DEVICES forced on them. This prevents breaking system tools that run as root via cron or systemd.
    - Added video group check — users granted bare-metal access via `bare-metal-access grant` are in the video group and should bypass GPU hiding.
    - Uses `groups` command (no args) which lists current user's groups. Uses `grep -qw video` for word-boundary matching.
    - Uses `return 0` (not `exit`) because profile.d scripts are sourced, not executed.

    **Do NOT:**
    - Use `exit` — profile.d scripts are sourced; exit would close the user's shell.
    - Query resource-limits.yaml — video group is the single source of truth for exemptions.
    - Add complex logic — keep it minimal. The bare-metal-access CLI manages video group membership.
  </action>
  <verify>
    - File config/deploy/profile.d/ds01-gpu-awareness.sh exists
    - Script contains video group check (grep for "video")
    - Script contains interactive shell check (grep for "\\$-")
    - Script uses "return" not "exit" (grep -c "exit" returns 0)
    - Script still exports CUDA_VISIBLE_DEVICES="" for non-exempt users
    - `bash -n config/deploy/profile.d/ds01-gpu-awareness.sh` exits 0 (syntax check — note: needs to be tested as sourced script, but -n catches basic syntax errors)
  </verify>
  <done>
    ds01-gpu-awareness.sh checks video group membership before setting CUDA_VISIBLE_DEVICES. Exempt users (video group members) get full bare-metal GPU access. Non-interactive shells are not affected.
  </done>
</task>

<task type="auto">
  <name>Task 2: Rewrite 03-03-PLAN.md to remove device permission anti-patterns</name>
  <files>.planning/phases/03-access-control/03-03-PLAN.md</files>
  <action>
    Rewrite 03-03-PLAN.md to align with the research-informed design. The current plan uses udev rules and device permissions (anti-patterns per research). The updated plan uses CUDA_VISIBLE_DEVICES as the host-level mechanism and video group for exemptions only.

    **Rewrite the entire 03-03-PLAN.md with the following changes:**

    **Frontmatter changes:**
    - Remove `config/deploy/udev/99-ds01-nvidia.rules` from files_modified (no longer created)
    - Add `config/deploy/profile.d/ds01-gpu-awareness.sh` to files_modified (profile.d exemption deployment)
    - Update depends_on: still depends on 03-01, 03-02
    - Update must_haves to remove udev/device permission truths, add profile.d exemption truths

    **Updated must_haves truths (replace current set):**
    - "deploy.sh deploys nvidia-* wrappers to /usr/local/bin"
    - "deploy.sh deploys bare-metal-access to /usr/local/bin"
    - "deploy.sh ensures at command is installed and atd is active"
    - "deploy.sh syncs bare_metal_access.exempt_users to video group (adding exempt users)"
    - "deploy.sh deploys updated ds01-gpu-awareness.sh with video group exemption check"
    - "MOTD mentions container-only GPU access policy"
    - "All nvidia-* wrappers are deployed and executable"
    - "Device permissions remain at defaults (0666) — NO udev rules deployed"

    **Updated must_haves artifacts:**
    - deploy.sh — contains nvidia-wrapper, bare-metal-access, video group sync, profile.d deployment
    - config/deploy/profile.d/ds01-motd.sh — MOTD with access control notice
    - config/deploy/profile.d/ds01-gpu-awareness.sh — updated with video group exemption (already modified in Task 1 of this plan)

    **Updated must_haves key_links:**
    - deploy.sh → /usr/local/bin/nvidia-smi (copies wrappers)
    - deploy.sh → /etc/profile.d/ds01-gpu-awareness.sh (copies updated awareness script)
    - deploy.sh → config/resource-limits.yaml (reads exempt_users for video group sync)

    **Task 1 of the updated plan should include:**

    1. **Prerequisites — ensure at command is available** (KEEP from current plan)

    2. **Deploy nvidia-* wrappers** (KEEP from current plan — they're UX tools, not security enforcement)

    3. **Deploy bare-metal-access admin CLI** (KEEP from current plan)

    4. **Deploy updated ds01-gpu-awareness.sh** (NEW — replaces udev rule deployment)
       - Copy config/deploy/profile.d/ds01-gpu-awareness.sh to /etc/profile.d/ds01-gpu-awareness.sh
       - This replaces the simple CUDA_VISIBLE_DEVICES="" export with the exemption-aware version
       - chmod 644

    5. **Sync video group membership from config** (SIMPLIFIED from current plan)
       - Read bare_metal_access.exempt_users from resource-limits.yaml
       - Ensure those users are in the video group
       - Do NOT remove non-exempt users from video group (without udev rules making device permissions 0660, being in the video group doesn't grant GPU access — CUDA_VISIBLE_DEVICES is the actual mechanism. The profile.d script checks video group membership to decide whether to set CUDA_VISIBLE_DEVICES.)
       - Preserve users with active temporary grants
       - Abort sync on YAML parse failure

    6. **Ensure state directories exist** (KEEP from current plan)

    7. **Deploy MOTD** (KEEP from current plan)

    **REMOVE from the plan entirely:**
    - config/deploy/udev/99-ds01-nvidia.rules file creation
    - udev rule deployment (cp to /etc/udev/rules.d/, udevadm reload/trigger)
    - Any reference to MODE="0660" or device permissions
    - Any reference to "crw-rw---- root:video"
    - The verification step about device permissions being 0660

    **Update the checkpoint task:**
    - Remove verification of udev rules and device permissions
    - Add verification of updated ds01-gpu-awareness.sh (video group exemption check)
    - Update verification steps to check profile.d deployment instead of udev deployment

    **Important notes in the plan:**
    - Clarify that CUDA_VISIBLE_DEVICES is a UX/deterrent layer, NOT a security boundary
    - Clarify that Docker --gpus device mapping is the actual security enforcement (via cgroups)
    - Clarify that nvidia-* wrappers are UX tools (helpful error messages), not security enforcement
    - Reference the design document: .planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md

    Keep the same plan structure (frontmatter, objective, context, tasks, verification, success_criteria, output). Keep it as a Wave 2 plan depending on 03-01 and 03-02. Keep autonomous: false (has checkpoint).
  </action>
  <verify>
    - File .planning/phases/03-access-control/03-03-PLAN.md exists
    - No udev deployment: grep for 'MODE=.*0660' returns no matches; grep for '99-ds01-nvidia.rules' returns no matches (historical removal mentions of "udev" as context are OK, but no rules file creation or deployment)
    - No device permission enforcement: grep for 'crw-rw----' returns no matches
    - File contains "ds01-gpu-awareness.sh" (profile.d deployment)
    - File contains "video" (video group sync)
    - File contains "bare-metal-access" (CLI deployment)
    - File contains "nvidia-wrapper" or "nvidia_wrapper" (wrapper deployment)
    - File contains reference to design document (grep for '02.1-DESIGN' or 'design document')
    - File contains "MOTD" or "motd" (MOTD deployment)
    - Frontmatter wave is 2, depends_on includes 03-01 and 03-02
  </verify>
  <done>
    03-03-PLAN.md rewritten to remove udev rules and device permission anti-patterns. Deployment now uses CUDA_VISIBLE_DEVICES via updated profile.d script as primary host-level mechanism, with video group for exemptions. Nvidia-* wrappers retained as UX tools. All deployment components (wrappers, CLI, profile.d, MOTD, video group sync) properly integrated.
  </done>
</task>

</tasks>

<verification>
1. ds01-gpu-awareness.sh has video group exemption check
2. ds01-gpu-awareness.sh has interactive shell check
3. 03-03-PLAN.md has no udev rules or device permission deployment
4. 03-03-PLAN.md deploys updated profile.d script with exemption logic
5. 03-03-PLAN.md syncs video group from config (adding exempt users)
6. 03-03-PLAN.md still deploys nvidia-* wrappers, bare-metal-access, MOTD
7. Both files pass syntax/structure validation
8. Phase 3 plans are now consistent with 02.1 design decisions
</verification>

<success_criteria>
- ds01-gpu-awareness.sh updated with video group exemption and interactive shell check
- 03-03-PLAN.md completely revised: no device permission anti-patterns, uses profile.d + video group approach
- Phase 3 deployment plan is research-informed and consistent with design document
- All existing Phase 3 work (03-01, 03-02) remains valid and unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-gpu-access-control-research/02.1-02-SUMMARY.md`
</output>
