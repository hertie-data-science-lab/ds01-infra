# Phase 2.1: GPU Access Control Research - Research

**Researched:** 2026-01-31
**Domain:** Multi-user GPU access control and isolation in HPC/data centre environments
**Confidence:** MEDIUM

## Summary

This research investigates how production HPC systems, Kubernetes clusters, and bare-metal GPU environments handle host vs container GPU access restriction. The investigation was triggered by DS01's Phase 3 implementation failure: device permission restrictions (`/dev/nvidia* 0660`, video group) broke `nvidia-smi`, which the GPU allocation pipeline depends on. Three debug sessions attempted fixes but the fundamental conflict remained.

Industry practice reveals a clear pattern: **environment variable isolation (CUDA_VISIBLE_DEVICES) combined with container-level device access control (cgroups/Docker device mapping) is the standard approach**. Device permission manipulation at the host level is rare and problematic because system tools (nvidia-smi, DCGM, monitoring) require device access even for queries.

DS01's quick fix — deploying `CUDA_VISIBLE_DEVICES=""` via `/etc/profile.d/` — aligns with HPC best practices. This research validates that approach and identifies refinements needed for production deployment.

**Primary recommendation:** Retain CUDA_VISIBLE_DEVICES="" approach for host-level GPU hiding. Add container-level enforcement via Docker device cgroups (already partially implemented via `--gpus device=X`). Implement admin bypass mechanism and optional per-user exemptions via video group (not as default enforcement, but as opt-in for approved users).

## Standard Stack

The established tools and patterns for multi-user GPU access control:

### Core Technologies
| Technology | Version/Status | Purpose | Why Standard |
|-----------|----------------|---------|--------------|
| CUDA_VISIBLE_DEVICES | Environment variable (all CUDA versions) | Application-level GPU visibility control | Universal CUDA runtime support, no kernel dependencies, HPC standard since CUDA 2.0 |
| cgroups v1 devices controller | Legacy (deprecated) | Kernel-level device access restriction | Used by SLURM, LSF, Mesos for production GPU isolation |
| cgroups v2 BPF device controller | Current (Linux 4.15+) | Modern device access restriction via eBPF | Replacement for v1 devices controller, required for systemd-based systems |
| Docker --gpus flag | Docker 19.03+ with NVIDIA Container Toolkit | Container GPU allocation | Industry standard for containerised GPU workloads |
| NVIDIA_VISIBLE_DEVICES | NVIDIA Container Toolkit env var | Container-level GPU enumeration | Translates to CUDA_VISIBLE_DEVICES inside containers |

### Supporting
| Technology | Version | Purpose | When to Use |
|-----------|---------|---------|-------------|
| NVIDIA MIG (Multi-Instance GPU) | A100/H100 only | Hardware GPU partitioning with memory/fault isolation | Multi-tenant clouds, strict isolation requirements, GPU sharing scenarios |
| nvidia-persistenced | Part of NVIDIA driver | Maintains GPU state without X server | Always-on GPU servers, reduces CUDA init time |
| systemd DeviceAllow | systemd 219+ | Service-level device access control | Restricting specific systemd services from GPU access |
| at command + atd daemon | POSIX standard | Scheduling one-time tasks (temporary access grants) | Temporary bare-metal access grants (simpler than systemd timers) |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| CUDA_VISIBLE_DEVICES | Device file permissions (0660, video group) | Breaks nvidia-smi and monitoring tools; not industry standard; requires complex permission management |
| cgroups device controller | Manual device permission enforcement | No kernel-level protection; race conditions on device creation; inconsistent across reboots |
| Docker device mapping | Time-slicing without isolation | Software isolation only; no memory protection; noisy neighbour issues |
| MIG partitioning | Full GPU per user | Wastes resources on underutilised GPUs; limits flexibility |

**Installation:**
```bash
# NVIDIA Container Toolkit (required)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit

# at daemon (for temporary access grants)
sudo apt-get install -y at
sudo systemctl enable --now atd
```

## Architecture Patterns

### Pattern 1: Layered GPU Access Control (Industry Standard)

**What:** Multi-layer enforcement combining host-level visibility control with container-level device isolation.

**When to use:** Multi-user GPU servers where users need containers for GPU work but should not access bare metal.

**Layers:**

1. **Host-level visibility hiding:** `CUDA_VISIBLE_DEVICES=""` in `/etc/profile.d/` makes CUDA applications see zero GPUs
2. **Container-level device allocation:** Docker `--gpus device=UUID` grants specific GPU access via cgroups
3. **Optional: Selective exemptions:** Admin users or approved researchers get video group membership for bare-metal access

**Example:**
```bash
# /etc/profile.d/ds01-gpu-awareness.sh
export CUDA_VISIBLE_DEVICES=""

# Container creation with specific GPU
docker run --gpus device=GPU-a1b2c3d4 pytorch/pytorch python train.py

# Inside container: CUDA_VISIBLE_DEVICES automatically set by NVIDIA Container Toolkit
# Container sees 1 GPU as device 0, regardless of physical GPU index
```

**Source:** [HPC @ QMUL GPU Documentation](https://docs.hpc.qmul.ac.uk/using/usingGPU/), [NVIDIA CUDA Pro Tip](https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/)

### Pattern 2: cgroups Device Controller for Kernel-Level Isolation

**What:** Linux cgroups devices controller restricts which device files processes can access.

**When to use:** Production schedulers (SLURM, LSF, PBS) needing kernel-enforced GPU isolation.

**Implementation (cgroups v1):**
```bash
# Create cgroup for job
cgcreate -g devices:/slurm/job_12345

# Deny all devices by default
echo "a" > /sys/fs/cgroup/devices/slurm/job_12345/devices.deny

# Allow specific GPU
echo "c 195:0 rwm" > /sys/fs/cgroup/devices/slurm/job_12345/devices.allow  # /dev/nvidia0
echo "c 195:255 rwm" > /sys/fs/cgroup/devices/slurm/job_12345/devices.allow  # /dev/nvidiactl
echo "c 238:* rwm" > /sys/fs/cgroup/devices/slurm/job_12345/devices.allow  # /dev/nvidia-uvm*

# Launch process in cgroup
cgexec -g devices:/slurm/job_12345 python train.py
```

**Implementation (cgroups v2 with BPF):**
```c
// Requires BPF program attached to cgroup
// See: https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html
int BPF_PROG(dev_cgroup, struct bpf_cgroup_dev_ctx *ctx) {
    // Allow access to specific GPU device only
    if (ctx->major == 195 && ctx->minor == 0) return 1;  // /dev/nvidia0
    if (ctx->major == 195 && ctx->minor == 255) return 1; // /dev/nvidiactl
    return 0; // Deny all other devices
}
```

**Critical limitation:** cgroups v1 devices controller is deprecated. cgroups v2 requires BPF programming. Most systems transitioning to environment variable + container runtime enforcement.

**Source:** [Linux Kernel cgroup-v2 documentation](https://docs.kernel.org/admin-guide/cgroup-v2.html), [LWN cgroup GPU support discussion](https://lwn.net/Articles/844199/)

### Pattern 3: MIG for Hardware-Level Isolation

**What:** NVIDIA Multi-Instance GPU partitions single GPU into up to 7 isolated instances with dedicated compute, memory, and fault isolation.

**When to use:** Multi-tenant clouds, strict security requirements, workloads needing predictable QoS.

**Device node structure:**
```
/dev/nvidia0              # Physical GPU
/dev/nvidia-caps/         # MIG capability devices
    nvidia-cap1           # MIG config capability (admin only)
    nvidia-cap2           # MIG monitor capability
    ...
```

**Access control:** Capabilities managed via cgroups (read access to `/dev/nvidia-caps/nvidia-capX` grants specific permissions). Administrators bind-mount only approved capability devices into containers.

**Example (MIG instance allocation):**
```bash
# Admin: Create MIG instances
sudo nvidia-smi mig -cgi 9,9,9 -C  # Create 3x 3g.20gb instances on GPU 0

# List instances
nvidia-smi mig -lgi
# Output:
# GPU 0: MIG 3g.20gb Device 0: (UUID: MIG-abc123...)
# GPU 0: MIG 3g.20gb Device 1: (UUID: MIG-def456...)
# GPU 0: MIG 3g.20gb Device 2: (UUID: MIG-ghi789...)

# Container with specific MIG instance
docker run --gpus device=MIG-abc123... pytorch/pytorch python train.py
```

**DS01 context:** A100 GPUs support MIG. Currently configured for 2g.20gb profile (3 instances per GPU).

**Source:** [NVIDIA MIG User Guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/), [Kubernetes GPU Multi-Tenancy Strategies](https://www.vcluster.com/blog/gpu-multitenancy-kubernetes-strategies)

### Pattern 4: SLURM GPU Enforcement

**What:** SLURM uses cgroups devices controller to enforce GPU allocation. Jobs only see GPUs they requested.

**Key mechanisms:**
- `gres.conf` defines GPU resources per node
- `cgroup.conf` enables `ConstrainDevices=yes` for device controller enforcement
- Jobs request GPUs via `--gres=gpu:N` flag
- SLURM dynamically configures cgroups devices.allow for each job

**Example SLURM configuration:**
```ini
# gres.conf
NodeName=gpu-node[01-04] Name=gpu Type=a100 File=/dev/nvidia[0-3]

# cgroup.conf
CgroupAutomount=yes
ConstrainCores=yes
ConstrainDevices=yes  # KEY: Enforces GPU isolation via devices cgroup
ConstrainRAMSpace=yes
```

**User workflow:**
```bash
# Request 2 GPUs
srun --gres=gpu:2 python train.py

# Inside job: sees exactly 2 GPUs as device 0 and 1
# SLURM sets CUDA_VISIBLE_DEVICES based on allocation
# cgroups prevents access to other GPU device files
```

**Current status (2026):** SLURM supports both cgroups v1 and v2. v2 requires additional BPF setup but is becoming standard.

**Source:** [CHPC Utah SLURM GPU Documentation](https://www.chpc.utah.edu/documentation/software/slurm-gpus.php), [SLURM cgroup v2 plugin documentation](https://slurm.schedmd.com/cgroup_v2.html)

### Pattern 5: Kubernetes GPU Sharing Strategies

**What:** Kubernetes offers multiple GPU sharing approaches with different isolation guarantees.

**Strategies:**

1. **Whole GPU per pod** (strongest isolation):
   - `resources.limits.nvidia.com/gpu: 1`
   - Kubernetes reserves GPU at node level via device plugin
   - Best isolation, lowest utilisation

2. **MIG partitioning** (hardware isolation):
   - Each MIG instance treated as separate device
   - `resources.limits.nvidia.com/mig-3g.20gb: 1`
   - Memory and fault isolation between pods

3. **Time-slicing** (software isolation):
   - GPU Operator configures replicas per GPU
   - Multiple pods share same GPU via time-slicing
   - No memory isolation, potential noisy neighbour issues

**Security note (critical):** CVE-2025-23266 in NVIDIA Container Toolkit (Jan 2025) allowed container escape. **Mitigation:** Use Container Toolkit v1.16.2+ or GPU Operator v24.6.2+.

**Source:** [vCluster GPU Multi-Tenancy Strategies](https://www.vcluster.com/blog/gpu-multitenancy-kubernetes-strategies), [NVIDIA GPU Operator Time-Slicing](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html)

### Anti-Patterns to Avoid

1. **Device file permission manipulation as primary enforcement:**
   - Why it's bad: Breaks nvidia-smi, DCGM, monitoring tools. NVIDIA driver may recreate devices with default permissions. Requires complex synchronisation with nvidia-persistenced.
   - What to do instead: Use CUDA_VISIBLE_DEVICES for host visibility control, cgroups/Docker for container isolation.

2. **Relying solely on CUDA_VISIBLE_DEVICES for security:**
   - Why it's bad: Application-level control only. Malicious code can ignore it. No kernel enforcement.
   - What to do instead: Combine with cgroups device controller or Docker device mapping for kernel-level protection.

3. **Using udev rules to permanently set device permissions:**
   - Why it's bad: NVIDIA driver parameters (DeviceFileGID, DeviceFileMode) override udev rules. Race conditions on boot. nvidia-persistenced may recreate devices.
   - What to do instead: If device permissions needed, use NVIDIA kernel module parameters in `/etc/modprobe.d/`.

4. **Mixing video group membership for both default access and exemptions:**
   - Why it's bad: Confuses enforcement mechanism with exception mechanism. Hard to audit who has access.
   - What to do instead: Use CUDA_VISIBLE_DEVICES="" as default. Video group membership only for explicit exemptions.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Temporary access grant scheduling | Custom cron job manager | `at` command with `atd` daemon | POSIX standard, handles one-time tasks, automatic cleanup, conflict-free |
| Container GPU device allocation | Manual /dev/nvidia* management | Docker `--gpus device=UUID` with NVIDIA Container Toolkit | Handles device mounting, capability setup, CUDA_VISIBLE_DEVICES injection, cgroup configuration |
| GPU state persistence | Custom daemon or systemd timer | nvidia-persistenced | Official NVIDIA daemon, handles driver state, faster CUDA init, designed for 24/7 servers |
| MIG instance enumeration | Parsing nvidia-smi XML output | nvidia-smi mig -lgi or NVML API | Official interface, stable output format, version-compatible |
| Device access control in containers | Custom device mounting logic | Docker device mapping or Kubernetes device plugins | Integrates with cgroups, handles permissions, manages lifecycle |

**Key insight:** GPU access control has sharp edges (driver behaviour, cgroup interactions, container runtime integration). Use proven tools rather than custom implementations. The NVIDIA Container Toolkit alone handles device mounting, capability setup, environment variable injection, and runtime hook integration — reimplementing this is error-prone.

## Common Pitfalls

### Pitfall 1: Device Permission Changes Overridden by NVIDIA Driver

**What goes wrong:** You set `/dev/nvidia*` permissions via udev rules or chmod, but they revert to defaults after reboot or driver reload.

**Why it happens:** NVIDIA driver kernel module has hardcoded `DeviceFileGID` and `DeviceFileMode` parameters (defaults: GID=0, Mode=0666). These override udev rules. Additionally, nvidia-persistenced (if running) recreates device files with these permissions.

**How to avoid:**
- Use kernel module parameters in `/etc/modprobe.d/nvidia.conf`:
  ```
  options nvidia NVreg_DeviceFileGID=44 NVreg_DeviceFileMode=0660
  ```
- Rebuild initramfs: `sudo update-initramfs -u`
- Reboot to apply

**Warning signs:**
- Permissions correct after manual chmod but wrong after reboot
- nvidia-persistenced logs showing device recreation
- udev rules triggered but no effect on final permissions

**Better approach:** Don't manipulate device permissions. Use CUDA_VISIBLE_DEVICES for host-level hiding and cgroups for container-level isolation.

**Source:** [NVIDIA Developer Forums: Device Permissions](https://forums.developer.nvidia.com/t/nvidia-device-permissions-for-multiple-users/16413), [NVIDIA Driver README Chapter 27](https://download.nvidia.com/XFree86/Linux-x86_64/396.51/README/nvidia-persistenced.html)

### Pitfall 2: cgroups v1 Devices Controller Deprecated

**What goes wrong:** You implement GPU isolation using cgroups v1 devices controller, but it's not available on newer systemd-based distros (Ubuntu 22.04+, RHEL 9+).

**Why it happens:** cgroups v2 is now default. The devices controller has "no future" and won't be ported to v2. Replacement is BPF-based device access control.

**How to avoid:**
- Check cgroup version: `stat -fc %T /sys/fs/cgroup/` (returns "cgroup2fs" for v2)
- For new implementations: Use Docker device mapping or systemd DeviceAllow (which abstracts BPF complexity)
- For cgroups v2: Implement BPF_CGROUP_DEVICE programs or use higher-level tools

**Warning signs:**
- `/sys/fs/cgroup/devices/` doesn't exist on systemd-based system
- "devices" controller not listed in `cat /sys/fs/cgroup/cgroup.controllers`
- systemd documentation references DeviceAllow but not devices.allow file

**Source:** [systemd-devel: deny access to GPU devices](https://lists.freedesktop.org/archives/systemd-devel/2016-November/037803.html), [Linux Kernel cgroup-v2 documentation](https://docs.kernel.org/admin-guide/cgroup-v2.html)

### Pitfall 3: CUDA_VISIBLE_DEVICES is Not a Security Boundary

**What goes wrong:** You rely on `CUDA_VISIBLE_DEVICES=""` to prevent GPU access, but user writes Python code that sets `os.environ['CUDA_VISIBLE_DEVICES'] = '0'` before importing torch.

**Why it happens:** `CUDA_VISIBLE_DEVICES` is a convenience mechanism for CUDA applications, not kernel-level enforcement. Applications can modify environment variables before initialising CUDA.

**How to avoid:**
- Treat `CUDA_VISIBLE_DEVICES` as **user experience layer** (prevents accidental bare-metal usage)
- Enforce via **cgroups or container device mapping** (kernel-level protection)
- For containers: Docker `--gpus device=X` provides actual enforcement
- For bare metal: Accept that determined users can access GPUs (monitor via detection, not prevention)

**Warning signs:**
- Relying on environment variables for security audit compliance
- No fallback enforcement mechanism if CUDA_VISIBLE_DEVICES ignored
- Assuming application-level control equals kernel-level isolation

**Example DS01 scenario:**
```bash
# Host: CUDA_VISIBLE_DEVICES="" set globally
python3 << 'EOF'
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # User overrides
import torch
print(torch.cuda.is_available())  # Returns True on bare metal without cgroup enforcement
EOF
```

**Mitigation:** DS01's awareness layer correctly uses `CUDA_VISIBLE_DEVICES=""` as deterrent + UX improvement, NOT as security boundary. Phase 2 (Awareness Layer) detects unauthorised GPU processes. Container enforcement is the actual security layer.

**Source:** [NVIDIA CUDA Pro Tip blog](https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/)

### Pitfall 4: nvidia-smi Requires Device Access for All Queries

**What goes wrong:** You restrict `/dev/nvidia*` permissions to prevent GPU access, then nvidia-smi fails even for admin queries (`nvidia-smi -L`, `nvidia-smi topo`).

**Why it happens:** nvidia-smi needs read access to `/dev/nvidiactl` and `/dev/nvidia-uvm` for all operations, even listing GPUs. No read-only query mode exists.

**How to avoid:**
- Don't use device permissions as enforcement mechanism
- If permissions needed for specific use case: Create nvidia-smi wrapper that temporarily elevates privileges
- Alternative: Use `nvidia-smi --query-gpu=... --format=csv` with cached output (fragile, not recommended)

**Warning signs:**
- GPU allocator scripts failing with "Insufficient Permissions" from nvidia-smi
- Monitoring tools (DCGM, Prometheus exporters) unable to read GPU metrics
- Admin tools broken by device permission restrictions

**DS01 debug history:** Three attempts to work around this (nvidia-smi caching, wrapper scripts, fallback logic) before switching to CUDA_VISIBLE_DEVICES approach. Caching approach was fragile and hid real allocation failures.

**Source:** DS01 debug session `.planning/debug/gpu-alloc-silent-exit-after-awareness-layer.md`

### Pitfall 5: MIG UUID vs Device Index Confusion

**What goes wrong:** You allocate MIG-abc123 to container via `--gpus device=MIG-abc123`, but inside container CUDA code tries to use device index 0, which maps to wrong MIG instance or full GPU.

**Why it happens:** Docker device parameter accepts UUIDs but CUDA applications expect device indices. NVIDIA Container Toolkit sets CUDA_VISIBLE_DEVICES to translate, but multi-MIG scenarios can create enumeration conflicts.

**How to avoid:**
- For single MIG per container: Trust NVIDIA Container Toolkit to set CUDA_VISIBLE_DEVICES correctly
- For multiple MIG per container: Explicitly set `CUDA_VISIBLE_DEVICES=MIG-uuid1,MIG-uuid2,...` as env var
- Always use UUIDs in allocation, never assume index stability
- Verify with `nvidia-smi -L` inside container before training

**Warning signs:**
- Container launches but CUDA sees different GPU count than allocated
- Training uses wrong GPU (visible in nvidia-smi output)
- MIG instances sharing memory when isolation expected

**DS01 note:** `mlc-patched.py` line 1719 comment warns about this: "Do NOT set CUDA_VISIBLE_DEVICES for multi-MIG - it conflicts with NVIDIA enumeration"

**Source:** [NVIDIA MIG User Guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/), DS01 codebase `scripts/docker/mlc-patched.py`

## Code Examples

Verified patterns from official sources:

### Host-Level GPU Hiding (HPC Standard)

```bash
# /etc/profile.d/ds01-gpu-awareness.sh
# Source: HPC cluster standard practice
# Effect: torch.cuda.is_available() returns False for all host processes
export CUDA_VISIBLE_DEVICES=""
```

**Verification:**
```bash
# On host (after sourcing profile):
python3 -c "import torch; print(torch.cuda.is_available())"  # False

# nvidia-smi still works (doesn't use CUDA):
nvidia-smi -L  # Lists GPUs successfully
```

**Source:** [CUDA Pro Tip: Control GPU Visibility](https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/)

### Container GPU Allocation with Docker

```bash
# Get GPU UUID (prefer UUIDs over indices for stability)
GPU_UUID=$(nvidia-smi -L | grep "GPU 0" | grep -oP 'UUID: \K[^ ]+')

# Launch container with specific GPU
docker run --rm \
  --gpus device=${GPU_UUID} \
  -e NVIDIA_VISIBLE_DEVICES=${GPU_UUID} \
  pytorch/pytorch:latest \
  python -c "import torch; print(f'GPUs visible: {torch.cuda.device_count()}')"

# Output: GPUs visible: 1
# Inside container: nvidia-smi shows only allocated GPU
```

**For MIG instances:**
```bash
# Get MIG instance UUID
MIG_UUID=$(nvidia-smi mig -lgi | grep "MIG 3g.20gb Device 0" | grep -oP 'MIG-[a-f0-9-]+')

# Allocate specific MIG instance
docker run --rm \
  --gpus device=${MIG_UUID} \
  -e CUDA_VISIBLE_DEVICES=${MIG_UUID} \
  pytorch/pytorch:latest \
  python -c "import torch; print(torch.cuda.get_device_properties(0))"
```

**Source:** [NVIDIA Container Toolkit Specialized Configurations](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html)

### Temporary Access Grant with at Command

```bash
#!/bin/bash
# bare-metal-access grant <username> <duration>
# Source: DS01 implementation, POSIX at command standard

USERNAME="$1"
DURATION="${2:-24h}"  # Default 24 hours

# Add to video group
sudo usermod -aG video "$USERNAME"

# Schedule revocation
REVOKE_SCRIPT=$(mktemp)
cat > "$REVOKE_SCRIPT" << 'EOF'
#!/bin/bash
# Revoke GPU access
sudo gpasswd -d USERNAME_PLACEHOLDER video

# Notify user
echo "Your temporary bare metal GPU access has expired." | \
  wall -u USERNAME_PLACEHOLDER 2>/dev/null || true
EOF

sed -i "s/USERNAME_PLACEHOLDER/$USERNAME/g" "$REVOKE_SCRIPT"
chmod +x "$REVOKE_SCRIPT"

# Parse duration and schedule
case "$DURATION" in
  *h) HOURS="${DURATION%h}" ;;
  *d) HOURS=$((${DURATION%d} * 24)) ;;
  *m) HOURS="0"; MINUTES="${DURATION%m}" ;;
  *) echo "Invalid duration format"; exit 1 ;;
esac

if [ -n "$MINUTES" ]; then
  echo "$REVOKE_SCRIPT" | at now + "$MINUTES" minutes
else
  echo "$REVOKE_SCRIPT" | at now + "$HOURS" hours
fi

echo "Access granted to $USERNAME for $DURATION"
echo "Note: User must start a new SSH session for access to take effect."
```

**Warning scheduling (1h before expiry):**
```bash
# Schedule warning 1 hour before revocation
WARNING_TIME=$((HOURS - 1))
if [ $WARNING_TIME -gt 0 ]; then
  echo "echo 'Your bare metal GPU access expires in 1 hour.' | wall -u $USERNAME" | \
    at now + "$WARNING_TIME" hours
fi
```

**Source:** POSIX at(1) manual, DS01 Phase 3 requirements

### systemd DeviceAllow for Service Restriction

```ini
# /etc/systemd/system/untrusted-service.service
# Prevent specific systemd service from accessing GPUs
[Unit]
Description=Untrusted Service

[Service]
Type=simple
ExecStart=/usr/bin/untrusted-app

# Strict device policy: only explicitly allowed devices accessible
DevicePolicy=strict

# Allow only essential devices (no GPU devices)
DeviceAllow=/dev/null rw
DeviceAllow=/dev/zero rw
DeviceAllow=/dev/urandom r

# GPU devices explicitly denied by omission:
# - /dev/nvidia* NOT listed → inaccessible
# - /dev/nvidia-uvm* NOT listed → inaccessible
```

**Verify isolation:**
```bash
sudo systemctl daemon-reload
sudo systemctl start untrusted-service

# Check that service cannot see GPUs
journalctl -u untrusted-service | grep -i "gpu\|nvidia"  # Should fail
```

**Source:** [systemd.resource-control man page](https://www.freedesktop.org/software/systemd/man/latest/systemd.resource-control.html)

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| cgroups v1 devices controller | cgroups v2 BPF device controller | Linux 4.15 (2018), systemd default 2020+ | Requires BPF programming or higher-level abstraction (systemd DeviceAllow, Docker device mapping) |
| Device file permissions for access control | CUDA_VISIBLE_DEVICES + container device mapping | HPC clusters: 2015+, Containers: 2019+ (Docker 19.03) | Simpler management, works with system tools, no driver conflicts |
| Full GPU per user | MIG partitioning or time-slicing | A100 release (2020), Kubernetes GPU Operator (2021) | Better utilisation, predictable QoS, multi-tenant support |
| Manual nvidia-smi parsing | NVML API or nvidia-smi --query-gpu | Always available, increasingly adopted | Structured output, version stability, programmatic access |
| OPA (Open Policy Agent) for Docker auth | Native Docker wrapper authorization | Varies by site | Lower complexity, fewer dependencies, faster policy evaluation |

**Deprecated/outdated:**
- **udev rules for permanent device permissions:** Overridden by NVIDIA driver module parameters, race conditions on boot. Use kernel module options instead.
- **cgroups v1 devices controller:** Not available in cgroups v2. Migrate to BPF-based control or abstraction layers.
- **video group as default enforcement:** Modern practice uses container isolation as primary enforcement, video group for explicit exemptions only.
- **CUDA_VISIBLE_DEVICES device indices:** Use GPU UUIDs for allocation stability (especially with MIG, device hotplug, multi-GPU systems).

**Recent developments (2025-2026):**
- **CVE-2025-23266 (NVIDIA Container Toolkit):** Container escape vulnerability. Mitigation: v1.16.2+. Highlights need for defense-in-depth (container isolation + host-level monitoring).
- **Kubernetes Dynamic Resource Allocation (DRA):** Stable in v1.35 (2026). Provides structured GPU allocation beyond device plugins. May influence future Docker implementations.
- **SLURM cgroups v2 support:** Production-ready in SLURM 23.11+. BPF device controller integration available but complex.

## Open Questions

Things that couldn't be fully resolved:

1. **Question: What is the performance impact of CUDA_VISIBLE_DEVICES="" on CUDA initialization time?**
   - What we know: CUDA enumerates all system GPUs at init. Empty CUDA_VISIBLE_DEVICES avoids enumeration. cgroups device restriction may also reduce init time.
   - What's unclear: Quantitative benchmarks for CUDA init time with CUDA_VISIBLE_DEVICES="" vs device cgroups vs no restriction.
   - Recommendation: Accept current implementation. CUDA init time is not critical path for DS01 (container launch overhead dominates). Monitor if users report slow pytorch import.

2. **Question: How do you enforce GPU access for processes launched via systemd user services?**
   - What we know: systemd user services (`systemd --user`) run in user's cgroup. DeviceAllow in user service units can restrict devices. CUDA_VISIBLE_DEVICES inherited if set in user environment.
   - What's unclear: Whether systemd user services respect /etc/profile.d/ environment (they don't by default). Workaround needed?
   - Recommendation: Low priority for DS01 (users don't typically use systemd --user for GPU workloads). If needed: Document manual CUDA_VISIBLE_DEVICES="" in user service unit or use DevicePolicy=strict.

3. **Question: What is the proper migration path from cgroups v1 to v2 for existing GPU isolation?**
   - What we know: cgroups v2 requires BPF programs for device access control. systemd's DeviceAllow abstracts BPF complexity. Docker and NVIDIA Container Toolkit handle v2 automatically.
   - What's unclear: Best practices for mixed-mode (v1 + v2) during transition. Whether to implement custom BPF or rely on container runtime.
   - Recommendation: DS01 should rely on Docker device mapping (handles v1/v2 transparently). Don't hand-roll BPF device controller. Monitor systemd/Docker documentation for emerging patterns.

4. **Question: Does nvidia-persistenced interfere with container-level GPU allocation?**
   - What we know: nvidia-persistenced maintains GPU state when no X server running. It runs as root and requires /dev/nvidia* access. DS01 currently doesn't run it.
   - What's unclear: Whether enabling nvidia-persistenced improves CUDA init time enough to justify complexity. Whether it conflicts with MIG instance management.
   - Recommendation: Benchmark container launch time with/without nvidia-persistenced. NVIDIA recommends it for 24/7 GPU servers. If enabled: Run as systemd service with DeviceAllow granting GPU access.

5. **Question: Can Phase 3's video group exemption mechanism conflict with container GPU allocation?**
   - What we know: Users in video group can run bare-metal CUDA code. Docker containers use cgroups device mapping regardless of host user's group membership.
   - What's unclear: If user launches container while running bare-metal GPU process, can this cause allocation conflict? Does Docker honour video group membership when mounting devices?
   - Recommendation: Phase 2 awareness layer detects bare-metal GPU processes. GPU allocator should check for user's host processes before allocating to container. Document expectation: users with video group membership use containers OR bare metal, not simultaneously.

## Sources

### Primary (HIGH confidence)
- [NVIDIA Multi-Instance GPU User Guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/) - Official MIG implementation documentation
- [NVIDIA Container Toolkit Specialized Configurations](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html) - NVIDIA_VISIBLE_DEVICES, driver capabilities
- [Linux Kernel cgroup-v2 documentation](https://docs.kernel.org/admin-guide/cgroup-v2.html) - BPF device controller, migration from v1
- [NVIDIA CUDA Pro Tip: CUDA_VISIBLE_DEVICES](https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/) - Official NVIDIA guidance on environment variable usage
- [systemd.resource-control man page](https://www.freedesktop.org/software/systemd/man/latest/systemd.resource-control.html) - DeviceAllow, DevicePolicy documentation
- [NVIDIA Container Toolkit GitHub repository](https://github.com/NVIDIA/nvidia-container-toolkit) - Implementation reference, CVE-2025-23266 details

### Secondary (MEDIUM confidence)
- [CHPC Utah SLURM GPU Documentation](https://www.chpc.utah.edu/documentation/software/slurm-gpus.php) - SLURM GPU allocation verified by HPC centre
- [vCluster GPU Multi-Tenancy Strategies](https://www.vcluster.com/blog/gpu-multitenancy-kubernetes-strategies) - Kubernetes patterns, industry comparison
- [NVIDIA GPU Operator Time-Slicing](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html) - Official GPU Operator documentation
- [HPC @ QMUL GPU Documentation](https://docs.hpc.qmul.ac.uk/using/usingGPU/) - Real-world HPC implementation (cgroups, CUDA_VISIBLE_DEVICES)
- [SLURM cgroup v2 plugin](https://slurm.schedmd.com/cgroup_v2.html) - Official SLURM v2 migration guidance
- [LWN: cgroup support for GPU devices](https://lwn.net/Articles/844199/) - Linux kernel development discussion on GPU cgroup controller proposals

### Tertiary (LOW confidence)
- [NVIDIA Developer Forums: Device Permissions](https://forums.developer.nvidia.com/t/nvidia-device-permissions-for-multiple-users/16413) - Community discussion, not official guidance
- [Bare Metal Kubernetes GPU Challenges](https://www.vcluster.com/blog/bare-metal-kubernetes-with-gpu-challenges-and-multi-tenancy-solutions) - Third-party analysis
- [systemd-devel: deny access to GPU devices](https://lists.freedesktop.org/archives/systemd-devel/2016-November/037803.html) - Mailing list discussion (2016, pre-v2)
- Various Stack Overflow and forum discussions on udev rules, device permissions - Used for pitfall identification only

## Metadata

**Confidence breakdown:**
- Standard stack: **HIGH** - Official NVIDIA documentation, kernel docs, widespread HPC adoption verified
- Architecture patterns: **MEDIUM** - Patterns derived from official docs + HPC centre implementations. Kubernetes patterns verified via NVIDIA and vCluster sources. SLURM specifics verified via official docs but implementation details vary by site.
- Pitfalls: **MEDIUM-HIGH** - Pitfalls 1, 2, 4 verified via official sources and DS01 debug experience. Pitfall 3 medium confidence (well-known but not explicitly documented by NVIDIA). Pitfall 5 verified via DS01 codebase and MIG user guide.

**Research date:** 2026-01-31
**Valid until:** 2026-07-31 (6 months - stable domain, but monitor for CVE updates and Kubernetes DRA evolution)

**Critical follow-up:** Verify NVIDIA Container Toolkit version on DS01 server against CVE-2025-23266. If < v1.16.2, upgrade before Phase 3 deployment.
