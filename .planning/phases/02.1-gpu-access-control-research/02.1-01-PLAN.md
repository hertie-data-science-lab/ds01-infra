---
phase: 02.1-gpu-access-control-research
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md
autonomous: false

must_haves:
  truths:
    - "Design document explains chosen approach (CUDA_VISIBLE_DEVICES + container enforcement) with rationale"
    - "Design document lists rejected alternatives (device permissions, udev rules, cgroups hand-rolling) with reasons"
    - "Design document audits existing Phase 3 implementation (03-01, 03-02) against research findings"
    - "Design document identifies what needs changing in Phase 3 plan 03-03"
    - "nvidia-smi works for all users (GPU allocator pipeline functional)"
    - "CUDA_VISIBLE_DEVICES='' is active in /etc/profile.d/ and hides GPUs from CUDA applications"
    - "container deploy works end-to-end for a regular user"
  artifacts:
    - path: ".planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md"
      provides: "Design decision document for GPU access control"
      contains: "CUDA_VISIBLE_DEVICES"
  key_links:
    - from: "02.1-DESIGN.md"
      to: "02.1-RESEARCH.md"
      via: "Design document references research findings"
      pattern: "research"
    - from: "02.1-DESIGN.md"
      to: "03-01-SUMMARY.md, 03-02-SUMMARY.md, 03-03-PLAN.md"
      via: "Audits existing Phase 3 implementation by specific plan"
      pattern: "03-0[123]"
---

<objective>
Write a design decision document synthesising the Phase 2.1 research into a concrete access control design, and verify the current CUDA_VISIBLE_DEVICES deployment works correctly end-to-end.

Purpose: Ground the DS01 GPU access control implementation in proven HPC/data centre patterns. Provide a clear rationale for the chosen approach and document what needs changing in the remaining Phase 3 plan (03-03). Verify the bug that triggered this research is resolved.

Output: Design document (02.1-DESIGN.md) and verified working deployment.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-gpu-access-control-research/02.1-RESEARCH.md
@.planning/phases/03-access-control/03-01-SUMMARY.md
@.planning/phases/03-access-control/03-02-SUMMARY.md
@.planning/phases/03-access-control/03-03-PLAN.md
@config/deploy/profile.d/ds01-gpu-awareness.sh
@scripts/system/verify-cuda-awareness.sh
@scripts/system/restore-nvidia-defaults.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write GPU access control design document</name>
  <files>.planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md</files>
  <action>
    Create 02.1-DESIGN.md — the design decision document that synthesises the research findings into a concrete implementation design. This is the primary deliverable of Phase 2.1.

    **Structure the document as follows:**

    **1. Design Decision: GPU Access Control Architecture**
    - State the chosen approach clearly: "Layered GPU access control using CUDA_VISIBLE_DEVICES for host-level hiding + Docker device mapping for container-level enforcement + video group for opt-in exemptions"
    - This is Pattern 1 from the research (Layered GPU Access Control)

    **2. Why This Approach**
    - CUDA_VISIBLE_DEVICES="" is the HPC standard (SLURM does the same thing)
    - Container-level enforcement via Docker --gpus is the actual security boundary (kernel-level via cgroups)
    - Device permissions stay at defaults (0666) — no manipulation needed
    - nvidia-smi and monitoring tools work without restriction
    - Simple to deploy, simple to understand, proven at scale

    **3. The Three Layers**
    Document each layer with its purpose and limitation:

    Layer 1: Host-Level Deterrent (CUDA_VISIBLE_DEVICES="")
    - Purpose: UX layer preventing accidental bare-metal GPU usage
    - Mechanism: /etc/profile.d/ds01-gpu-awareness.sh exports CUDA_VISIBLE_DEVICES=""
    - Effect: torch.cuda.is_available() returns False; nvidia-smi still works
    - Limitation: NOT a security boundary — users can override via os.environ
    - Status: DEPLOYED (Phase 2 awareness layer)

    Layer 2: Container Enforcement (Docker device mapping)
    - Purpose: Actual security boundary — kernel-level GPU access control
    - Mechanism: Docker --gpus device=UUID maps specific GPU into container via cgroups
    - Effect: Container sees only allocated GPU(s); cannot access others
    - Limitation: Requires Docker (not applicable to bare-metal processes)
    - Status: DEPLOYED (Docker wrapper + gpu_allocator_v2.py)

    Layer 3: Exemption Management (video group)
    - Purpose: Opt-in bare-metal GPU access for approved users
    - Mechanism: Video group membership + CUDA_VISIBLE_DEVICES unset for exempted users
    - Effect: Exempted users can run nvidia-smi, CUDA programs on bare metal
    - Limitation: Exempted users bypass Layer 1; monitored via Phase 2 awareness layer
    - Status: PARTIALLY IMPLEMENTED (bare-metal-access CLI exists from 03-01, but exempted users still get CUDA_VISIBLE_DEVICES="" — needs profile.d update to skip exempt users)

    **4. Rejected Alternatives**
    For each, state what it is, why it was considered, and why it was rejected:

    a. Device file permissions (0660, video group as default enforcement)
    - Rejected: Breaks nvidia-smi, DCGM, monitoring. DS01 experienced this failure directly (three debug sessions).
    - Research: Anti-pattern per HPC standards. No production systems use this as primary enforcement.

    b. udev rules for device permission enforcement
    - Rejected: NVIDIA driver parameters (NVreg_DeviceFileGID, NVreg_DeviceFileMode) override udev rules. Race conditions on boot.
    - Research: Even when they work, they break system tools.

    c. Hand-rolled cgroups v2 BPF device controller
    - Rejected: Complex, fragile, not portable. Docker already handles this transparently.
    - Research: "Don't hand-roll" — let Docker/NVIDIA Container Toolkit manage cgroups.

    d. nvidia-* command wrappers as primary security enforcement
    - Rejected as SECURITY: Wrapping nvidia-smi doesn't prevent CUDA access — CUDA uses /dev/nvidia* directly, not nvidia-smi.
    - Retained as UX: Wrappers provide helpful error messages directing users to containers. This is a UX enhancement, not a security mechanism.

    **5. Audit of Existing Phase 3 Implementation**
    Audit each completed plan:

    03-01 (Bare metal restriction) — VALID, KEEP
    - nvidia-wrapper.sh: Valid as UX layer (directs users to containers). Not security enforcement.
    - bare-metal-access CLI: Valid for video group exemption management.
    - resource-limits.yaml config: Valid.
    - Note: The wrapper checks video group membership, but the actual bare-metal deterrent is CUDA_VISIBLE_DEVICES. If a user is NOT in the video group AND overrides CUDA_VISIBLE_DEVICES, they CAN still access GPUs on bare metal. The wrapper only intercepts nvidia-* commands, not arbitrary CUDA programs. This is acceptable — Phase 2 awareness layer detects bare-metal GPU processes.

    03-02 (Container isolation) — VALID, KEEP, INDEPENDENT
    - Docker wrapper container isolation is independent of GPU access control.
    - No changes needed.

    03-03 (Deployment integration) — NEEDS MAJOR REVISION
    - REMOVE: udev rules (99-ds01-nvidia.rules) — anti-pattern per research
    - REMOVE: Device permission enforcement (0660) — breaks nvidia-smi
    - REVISE: Video group sync — should ADD exempt users, not remove others (device permissions are 0666 by default, so non-exempt users are restricted by CUDA_VISIBLE_DEVICES, not by file permissions)
    - KEEP: MOTD deployment
    - KEEP: bare-metal-access CLI deployment
    - KEEP: nvidia wrapper deployment (UX tool)
    - ADD: Profile.d update for exempt users (CUDA_VISIBLE_DEVICES should be unset for video group members)

    **6. What Needs to Change in 03-03**
    Specific changes for the updated 03-03-PLAN.md:
    - Remove udev rules file creation and deployment
    - Remove udevadm trigger and reload commands
    - Remove references to MODE="0660" device permissions
    - Video group sync becomes simpler: just ensure exempt users are in video group (no need to remove others — without udev rules enforcing 0660, video group membership alone doesn't grant/deny access. The actual mechanism is CUDA_VISIBLE_DEVICES.)
    - Update profile.d script: ds01-gpu-awareness.sh should check if user is in video group — if yes, don't set CUDA_VISIBLE_DEVICES="" (allow bare-metal access). This is the actual enforcement mechanism for exemptions.
    - Keep at/atd prerequisite, MOTD, wrapper deployment, bare-metal-access deployment

    **7. Open Items**
    - CVE-2025-23266: Verify NVIDIA Container Toolkit version (must be >= v1.16.2)
    - Profile.d exemption logic: Need to update ds01-gpu-awareness.sh to skip video group members
    - CUDA_VISIBLE_DEVICES is NOT a security boundary — documented and accepted. Container enforcement is the security layer.

    **Formatting:** Use markdown headers, bullet points, and a clear decision/rationale structure. Include a one-paragraph executive summary at the top. Keep it concise (~150-200 lines) — this is a decision document, not a research report.
  </action>
  <verify>
    - File .planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md exists
    - Research coverage: .planning/phases/02.1-gpu-access-control-research/02.1-RESEARCH.md exists and contains SLURM, Kubernetes, and bare-metal patterns (grep -l 'SLURM' and grep -l 'Kubernetes' and grep -l 'bare.metal')
    - Document contains "Chosen Approach" or "Design Decision" heading
    - Document mentions CUDA_VISIBLE_DEVICES as primary host-level mechanism
    - Document mentions Docker device mapping as security boundary
    - Document lists rejected alternatives (device permissions, udev rules)
    - Document references research findings: grep -i 'research\|02.1-RESEARCH' .planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md returns matches
    - Document references specific Phase 3 plans: grep '03-01' and grep '03-02' and grep '03-03' all return matches (not just generic "Phase 3" mentions)
    - Document identifies 03-03 changes needed (udev removal, video group revision)
    - Document is under 250 lines (concise decision document, not a research report)
  </verify>
  <done>
    Design document exists with clear chosen approach, rationale, rejected alternatives, Phase 3 audit, and specific 03-03 changes needed.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    1. Design decision document (02.1-DESIGN.md) synthesising research into concrete access control architecture
    2. Documents the three-layer approach: CUDA_VISIBLE_DEVICES + Docker device mapping + video group exemptions
    3. Audits all Phase 3 plans against research findings
    4. Identifies specific changes needed for 03-03-PLAN.md
  </what-built>
  <how-to-verify>
    1. **Design doc review** — Read 02.1-DESIGN.md: confirm three-layer approach (CUDA_VISIBLE_DEVICES + Docker device mapping + video group), rejected alternatives, and specific 03-03 revision items are all present.

    2. **Deployment functional** — Run these checks:
       ```bash
       cat /etc/profile.d/ds01-gpu-awareness.sh   # Profile.d deployed
       nvidia-smi -L                                # nvidia-smi works
       echo "CUDA_VISIBLE_DEVICES='$CUDA_VISIBLE_DEVICES'"  # Empty string set
       python3 scripts/docker/gpu_allocator_v2.py status     # GPU allocator functional
       ```

    3. **Approve 03-03 revisions** — Confirm you agree with the design's 03-03 changes: remove udev rules, keep video group for exemptions only, add profile.d exemption logic.
  </how-to-verify>
  <resume-signal>Type "approved" to proceed to Plan 02 (Phase 3 plan updates), or describe any changes needed to the design.</resume-signal>
</task>

</tasks>

<verification>
1. **Design doc complete:** 02.1-DESIGN.md exists with three-layer approach, rejected alternatives, and Phase 3 audit referencing 03-01, 03-02, 03-03 by ID
2. **Research referenced:** Design doc links to 02.1-RESEARCH.md findings; RESEARCH.md covers SLURM, Kubernetes, bare-metal patterns
3. **Deployment functional:** nvidia-smi works, CUDA_VISIBLE_DEVICES="" set in profile.d, GPU allocator status reports correctly
4. **Human approved:** Design decisions and 03-03 revision plan reviewed and accepted
</verification>

<success_criteria>
- Design document captures the research-informed GPU access control architecture
- Chosen approach and rejected alternatives clearly documented
- Phase 3 plans audited with specific revision instructions for 03-03
- Current deployment verified working (nvidia-smi, CUDA hiding, container deploy)
- Human has reviewed and approved the design before Phase 3 plan updates proceed
</success_criteria>

<output>
After completion, create `.planning/phases/02.1-gpu-access-control-research/02.1-01-SUMMARY.md`
</output>
