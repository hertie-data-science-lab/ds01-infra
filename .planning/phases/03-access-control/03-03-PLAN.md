---
phase: 03-access-control
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - scripts/system/deploy.sh
  - config/deploy/profile.d/ds01-motd.sh
  - config/deploy/profile.d/ds01-gpu-awareness.sh
autonomous: false

must_haves:
  truths:
    - "deploy.sh deploys nvidia-* wrappers to /usr/local/bin"
    - "deploy.sh deploys bare-metal-access to /usr/local/bin"
    - "deploy.sh ensures at command is installed and atd is active"
    - "deploy.sh syncs bare_metal_access.exempt_users to video group (adding exempt users)"
    - "deploy.sh deploys updated ds01-gpu-awareness.sh with video group exemption check"
    - "MOTD mentions container-only GPU access policy"
    - "All nvidia-* wrappers are deployed and executable"
    - "Device permissions remain at defaults (0666) — NO udev rules deployed"
  artifacts:
    - path: "scripts/system/deploy.sh"
      provides: "Deployment of access control components"
      contains: "nvidia-wrapper"
    - path: "config/deploy/profile.d/ds01-motd.sh"
      provides: "Updated MOTD with access control notice"
      contains: "container-only"
    - path: "config/deploy/profile.d/ds01-gpu-awareness.sh"
      provides: "Updated GPU awareness script with video group exemption"
      contains: "video"
  key_links:
    - from: "scripts/system/deploy.sh"
      to: "/usr/local/bin/nvidia-smi"
      via: "copies nvidia-wrapper.sh as nvidia-smi, nvidia-settings, etc."
      pattern: "nvidia"
    - from: "scripts/system/deploy.sh"
      to: "/etc/profile.d/ds01-gpu-awareness.sh"
      via: "copies updated awareness script"
      pattern: "profile.d"
    - from: "scripts/system/deploy.sh"
      to: "config/resource-limits.yaml"
      via: "reads bare_metal_access.exempt_users for video group sync"
      pattern: "bare_metal_access"
---

<objective>
Integrate access control components into the deploy pipeline and update MOTD.

Purpose: Wire the bare metal restriction and container isolation into the deployment system so they are activated consistently via `sudo deploy`. Update MOTD to inform users about the container-only GPU access policy.

**Architecture context:** This plan implements the three-layer GPU access control architecture defined in .planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md:
- Layer 1: Host-level deterrent (CUDA_VISIBLE_DEVICES via profile.d)
- Layer 2: Container enforcement (Docker device mapping, already deployed)
- Layer 3: Exemption management (video group + profile.d exemption check)

**Key revision:** This plan was revised to remove udev rules and device permission manipulation (anti-patterns per GPU access control research). CUDA_VISIBLE_DEVICES via profile.d script is the host-level mechanism. Docker --gpus device mapping is the security boundary.

Output: Updated deploy.sh with nvidia-* wrapper deployment, video group sync, and profile.d deployment. Updated MOTD with access control notice.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md
@.planning/phases/03-access-control/03-CONTEXT.md
@.planning/phases/03-access-control/03-01-SUMMARY.md
@.planning/phases/03-access-control/03-02-SUMMARY.md
@scripts/system/deploy.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add access control deployment to deploy.sh and create MOTD</name>
  <files>
    scripts/system/deploy.sh
    config/deploy/profile.d/ds01-motd.sh
  </files>
  <action>
    **scripts/system/deploy.sh** — Add a new section "Deploy Access Control" after the Internal commands section. This section:

    1. **Prerequisites — ensure `at` command is available:**
       The temporary grant revocation system (bare-metal-access CLI) requires `at`/`atd`. Install if missing:
       ```bash
       if ! command -v at &>/dev/null || ! systemctl is-active --quiet atd; then
           echo -e "  ${DIM}Installing at scheduler (required for temporary grants)...${NC}"
           apt-get install -y at &>/dev/null && systemctl enable --now atd &>/dev/null && \
               echo -e "  ${GREEN}✓${NC} at/atd installed and active" || \
               echo -e "  ${YELLOW}!${NC} WARNING: Failed to install at — temporary grants will not auto-revoke"
       fi
       ```

    2. **Deploy nvidia-* wrappers (UX tools, not security enforcement):**
       Deploy scripts/admin/nvidia-wrapper.sh to /usr/local/bin under multiple names:
       ```bash
       NVIDIA_COMMANDS="nvidia-smi nvidia-settings nvidia-debugdump nvidia-cuda-mps-control nvidia-cuda-mps-server nvidia-xconfig"
       for cmd in $NVIDIA_COMMANDS; do
           if [ -f "/usr/bin/$cmd" ]; then
               deploy_cmd "$INFRA_ROOT/scripts/admin/nvidia-wrapper.sh" "$cmd" "Access Control"
           fi
       done
       ```
       Only deploy wrappers for nvidia commands that actually exist in /usr/bin (skip nvidia-xconfig etc. if not installed). Use the existing deploy_cmd function.

       **Note:** These wrappers are UX tools (helpful error messages), NOT security enforcement. They direct users to containers and provide contextual guidance. Actual GPU access control happens via CUDA_VISIBLE_DEVICES (Layer 1 deterrent) and Docker device mapping (Layer 2 security boundary).

    3. **Deploy bare-metal-access admin CLI:**
       ```bash
       deploy_cmd "$INFRA_ROOT/scripts/admin/bare-metal-access" "bare-metal-access" "Access Control"
       ```

    4. **Deploy updated ds01-gpu-awareness.sh (Layer 1 + Layer 3):**
       The updated version (from plan 02.1-02) includes video group exemption logic. Copy to /etc/profile.d/:
       ```bash
       if [ -f "$INFRA_ROOT/config/deploy/profile.d/ds01-gpu-awareness.sh" ]; then
           cp "$INFRA_ROOT/config/deploy/profile.d/ds01-gpu-awareness.sh" /etc/profile.d/ds01-gpu-awareness.sh
           chmod 644 /etc/profile.d/ds01-gpu-awareness.sh
           echo -e "  ${GREEN}✓${NC} GPU awareness layer deployed (CUDA_VISIBLE_DEVICES with video group exemption)"
       fi
       ```

       This replaces the simple CUDA_VISIBLE_DEVICES="" export with the exemption-aware version. Users in video group bypass GPU hiding (Layer 3: exemption management).

    5. **Sync video group membership from config:**
       Read bare_metal_access.exempt_users from resource-limits.yaml and ensure those users are in the video group. The video group is the exemption marker checked by ds01-gpu-awareness.sh.

       **CRITICAL: YAML parse failure must abort the sync, NOT fall back to a partial list.** If the fallback were `|| echo "datasciencelab"`, any YAML error would cause incorrect group membership.

       **Simplified video group sync (no removal of non-exempt users):**
       Without udev rules enforcing device permissions 0660, video group membership alone doesn't grant GPU access. The actual mechanism is the profile.d script checking video group to decide whether to set CUDA_VISIBLE_DEVICES. Therefore, we only need to ensure exempt users are IN the video group. Users with active temporary grants remain in the video group until revocation (bare-metal-access revoke removes them).

       ```bash
       echo -e "${DIM}Syncing video group membership...${NC}"
       # Read exempt users from config — abort sync on failure
       CONFIG_FILE="$INFRA_ROOT/config/resource-limits.yaml"
       EXEMPT_USERS=$(python3 -c "
       import yaml
       with open('$CONFIG_FILE') as f:
           config = yaml.safe_load(f)
       bma = config.get('bare_metal_access', {})
       users = bma.get('exempt_users', [])
       print(' '.join(str(u) for u in users))
       " 2>/dev/null)

       if [[ -z "$EXEMPT_USERS" ]]; then
           echo -e "  ${YELLOW}!${NC} WARNING: Failed to read exempt_users from config. Skipping video group sync."
       else
           # Ensure exempt users are in video group
           for user in $EXEMPT_USERS; do
               if id "$user" &>/dev/null; then
                   if ! groups "$user" 2>/dev/null | grep -q '\bvideo\b'; then
                       usermod -aG video "$user" 2>/dev/null && \
                           echo -e "  ${GREEN}+${NC} Added $user to video group (exempt)" || true
                   fi
               fi
           done
           echo -e "  ${GREEN}✓${NC} Video group synced (exempt users added)"
       fi
       ```

       **Why no removal logic:** Video group membership is only meaningful when combined with profile.d exemption. Users NOT in exempt list who are in video group (e.g., from temporary grants) will have grants revoked by bare-metal-access revoke, which removes them from video group. No need for deploy.sh to clean up.

    6. **Ensure state directories exist:**
       ```bash
       mkdir -p /var/lib/ds01/rate-limits 2>/dev/null || true
       mkdir -p /var/lib/ds01/bare-metal-grants 2>/dev/null || true
       ```

    7. **Deploy MOTD:**
       ```bash
       if [ -f "$INFRA_ROOT/config/deploy/profile.d/ds01-motd.sh" ]; then
           cp "$INFRA_ROOT/config/deploy/profile.d/ds01-motd.sh" /etc/profile.d/ds01-motd.sh
           chmod 644 /etc/profile.d/ds01-motd.sh
           echo -e "  ${GREEN}✓${NC} MOTD deployed to /etc/profile.d/"
       fi
       ```

    8. **Add "Access Control" to the summary print_category calls at the end of deploy.sh.**

    Confirm the existing `deploy_cmd "$INFRA_ROOT/scripts/docker/docker-wrapper.sh" "docker" "Internal"` line remains — this already deploys the Docker wrapper with container isolation from Plan 03-02.

    **config/deploy/profile.d/ds01-motd.sh** — Check if this file already exists. If it does, update it. If not, create it. This script runs on user login (via /etc/profile.d/) and shows a brief MOTD:

    ```bash
    #!/bin/bash
    # DS01 Message of the Day
    # Deployed to /etc/profile.d/ds01-motd.sh

    # Only show for interactive shells
    [[ $- == *i* ]] || return

    # Only show once per day per user
    MOTD_STATE="/tmp/.ds01-motd-$(whoami)-$(date +%Y%m%d)"
    [[ -f "$MOTD_STATE" ]] && return
    touch "$MOTD_STATE" 2>/dev/null || true

    echo ""
    echo "  DS01 GPU Server"
    echo "  ───────────────────────────────────────"
    echo "  GPU access is container-only by default."
    echo "  Quick start: container deploy my-project"
    echo "  Help:        help"
    echo ""
    ```

    **Important notes for deployment:**
    - Device permissions remain at defaults (0666) — nvidia-smi and monitoring tools work for all users
    - CUDA_VISIBLE_DEVICES is a UX/deterrent layer (Layer 1), NOT a security boundary
    - Docker --gpus device mapping is the actual security enforcement (Layer 2)
    - nvidia-* wrappers are UX tools (helpful error messages), not security enforcement
    - Video group membership + profile.d exemption check = opt-in bare-metal access (Layer 3)
  </action>
  <verify>
    - `bash -n scripts/system/deploy.sh` exits 0 (syntax check)
    - grep for `nvidia-wrapper\|nvidia-smi` in deploy.sh returns matches
    - grep for `bare-metal-access` in deploy.sh returns matches
    - grep for `video` in deploy.sh returns matches (group sync)
    - grep for `Access Control` in deploy.sh returns matches (category)
    - grep for `ds01-gpu-awareness.sh` in deploy.sh returns matches (profile.d deployment)
    - grep for `apt.*install.*at\|command -v at` in deploy.sh returns match (at prerequisite)
    - grep for `Skipping video group sync` in deploy.sh returns match (YAML failure fallback)
    - NO udev rule deployment: grep for 'MODE.*0660' in deploy.sh returns no matches
    - NO udev rule deployment: grep for '99-ds01-nvidia.rules' in deploy.sh returns no matches
    - File config/deploy/profile.d/ds01-motd.sh exists
    - grep for `container-only` in config/deploy/profile.d/ds01-motd.sh returns match
  </verify>
  <done>
    deploy.sh installs at/atd prerequisite, deploys nvidia-* wrappers (UX tools), bare-metal-access CLI, updated ds01-gpu-awareness.sh with video group exemption (replaces old version), syncs video group from config exempt list (adding exempt users only), creates state directories. MOTD informs users about container-only GPU policy. Device permissions remain at defaults (no udev rules). Profile.d exemption logic is the actual bare-metal access control mechanism for Layer 3.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete Phase 3 access control:
    1. nvidia-* command wrappers that provide UX guidance (video group check)
    2. bare-metal-access admin CLI for temporary/permanent access grants
    3. Docker wrapper expanded with container isolation (user filtering, ownership verification)
    4. Deployment integration (deploy.sh, video group sync, profile.d, MOTD)
    5. Configuration in resource-limits.yaml (exempt users, admin model, wrapper settings)
    6. Updated ds01-gpu-awareness.sh with video group exemption logic

    **Architecture:** Three-layer GPU access control per 02.1-DESIGN.md
    - Layer 1: CUDA_VISIBLE_DEVICES="" (host deterrent)
    - Layer 2: Docker --gpus device=UUID (container security boundary)
    - Layer 3: Video group exemption (opt-in bare-metal access)
  </what-built>
  <how-to-verify>
    Review the implementation before deploying:

    1. Check nvidia wrapper logic:
       ```bash
       cat scripts/admin/nvidia-wrapper.sh
       # Verify: video group check, contextual error, rate limiting
       ```

    2. Check bare-metal-access CLI:
       ```bash
       bash -n scripts/admin/bare-metal-access
       # Verify: grant, revoke, status subcommands
       ```

    3. Check Docker wrapper changes:
       ```bash
       # Verify container isolation logic:
       grep -n "verify_container_ownership\|filter_container_list\|is_admin" scripts/docker/docker-wrapper.sh
       ```

    4. Check resource-limits.yaml additions:
       ```bash
       python3 -c "import yaml; c=yaml.safe_load(open('config/resource-limits.yaml')); print(c.get('bare_metal_access',{})); print(c.get('access_control',{}))"
       ```

    5. Check deploy.sh integration:
       ```bash
       grep -A2 "nvidia\|bare-metal\|video\|Access Control" scripts/system/deploy.sh
       ```

    6. Check updated GPU awareness script:
       ```bash
       cat config/deploy/profile.d/ds01-gpu-awareness.sh
       # Verify: video group exemption check, interactive shell check
       ```

    7. Verify NO udev rules in deployment:
       ```bash
       grep -i "udev\|0660" scripts/system/deploy.sh
       # Should return no matches for udev rule deployment
       ```

    8. When satisfied, deploy in MONITORING MODE first (safe — logs but does not block):
       ```bash
       # Step 1: Deploy with monitoring mode (no enforcement, just logging)
       sudo DS01_ISOLATION_MODE=monitoring deploy --verbose

       # Step 2: Verify device permissions are DEFAULT (0666)
       ls -la /dev/nvidia*  # Should show crw-rw-rw- root:root (0666)

       # Step 3: Verify containers still work with --gpus
       docker run --rm --gpus device=0 nvidia/cuda:12.0-base nvidia-smi

       # Step 4: Check logs for any false positives
       journalctl -t ds01-wrapper --since "5 minutes ago"
       journalctl -t ds01-access --since "5 minutes ago"

       # Step 5: Test as non-admin user (should see MONITORING log but operations allowed)
       # su - testuser -c "docker ps"
       # su - testuser -c "nvidia-smi"  # Should be intercepted by wrapper (UX layer)

       # Step 6: Test GPU awareness layer
       # su - testuser -c "bash -c 'echo \$CUDA_VISIBLE_DEVICES'"  # Should be empty
       # su - exemptuser -c "bash -c 'echo \$CUDA_VISIBLE_DEVICES'"  # Should be unset (video group exempt)

       # Step 7: When confident, switch to full enforcement
       # Remove DS01_ISOLATION_MODE from environment or set to "full"
       # Re-run: sudo deploy --verbose
       ```
  </how-to-verify>
  <resume-signal>Type "approved" to complete Phase 3, or describe any issues found.</resume-signal>
</task>

</tasks>

<verification>
1. `bash -n scripts/system/deploy.sh` passes
2. deploy.sh contains at/atd prerequisite check and installation
3. deploy.sh contains nvidia wrapper deployment section (UX tools)
4. deploy.sh contains video group sync logic (adding exempt users)
5. deploy.sh aborts video group sync on YAML parse failure (not fallback to partial list)
6. deploy.sh deploys updated ds01-gpu-awareness.sh to /etc/profile.d/
7. deploy.sh does NOT deploy udev rules (no MODE="0660" enforcement)
8. deploy.sh does NOT manipulate device permissions
9. MOTD file exists at config/deploy/profile.d/ds01-motd.sh
10. After `sudo deploy`: nvidia-* wrappers present in /usr/local/bin
11. After `sudo deploy`: bare-metal-access present in /usr/local/bin
12. After `sudo deploy`: video group contains exempt users
13. After `sudo deploy`: /etc/profile.d/ds01-gpu-awareness.sh has video group exemption check
14. After `sudo deploy`: nvidia device permissions remain 0666 (default)
</verification>

<success_criteria>
- deploy.sh deploys all access control components (wrappers, CLI, profile.d, MOTD)
- NO udev rules deployed — device permissions remain at defaults (0666)
- at/atd installed and active (required for temporary grant auto-revocation)
- Video group synced from config exempt list (adding exempt users)
- Updated ds01-gpu-awareness.sh deployed with video group exemption logic
- YAML parse failure aborts sync rather than proceeding with incomplete data
- MOTD deployed to /etc/profile.d/
- All nvidia-* wrappers are functional after deployment
- Monitoring-mode deployment path verified before full enforcement
- Implementation aligns with three-layer architecture from 02.1-DESIGN.md
- Human has verified implementation before live deployment
</success_criteria>

<output>
After completion, create `.planning/phases/03-access-control/03-03-SUMMARY.md`
</output>
