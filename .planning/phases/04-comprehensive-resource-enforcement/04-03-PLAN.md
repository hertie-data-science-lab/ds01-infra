---
phase: 04-comprehensive-resource-enforcement
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - scripts/docker/gpu_allocator_v2.py
  - scripts/docker/get_resource_limits.py
autonomous: true

must_haves:
  truths:
    - "GPU quota limits are read from the same aggregate section as CPU/memory"
    - "gpu_allocator_v2.py checks per-user GPU quota from the unified resource framework"
    - "GPU quota enforcement applies to all container types (DS01-managed and external)"
    - "Admin users have no GPU quota limit"
  artifacts:
    - path: "scripts/docker/gpu_allocator_v2.py"
      provides: "Unified GPU quota check using resource-limits aggregate section"
      contains: "aggregate"
    - path: "config/resource-limits.yaml"
      provides: "gpu_limit field in aggregate section"
      contains: "gpu_limit"
  key_links:
    - from: "scripts/docker/gpu_allocator_v2.py"
      to: "scripts/docker/get_resource_limits.py"
      via: "imports or calls for aggregate limits"
      pattern: "get_aggregate_limits\\|aggregate"
---

<objective>
Unify GPU quota enforcement into the resource-limits aggregate framework, replacing the fragmented max_mig_instances/max_mig_per_container pattern with a cohesive resource model.

Purpose: Per CONTEXT.md decision, GPU quotas should be enforced "at same level as CPU/memory (not a separate system)". This plan refactors the GPU allocator to read limits from the aggregate section alongside CPU/memory, creating a consistent whole.

Output: Refactored GPU allocator with unified quota check, gpu_limit field added to aggregate config.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/04-comprehensive-resource-enforcement/04-CONTEXT.md
@.planning/phases/04-comprehensive-resource-enforcement/04-01-SUMMARY.md
@scripts/docker/gpu_allocator_v2.py
@scripts/docker/get_resource_limits.py
@config/resource-limits.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add gpu_limit to aggregate config and refactor GPU allocator quota check</name>
  <files>
    config/resource-limits.yaml
    scripts/docker/gpu_allocator_v2.py
    scripts/docker/get_resource_limits.py
  </files>
  <action>
1. **Extend resource-limits.yaml aggregate sections** — Add `gpu_limit` field to each group's aggregate section. This is the maximum number of GPU/MIG slots a user can have allocated simultaneously across all containers.

Values (from existing config — these already exist as `max_mig_instances` at per-container level, now also at aggregate level for unified enforcement):
- student aggregate: `gpu_limit: 3` (same as max_mig_instances)
- researcher aggregate: `gpu_limit: 6` (same as max_mig_instances)
- faculty aggregate: `gpu_limit: 8` (same as max_mig_instances)
- admin: no aggregate section (unlimited)

Keep the existing `max_mig_instances` and `max_mig_per_container` fields for backward compatibility — they still control per-container GPU limits. The new `gpu_limit` in `aggregate` is the per-USER total cap.

2. **Extend get_resource_limits.py** — Ensure `get_aggregate_limits()` includes `gpu_limit` in its output. Add CLI option `--aggregate-gpu-limit` that returns just the GPU limit (used by GPU allocator for quick lookups).

3. **Refactor gpu_allocator_v2.py** — The GPU allocator already has quota checking logic (it checks `max_mig_instances`). Refactor to ALSO check the aggregate `gpu_limit`:

   a. Read the file to understand the current allocation flow (the `allocate` and `allocate-external` commands)
   b. Find the quota check in the allocation path (likely checks `max_mig_instances`)
   c. Add a new method `_check_aggregate_gpu_quota(self, username: str, requested_count: int) -> bool` that:
      - Calls `get_resource_limits.py --aggregate-gpu-limit <username>` (or imports ResourceLimitParser and calls get_aggregate_limits)
      - If result is None or "unlimited": return True (no limit)
      - Counts current GPU allocations for this user from gpu-state.json
      - If current_count + requested_count > gpu_limit: return False
      - Otherwise: return True
   d. Call this method in BOTH `allocate` and `allocate-external` paths, BEFORE the existing per-container quota check
   e. On failure, return a structured error: `AGGREGATE_GPU_QUOTA_EXCEEDED` with details showing current vs limit
   f. The Docker wrapper already handles displaying the QUOTA_EXCEEDED error — ensure the new error string matches the pattern the wrapper greps for (check `docker-wrapper.sh` line ~380: `grep -q "QUOTA_EXCEEDED\|USER_AT_LIMIT"`)

IMPORTANT: Keep backward compatibility. The existing `max_mig_instances` check (per-container) remains. The new aggregate `gpu_limit` check is an ADDITIONAL gate. Both must pass for allocation to proceed. This is the two-layer stacking model from CONTEXT.md.

IMPORTANT: Follow fail-open pattern. If get_resource_limits.py can't be called or returns an error, log to stderr and allow the allocation (never block on infrastructure failure).
  </action>
  <verify>
    - `python3 -c "import yaml; c=yaml.safe_load(open('config/resource-limits.yaml')); assert 'gpu_limit' in c['groups']['student']['aggregate']"`
    - `python3 scripts/docker/get_resource_limits.py testuser --aggregate` includes gpu_limit
    - `grep -q "aggregate.*gpu" scripts/docker/gpu_allocator_v2.py` (aggregate GPU check exists)
    - `ruff check scripts/docker/gpu_allocator_v2.py scripts/docker/get_resource_limits.py`
  </verify>
  <done>GPU allocator checks aggregate gpu_limit alongside existing per-container limits, creating unified two-layer enforcement</done>
</task>

</tasks>

<verification>
- Config valid: `python3 -c "import yaml; yaml.safe_load(open('config/resource-limits.yaml'))"`
- GPU limit in aggregate: `python3 scripts/docker/get_resource_limits.py testuser --aggregate` shows gpu_limit
- Allocator references aggregate: `grep -c "aggregate\|gpu_limit" scripts/docker/gpu_allocator_v2.py` >= 3
- Ruff clean: `ruff check scripts/docker/gpu_allocator_v2.py`
- Backward compatible: existing max_mig_instances checks still present
</verification>

<success_criteria>
1. GPU quota lives in aggregate section alongside CPU/memory — single framework
2. GPU allocator checks aggregate gpu_limit before per-container max_mig_instances
3. Error message matches wrapper's QUOTA_EXCEEDED pattern for consistent UX
4. Admin users have no GPU quota limit (unlimited)
5. Fail-open on infrastructure errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-comprehensive-resource-enforcement/04-03-SUMMARY.md`
</output>
