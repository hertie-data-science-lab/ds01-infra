---
phase: 04-comprehensive-resource-enforcement (post-merge fixes)
task: code fixes done, reboot pending
status: blocked_on_reboot
last_updated: 2026-02-06
branch: fix/cgroup-v2-paths
---

<current_state>
Phase 4 merged to main but UAT found 3 issues (2 blockers + 1 major) — all caused by
cgroup path construction assuming pure v2 when the system runs cgroup v1 hybrid.

Code fixes are DONE and committed on `fix/cgroup-v2-paths` branch (commit 1ce9968).
Next step is GRUB change to enable pure cgroup v2, then reboot, then test.
</current_state>

<completed_work>

- Phase 4 plans 01-05: All complete and merged to main (commit 5b82ecf)
- UAT: 8/11 pass, 3 failures identified (all cgroup path related)
- Code fix 1 (BLOCKER): docker-wrapper.sh — 3-tier cgroup path detection (v2/unified/v1),
  intermediate group slice level, version-aware memory.current vs memory.usage_in_bytes,
  v1 pids in separate hierarchy
- Code fix 2 (BLOCKER): collect-resource-stats.sh — detect_cgroup_root() function,
  nested group→user slice iteration (was flat), v1 memory/pids/PSI file handling
- Code fix 3 (MAJOR): ds01-quota-greeting.sh — queries nvidia-smi for MIG vs GPU topology
  (no hardcoded fallback), caps displayed limit to actual hardware, shows correct unit label
</completed_work>

<remaining_work>

1. GRUB change: add `systemd.unified_cgroup_hierarchy=1` to /etc/default/grub, run update-grub
2. Reboot the server
3. Verify cgroup v2 is active: `cat /sys/fs/cgroup/cgroup.controllers` should list controllers
4. Test docker-wrapper enforcement: `docker run --memory=500g hello-world` as non-admin should be blocked
5. Test collect-resource-stats: `sudo scripts/monitoring/collect-resource-stats.sh --verbose` should find slices
6. Test greeting: SSH login should show correct GPU count (capped to hardware)
7. Check `/var/log/ds01/resource-stats.log` has entries
8. Squash-merge fix/cgroup-v2-paths to main
9. Consider: Phase 5 (Lifecycle Bug Fixes) or address deferred backlog items
</remaining_work>

<decisions_made>

- Cgroup detection pattern: 3-tier fallback (pure v2 → unified → v1 memory) — matches
  the pattern already established in check-limits helper
- v1 pids in separate hierarchy: read from /sys/fs/cgroup/pids/ instead of same path
- v1 PSI: read from /sys/fs/cgroup/unified/ (only place PSI exists on v1 hybrid)
- GPU topology: query nvidia-smi -L once, count ^GPU lines and MIG lines separately,
  no hardcoded fallback (0 if nvidia-smi fails)
- Greeting label: "MIG slots" when MIG active, "GPUs" otherwise (matches check-limits)
</decisions_made>

<blockers>

- REBOOT REQUIRED: System must be rebooted with `systemd.unified_cgroup_hierarchy=1` GRUB
  parameter to switch from cgroup v1 hybrid to pure v2. Without this, the v1 fallback
  paths in the code will be used (which is correct but not the target state).
- This is a multi-user server — coordinate reboot timing with users.
</blockers>

<context>
The system runs kernel 5.15 with cgroup v1 hybrid. Memory stats are at
/sys/fs/cgroup/memory/... (v1), not /sys/fs/cgroup/... (v2). PSI is at
/sys/fs/cgroup/unified/... but has no memory.current there. Systemd slices
nest as: ds01.slice/ds01-{group}.slice/ds01-{group}-{user}.slice — the original
code missed the intermediate group slice level.

The code now handles BOTH v1 hybrid AND pure v2 correctly, so it will work
before and after the GRUB migration. The GRUB change is still recommended
because pure v2 gives unified memory+pids+PSI in one hierarchy.

Note: check-limits helper at line 188 also has a hardcoded `phys_count=4`
fallback — outside scope of this fix but worth addressing later.
</context>

<next_action>
1. Coordinate reboot window with users
2. Run: `sudo sed -i 's/GRUB_CMDLINE_LINUX="/GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=1 /' /etc/default/grub`
3. Run: `sudo update-grub`
4. Reboot
5. Verify and test (see remaining_work items 3-7)
6. `/squash-merge` the fix/cgroup-v2-paths branch to main
</next_action>
