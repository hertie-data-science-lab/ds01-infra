---
phase: 04-comprehensive-resource-enforcement
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - config/resource-limits.yaml
  - scripts/system/generate-user-slice-limits.py
  - scripts/system/setup-resource-slices.sh
  - scripts/system/create-user-slice.sh
  - scripts/docker/get_resource_limits.py
  - scripts/system/deploy.sh
autonomous: true

must_haves:
  truths:
    - "resource-limits.yaml contains per-user aggregate limits section for each group tier"
    - "Running generate-user-slice-limits.py produces systemd drop-in files with CPU/memory/pids limits for each user"
    - "deploy.sh calls the generator as part of deployment"
    - "Admin group (admin) has no aggregate limits (unlimited)"
  artifacts:
    - path: "config/resource-limits.yaml"
      provides: "Per-group aggregate section with cpu_quota, memory_max, memory_high, tasks_max"
      contains: "aggregate:"
    - path: "scripts/system/generate-user-slice-limits.py"
      provides: "Systemd drop-in generator from resource-limits.yaml"
      min_lines: 80
    - path: "scripts/docker/get_resource_limits.py"
      provides: "New --aggregate CLI option and get_aggregate_limits method"
      contains: "get_aggregate_limits"
  key_links:
    - from: "scripts/system/generate-user-slice-limits.py"
      to: "config/resource-limits.yaml"
      via: "yaml.safe_load reads aggregate section"
      pattern: "aggregate"
    - from: "scripts/system/generate-user-slice-limits.py"
      to: "/etc/systemd/system/ds01-*.slice.d/"
      via: "writes drop-in conf files"
      pattern: "10-resource-limits.conf"
    - from: "scripts/system/deploy.sh"
      to: "scripts/system/generate-user-slice-limits.py"
      via: "calls during deployment"
      pattern: "generate-user-slice-limits"
---

<objective>
Extend resource-limits.yaml with per-user aggregate limits and create a systemd drop-in generator that enforces CPU, memory, and pids limits on existing DS01 user slices.

Purpose: This is the foundation of Phase 4 — the aggregate config section and the drop-in generator that translates YAML config into systemd enforcement. All subsequent plans depend on this.

Output: Extended config schema, Python generator script, updated deploy.sh integration, updated get_resource_limits.py with aggregate support.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-comprehensive-resource-enforcement/04-CONTEXT.md
@.planning/phases/04-comprehensive-resource-enforcement/04-RESEARCH.md
@config/resource-limits.yaml
@scripts/system/create-user-slice.sh
@scripts/system/setup-resource-slices.sh
@scripts/docker/get_resource_limits.py
@scripts/system/deploy.sh
@config/groups/student.members
@config/groups/researcher.members
@config/groups/faculty.members
@config/groups/admin.members
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend resource-limits.yaml with aggregate section and create drop-in generator</name>
  <files>
    config/resource-limits.yaml
    scripts/system/generate-user-slice-limits.py
    scripts/docker/get_resource_limits.py
  </files>
  <action>
1. **Extend resource-limits.yaml** — Add an `aggregate` subsection to each group under `groups:`. This section defines PER-USER (not per-container) aggregate resource limits enforced via systemd user slices. The values come from the CONTEXT.md decision: per-container limit x max_containers_per_user.

Add to each group (preserve all existing fields, add `aggregate:` subsection):

```yaml
groups:
  student:
    # ... existing fields unchanged ...
    aggregate:
      cpu_quota: "9600%"      # 32 CPUs × 3 containers = 96 CPUs
      memory_max: "96G"       # 32GB × 3 = 96GB
      memory_high: "86G"      # 90% of 96GB
      tasks_max: 12288        # 4096 × 3

  researcher:
    # ... existing fields unchanged ...
    aggregate:
      cpu_quota: "24000%"     # 48 CPUs × 5 = 240 CPUs
      memory_max: "320G"      # 64GB × 5 = 320GB
      memory_high: "288G"     # 90% of 320GB
      tasks_max: 327680       # 65536 × 5

  faculty:
    # ... existing fields unchanged ...
    aggregate:
      cpu_quota: "32000%"     # 64 CPUs × 5 = 320 CPUs
      memory_max: "640G"      # 128GB × 5 = 640GB
      memory_high: "576G"     # 90% of 640GB
      tasks_max: 327680       # 65536 × 5

  admin:
    # ... existing fields unchanged ...
    # NOTE: No aggregate section — admin users have no aggregate cap
```

Also add a top-level `enforcement:` section:

```yaml
enforcement:
  aggregate_limits: true        # Enable per-user aggregate enforcement
  cgroup_driver: systemd        # Required Docker cgroup driver
```

2. **Create scripts/system/generate-user-slice-limits.py** — Python script that:
   - Reads resource-limits.yaml
   - Reads all group .members files to enumerate users
   - For each user, determines their group and aggregate limits
   - Generates/updates systemd drop-in files at `/etc/systemd/system/ds01-{group}-{sanitized_user}.slice.d/10-resource-limits.conf`
   - For admin users: does NOT generate drop-in (no aggregate cap)
   - Calls `systemctl daemon-reload` after writing
   - Cleans up stale drop-in dirs for users no longer in any group
   - Must use the same username sanitization as create-user-slice.sh (import from username_utils)
   - Drop-in content format:
     ```
     [Slice]
     CPUQuota={cpu_quota}
     MemoryMax={memory_max}
     MemoryHigh={memory_high}
     TasksMax={tasks_max}
     ```
   - Must be idempotent (running twice produces same result)
   - CLI: `generate-user-slice-limits.py [--dry-run] [--verbose]`
   - Dry-run mode prints what would be written without writing
   - Requires root (check at start)
   - Follow existing Python conventions (see scripts/docker/get_resource_limits.py for patterns)

3. **Extend get_resource_limits.py** — Add method `get_aggregate_limits(username)` that returns the aggregate limits dict for a user. Add CLI option `--aggregate` that prints the aggregate limits as JSON. This will be used by the Docker wrapper and check-limits.

The resolution order is: user_overrides aggregate > group aggregate > no aggregate (admin). If a user is in the admin group, return None (no aggregate limits).
  </action>
  <verify>
    - `python3 -c "import yaml; c=yaml.safe_load(open('config/resource-limits.yaml')); assert 'aggregate' in c['groups']['student']"`
    - `python3 scripts/system/generate-user-slice-limits.py --dry-run` runs without error and shows drop-in content
    - `python3 scripts/docker/get_resource_limits.py testuser --aggregate` returns JSON with aggregate limits
    - `ruff check scripts/system/generate-user-slice-limits.py scripts/docker/get_resource_limits.py`
  </verify>
  <done>resource-limits.yaml has aggregate sections, generator produces valid drop-in content in dry-run, get_resource_limits.py supports --aggregate</done>
</task>

<task type="auto">
  <name>Task 2: Update slice infrastructure and deploy.sh integration</name>
  <files>
    scripts/system/create-user-slice.sh
    scripts/system/setup-resource-slices.sh
    scripts/system/deploy.sh
    config/permissions-manifest.sh
  </files>
  <action>
1. **Update create-user-slice.sh** — After creating the slice file (existing behaviour), also call generate-user-slice-limits.py for that specific user to apply aggregate limits immediately. Add a `--user` flag to the generator that limits generation to a single user (avoids full regeneration on every container create). The slice creation remains idempotent — if drop-in already exists with correct content, skip.

2. **Update setup-resource-slices.sh** — After creating group slices, call `generate-user-slice-limits.py` to generate drop-ins for all existing users. This ensures a fresh `setup-resource-slices.sh` run configures everything.

3. **Update deploy.sh** — Add a section that:
   - Deploys `generate-user-slice-limits.py` as a symlink to `/usr/local/bin/ds01-generate-limits`
   - Calls `python3 $INFRA_ROOT/scripts/system/generate-user-slice-limits.py` during deployment to ensure all user slice limits are current
   - Add this AFTER the existing slice setup section, BEFORE the final summary
   - Use the existing `deploy_cmd` pattern for the symlink

4. **Update config/permissions-manifest.sh** — Add entry for `scripts/system/generate-user-slice-limits.py` with 755 permissions (world-executable, same as other Python scripts in system/).
  </action>
  <verify>
    - `grep -q "generate-user-slice-limits" scripts/system/deploy.sh`
    - `grep -q "generate-user-slice-limits" scripts/system/create-user-slice.sh`
    - `grep -q "generate-user-slice-limits" scripts/system/setup-resource-slices.sh`
    - `bash -n scripts/system/create-user-slice.sh` (syntax check)
    - `bash -n scripts/system/setup-resource-slices.sh` (syntax check)
  </verify>
  <done>create-user-slice.sh applies aggregate limits on creation, setup-resource-slices.sh regenerates all limits, deploy.sh integrates the generator into deployment pipeline</done>
</task>

</tasks>

<verification>
- resource-limits.yaml validates: `python3 -c "import yaml; yaml.safe_load(open('config/resource-limits.yaml'))"`
- Generator dry-run succeeds: `python3 scripts/system/generate-user-slice-limits.py --dry-run`
- get_resource_limits.py aggregate: `python3 scripts/docker/get_resource_limits.py datasciencelab --aggregate` returns None (admin)
- deploy.sh syntax: `bash -n scripts/system/deploy.sh`
- All Python passes ruff: `ruff check scripts/system/generate-user-slice-limits.py scripts/docker/get_resource_limits.py`
</verification>

<success_criteria>
1. resource-limits.yaml contains aggregate CPU/memory/pids limits for student, researcher, faculty groups
2. generate-user-slice-limits.py can produce correct systemd drop-in content for any user
3. deploy.sh calls the generator during deployment
4. create-user-slice.sh applies limits when creating new user slices
5. get_resource_limits.py exposes aggregate limits via API and CLI
</success_criteria>

<output>
After completion, create `.planning/phases/04-comprehensive-resource-enforcement/04-01-SUMMARY.md`
</output>
