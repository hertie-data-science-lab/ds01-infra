---
phase: 01-foundation-observability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/lib/ds01_events.py
  - scripts/lib/ds01_events.sh
  - config/deploy/logrotate.d/ds01
autonomous: true

must_haves:
  truths:
    - "Python scripts can import ds01_events and call log_event() to write structured JSON to /var/log/ds01/events.jsonl"
    - "Bash scripts can source ds01_events.sh and call log_event to write same JSON format to same file"
    - "Event logging never blocks the calling script — failures warn on stderr but execution continues"
    - "Logrotate correctly handles events.jsonl with copytruncate to avoid file descriptor issues"
  artifacts:
    - path: "scripts/lib/ds01_events.py"
      provides: "Shared Python event logging library"
      exports: ["log_event"]
      min_lines: 60
    - path: "scripts/lib/ds01_events.sh"
      provides: "Bash wrapper for event logging"
      contains: "log_event"
      min_lines: 20
    - path: "config/deploy/logrotate.d/ds01"
      provides: "Logrotate config with copytruncate for JSONL"
      contains: "copytruncate"
  key_links:
    - from: "scripts/lib/ds01_events.sh"
      to: "scripts/lib/ds01_events.py"
      via: "python3 CLI call"
      pattern: "python3.*ds01_events"
    - from: "scripts/lib/ds01_events.py"
      to: "/var/log/ds01/events.jsonl"
      via: "file append"
      pattern: "events\\.jsonl"
---

<objective>
Create the shared event logging library (Python + Bash bridge) and fix logrotate configuration for JSONL files.

Purpose: This is the foundation for all Phase 1 event logging. Every other plan that logs events depends on this shared library. The library establishes the standardised JSON envelope schema (timestamp, event_type, user, source, details{}) and ensures both Python and Bash scripts can emit events in identical format.

Output: `scripts/lib/ds01_events.py` (importable Python module), `scripts/lib/ds01_events.sh` (sourceable Bash wrapper), updated logrotate config with `copytruncate` for JSONL.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-observability/01-CONTEXT.md
@.planning/phases/01-foundation-observability/01-RESEARCH.md
@scripts/lib/ds01_core.py
@scripts/lib/init.sh
@scripts/docker/event-logger.py
@config/deploy/logrotate.d/ds01
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create shared Python event logging library</name>
  <files>scripts/lib/ds01_events.py</files>
  <action>
Create `scripts/lib/ds01_events.py` as an importable Python module following the conventions in `ds01_core.py`.

**Schema (standardised envelope from CONTEXT.md decisions):**
```python
{
    "timestamp": "2026-01-30T14:30:00Z",  # UTC ISO 8601
    "event_type": "container.create",      # Dot-separated category.action
    "user": "alice",                       # Optional, omit if system event
    "source": "docker-wrapper",            # Script/component name
    "details": {                           # Optional, event-specific fields
        "container": "alice-jupyter",
        "image": "ds01/pytorch:latest"
    },
    "schema_version": "1"                  # For future evolution
}
```

**Implementation requirements:**
- `log_event(event_type: str, user: str | None = None, source: str | None = None, **details) -> bool`
- Return True on success, False on failure
- Write to `EVENTS_FILE = Path("/var/log/ds01/events.jsonl")`
- Create parent directories if missing (`mkdir -p` equivalent)
- Append single JSON line per event (must be under 4KB for atomic writes — see PIPE_BUF in research)
- Use `datetime.now(timezone.utc).isoformat()` with 'Z' suffix for timestamps
- On ANY exception: print warning to stderr, return False, NEVER raise
- Do NOT configure Python logging handlers at import time (anti-pattern from research Pitfall 4)
- Add `NullHandler` to module logger
- Include module docstring with usage examples
- Include `EVENT_TYPES` dict documenting known event types and their expected detail fields (carry forward from event-logger.py but with expanded scope from CONTEXT.md)
- Follow existing conventions: type hints, Google-style docstrings, `from __future__ import annotations`

**Also add a CLI interface** so bash can call it directly:
```
python3 scripts/lib/ds01_events.py log container.create user=alice source=docker-wrapper container=proj
```
This enables the bash wrapper to invoke it without subprocess overhead of importing.
  </action>
  <verify>
Run: `python3 -c "import sys; sys.path.insert(0, '/opt/ds01-infra/scripts/lib'); from ds01_events import log_event; result = log_event('test.selftest', source='verify'); print('OK' if result else 'FAIL')"`
Then verify the JSONL line was written: `tail -1 /var/log/ds01/events.jsonl | python3 -m json.tool` should show valid JSON with timestamp, event_type, source, schema_version fields.
  </verify>
  <done>
- `ds01_events.py` exists in `scripts/lib/`
- `log_event()` successfully writes structured JSON to `/var/log/ds01/events.jsonl`
- JSON envelope contains: timestamp, event_type, source, schema_version, and optionally user + details
- Failures warn on stderr but never raise exceptions
- CLI mode works: `python3 scripts/lib/ds01_events.py log test.cli source=verify`
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Bash event logging wrapper</name>
  <files>scripts/lib/ds01_events.sh</files>
  <action>
Create `scripts/lib/ds01_events.sh` as a sourceable Bash function library.

**Function signature:**
```bash
# Usage: log_event <event_type> [user] [source] [key=value ...]
# If user is empty string "", it is omitted from the event.
# If source is omitted, defaults to basename of calling script.
log_event() { ... }
```

**Implementation:**
- Call the Python module's CLI interface: `python3 /opt/ds01-infra/scripts/lib/ds01_events.py log "$event_type" [args...]`
- Pass key=value pairs through directly
- If user is provided and non-empty, pass `user=$user`
- If source is provided and non-empty, pass `source=$source`; otherwise pass `source=$(basename "$0")`
- Capture stderr but do NOT fail the calling script (use `2>/dev/null || true` pattern)
- Keep the function under 30 lines — it's a thin wrapper

**Important:** The bash wrapper must NEVER cause the calling script to exit on failure (even with `set -e`). Use subshell or explicit error suppression.

Follow conventions from `init.sh`: function name `log_event`, local variables, comment header with usage.
  </action>
  <verify>
Run from bash: `source /opt/ds01-infra/scripts/lib/ds01_events.sh && log_event "test.bash_wrapper" "" "verify-task2" result=success`
Then verify: `tail -1 /var/log/ds01/events.jsonl | python3 -c "import sys,json; e=json.load(sys.stdin); print('OK' if e['event_type']=='test.bash_wrapper' and e['source']=='verify-task2' else 'FAIL')"`
  </verify>
  <done>
- `ds01_events.sh` exists in `scripts/lib/`
- `log_event` function callable from bash after sourcing
- Events written match same JSON schema as Python library
- Function never causes calling script to exit on failure
  </done>
</task>

<task type="auto">
  <name>Task 3: Fix logrotate configuration for JSONL files</name>
  <files>config/deploy/logrotate.d/ds01</files>
  <action>
Update `config/deploy/logrotate.d/ds01` to fix the JSONL rotation section.

**Current issue:** The JSONL section is missing `copytruncate` which is critical for append-only files (open file descriptors will keep writing to the old file after rotation without it).

**Changes to the JSONL section (`/var/log/ds01/*.jsonl`):**
1. Add `copytruncate` — copy file contents, then truncate original (keeps file descriptors valid)
2. Add `maxsize 100M` — rotate early if file exceeds 100MB before daily rotation
3. Keep existing: daily, rotate 30, compress, delaycompress, missingok, notifempty, dateext, dateformat
4. Remove `create 0644 root root` — not needed with copytruncate (file is truncated, not replaced)

Leave the other sections (*.log, gpu-utilization.jsonl) unchanged.
  </action>
  <verify>
Run: `grep -c "copytruncate" /opt/ds01-infra/config/deploy/logrotate.d/ds01` should return at least 1.
Verify syntax: `logrotate -d /opt/ds01-infra/config/deploy/logrotate.d/ds01 2>&1 | head -20` (dry-run should not show errors, though it may warn about state file).
  </verify>
  <done>
- Logrotate config updated with `copytruncate` for JSONL section
- `maxsize 100M` added as safety valve
- Dry-run logrotate shows no syntax errors for the config
  </done>
</task>

</tasks>

<verification>
1. Python library: `python3 -c "import sys; sys.path.insert(0, '/opt/ds01-infra/scripts/lib'); from ds01_events import log_event; assert log_event('verify.python', source='plan-01-verify')"`
2. Bash wrapper: `bash -c 'source /opt/ds01-infra/scripts/lib/ds01_events.sh; log_event verify.bash "" plan-01-verify'`
3. Schema validation: `tail -2 /var/log/ds01/events.jsonl | jq -e '.timestamp and .event_type and .schema_version'`
4. Logrotate: `grep copytruncate /opt/ds01-infra/config/deploy/logrotate.d/ds01`
</verification>

<success_criteria>
- Shared event logging library works from both Python and Bash
- Events written in standardised JSON envelope with timestamp, event_type, source, schema_version
- Logging never blocks calling scripts
- Logrotate correctly configured with copytruncate for JSONL
- Library ready for downstream consumers (event-logger.py refactor, script instrumentation)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-observability/01-01-SUMMARY.md`
</output>
