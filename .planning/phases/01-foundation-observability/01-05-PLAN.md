---
phase: 01-foundation-observability
plan: 05
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - scripts/docker/event-logger.py
  - scripts/monitoring/ds01-events
autonomous: true

must_haves:
  truths:
    - "event-logger.py uses the shared ds01_events library for consistent schema (timestamp, event_type, user, source, details)"
    - "ds01-events supports --user, --type, --since, --until, --container filters with AND logic"
    - "ds01-events has --json flag for machine-readable output"
    - "ds01-events has --follow flag for live event streaming"
    - "ds01-events has --summary flag for aggregate view (counts by type, by user)"
    - "ds01-events defaults to showing last 50 events in human-readable table"
    - "ds01-events supports 4-tier help (--help, --info, --concepts, --guided)"
  artifacts:
    - path: "scripts/docker/event-logger.py"
      provides: "Event logger CLI using shared library"
      contains: "from ds01_events import"
    - path: "scripts/monitoring/ds01-events"
      provides: "Enhanced event query tool"
      contains: "jq"
      min_lines: 150
  key_links:
    - from: "scripts/docker/event-logger.py"
      to: "scripts/lib/ds01_events.py"
      via: "Python import"
      pattern: "from ds01_events import"
    - from: "scripts/monitoring/ds01-events"
      to: "/var/log/ds01/events.jsonl"
      via: "jq queries"
      pattern: "jq.*events\\.jsonl"
---

<objective>
Refactor event-logger.py to use the shared logging library and rewrite ds01-events as a first-class admin query tool with jq-based filtering.

Purpose: With the shared logging library created in Plan 01, the existing event-logger.py needs to delegate its logging to `ds01_events.py` for schema consistency. The ds01-events query tool needs a major upgrade from grep-based filtering to jq-based structured queries, plus new features (--since, --follow, --summary, --json, 4-tier help) to become the "first-class admin tool" described in CONTEXT.md.

Output: Refactored `event-logger.py` (thin wrapper around shared library), rewritten `ds01-events` (jq-based, full-featured query tool).
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-observability/01-CONTEXT.md
@.planning/phases/01-foundation-observability/01-RESEARCH.md
@.planning/phases/01-foundation-observability/01-01-SUMMARY.md
@scripts/docker/event-logger.py
@scripts/monitoring/ds01-events
@scripts/lib/ds01_core.py
@scripts/lib/init.sh
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor event-logger.py to use shared library</name>
  <files>scripts/docker/event-logger.py</files>
  <action>
Refactor `scripts/docker/event-logger.py` to delegate logging to the shared `ds01_events.py` library from Plan 01.

**What to keep:**
- CLI interface (`log`, `tail`, `search`, `user`, `container`, `types` commands)
- `EVENT_TYPES` dict (but expand with new event types from CONTEXT.md)
- Module docstring explaining append-only audit trail design
- The principle: "If events.jsonl is lost, NO IMPACT on system operation"

**What to change:**
1. Import and delegate to shared library:
   ```python
   sys.path.insert(0, str(Path(__file__).parent.parent / "lib"))
   from ds01_events import log_event, EVENTS_FILE
   ```

2. Remove the `EventLogger` class — replace with calls to `log_event()` from shared library for the `log` command, and direct file reads for `tail`/`search` (or keep a simplified reader class).

3. Remove the internal `_maybe_rotate` method — logrotate handles rotation now (Plan 01, Task 3).

4. Update `EVENT_TYPES` to include expanded scope from CONTEXT.md:
   ```python
   EVENT_TYPES = {
       # Container lifecycle
       "container.create": ["user", "container", "image", "gpu"],
       "container.start": ["user", "container"],
       "container.stop": ["user", "container", "reason"],
       "container.remove": ["user", "container"],
       # GPU allocation
       "gpu.allocate": ["user", "container", "gpu_uuid", "mig_profile"],
       "gpu.release": ["user", "container", "gpu_uuid", "reason"],
       "gpu.reject": ["user", "container", "reason"],
       # Auth events
       "auth.denied": ["user", "reason", "requested"],
       # Resource events
       "resource.cgroup_limit": ["user", "container", "resource", "limit"],
       "resource.oom_kill": ["user", "container"],
       # Maintenance
       "maintenance.cleanup": ["containers_stopped", "gpus_released", "source"],
       "maintenance.idle_kill": ["user", "container", "idle_duration"],
       "maintenance.runtime_kill": ["user", "container", "runtime"],
       # Monitoring
       "monitoring.dcgm_restart": ["reason"],
       "monitoring.scrape_failure": ["target", "error"],
       # Config changes
       "config.change": ["field", "old_value", "new_value", "changed_by"],
       # Unmanaged workload detection (LOG-03)
       "detection.unmanaged_container": ["container", "user", "source"],
       "detection.host_gpu_process": ["user", "pid", "command"],
   }
   ```

5. The `log` CLI command should call `log_event()` from the shared library, passing kwargs.

6. The `tail` and `search` commands can read the JSONL file directly (simple file reads, no need for the full EventLogger class).

**Schema migration note:** The old schema used `ts` and `event` fields. The new schema uses `timestamp` and `event_type`. The query tool (Task 2) should handle both for backward compatibility with any existing events.
  </action>
  <verify>
Run: `python3 /opt/ds01-infra/scripts/docker/event-logger.py log test.refactor user=verify source=plan-05`
Then: `tail -1 /var/log/ds01/events.jsonl | python3 -c "import sys,json; e=json.load(sys.stdin); assert 'timestamp' in e and 'event_type' in e, f'Missing fields: {e.keys()}'; print('OK')"` should print OK (new schema).
Run: `python3 /opt/ds01-infra/scripts/docker/event-logger.py types` should list expanded event types.
  </verify>
  <done>
- event-logger.py imports and delegates to shared ds01_events library
- EventLogger class removed (replaced by shared library + simple file readers)
- Internal rotation removed (logrotate handles it)
- EVENT_TYPES expanded with full scope from CONTEXT.md
- CLI interface preserved and working
- Events written use new schema (timestamp, event_type, details{})
  </done>
</task>

<task type="auto">
  <name>Task 2: Rewrite ds01-events as first-class query tool</name>
  <files>scripts/monitoring/ds01-events</files>
  <action>
Rewrite `scripts/monitoring/ds01-events` as a jq-based query tool following CONTEXT.md decisions.

**Interface design (from CONTEXT.md):**
```
ds01-events [OPTIONS]

Options:
  --user USER          Filter by username
  --type TYPE          Filter by event type (prefix match: "container" matches "container.create")
  --since TIMESPEC     Events after this time (ISO 8601 or relative: "1 hour ago", "today", "yesterday")
  --until TIMESPEC     Events before this time
  --container NAME     Filter by container name
  --limit N            Number of events to show (default: 50)
  --all                Show all events (no limit)
  --json               Machine-readable JSON output (one object per line)
  --follow             Live stream new events (tail -f with jq filtering)
  --summary            Aggregate view: counts by type, by user, time distribution
  --help               Quick reference
  --info               Full reference (all options and examples)
  --concepts           What are events, how logging works, schema format
  --guided             Interactive walkthrough
```

**Implementation approach:**
- Source `scripts/lib/init.sh` for colour variables and paths
- Use `jq` for ALL filtering (not grep) — structured JSON parsing
- Filters combine with AND logic
- Default output: human-readable table with columns: TIMESTAMP | EVENT_TYPE | USER | DETAILS
- `--json` output: raw JSONL lines (pass through filtered events)
- `--follow`: `tail -f $EVENTS_FILE | jq --unbuffered [filters]`
- `--summary`: `jq` aggregation piped to format
- `--since`/`--until`: Convert relative times via `date -d` to ISO 8601, then jq lexical comparison

**Backward compatibility:** Handle both old schema (`ts`, `event`) and new schema (`timestamp`, `event_type`):
```bash
# jq filter that works with both schemas
jq_timestamp='.timestamp // .ts'
jq_event_type='.event_type // .event'
```

**4-tier help system:**
- `--help`: ~15 lines, show usage + common examples
- `--info`: Full option reference with all flags and examples
- `--concepts`: Explain event logging architecture, schema, how events are generated
- `--guided`: Interactive — ask what they want to find, build the command

**Important implementation details:**
- Check `jq` is installed, show helpful message if missing
- Handle empty events file gracefully
- Handle malformed JSON lines gracefully (jq -R with try-catch)
- For `--since "1 hour ago"`: convert using `date -d "1 hour ago" -Iseconds`
- Column formatting: use `column -t` for alignment
- Color: highlight event types by category (container=blue, gpu=green, error=red)

**Size guidance:** This is a comprehensive rewrite. The script will be ~250-350 lines. Focus on clean structure: parse args at top, build jq filter chain, execute query, format output.
  </action>
  <verify>
Test basic query: `ds01-events --limit 5` should show last 5 events in table format.
Test JSON output: `ds01-events --json --limit 2 | python3 -m json.tool` should show valid JSON.
Test filter: `ds01-events --type test --limit 5` should show only test events.
Test help: `ds01-events --help` should show usage.
Test summary: `ds01-events --summary` should show counts.
Check jq usage: `grep -c "jq" /opt/ds01-infra/scripts/monitoring/ds01-events` should be >= 5 (jq used throughout, not grep).
  </verify>
  <done>
- ds01-events rewritten with jq-based filtering
- Supports: --user, --type, --since, --until, --container, --limit, --all, --json, --follow, --summary
- 4-tier help: --help, --info, --concepts, --guided
- Default: last 50 events in human-readable table
- Handles both old (ts/event) and new (timestamp/event_type) schema
- Feels like a first-class admin tool, not a log file wrapper
  </done>
</task>

</tasks>

<verification>
1. event-logger.py uses shared library: `grep "from ds01_events import" /opt/ds01-infra/scripts/docker/event-logger.py`
2. ds01-events uses jq: `grep -c "jq" /opt/ds01-infra/scripts/monitoring/ds01-events` >= 5
3. ds01-events help: `/opt/ds01-infra/scripts/monitoring/ds01-events --help`
4. ds01-events summary: `/opt/ds01-infra/scripts/monitoring/ds01-events --summary` (may show 0 events if log is empty — that's fine)
5. ds01-events JSON: `/opt/ds01-infra/scripts/monitoring/ds01-events --json --limit 1 2>/dev/null | python3 -c "import sys,json; [json.loads(l) for l in sys.stdin]" || echo "No events yet"`
</verification>

<success_criteria>
- event-logger.py delegates to shared library, uses new schema
- ds01-events is a comprehensive jq-based query tool with all filters from CONTEXT.md
- Both tools handle empty event files gracefully
- Both tools handle schema migration (old ts/event vs new timestamp/event_type)
- 4-tier help system implemented
- Admin can query events by user, type, time range, container
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-observability/01-05-SUMMARY.md`
</output>
