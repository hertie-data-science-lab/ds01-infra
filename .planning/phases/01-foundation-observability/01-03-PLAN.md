---
phase: 01-foundation-observability
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - monitoring/alertmanager/alertmanager.yml
  - monitoring/docker-compose.yaml
  - monitoring/prometheus/rules/ds01_alerts.yml
autonomous: false
user_setup:
  - service: SMTP
    why: "Email alerting for monitoring stack health"
    env_vars:
      - name: SMTP_AUTH_PASSWORD
        source: "IT department — SMTP credentials for ds01-alerts@hertie-school.org"
    dashboard_config: []
  - service: Microsoft Teams
    why: "Teams webhook for real-time alert notifications"
    env_vars:
      - name: TEAMS_WEBHOOK_URL
        source: "Teams channel -> Manage channel -> Connectors -> Incoming Webhook -> Create -> Copy URL (use Workflows-based webhook if available)"
    dashboard_config: []

must_haves:
  truths:
    - "Alertmanager has email receiver configured with SMTP credentials (or placeholder ready for credentials)"
    - "Alertmanager has Teams webhook receiver configured (or placeholder ready for webhook URL)"
    - "Alert rules detect DCGM exporter down within 5 minutes"
    - "Alert rules detect Prometheus scrape failures within 3 minutes"
    - "Critical alerts use shorter notification intervals than warnings"
    - "Alertmanager upgraded to v0.28.1+ for native Teams webhook support"
  artifacts:
    - path: "monitoring/alertmanager/alertmanager.yml"
      provides: "Dual-channel alerting (email + Teams)"
      contains: "webhook_configs"
      min_lines: 40
    - path: "monitoring/prometheus/rules/ds01_alerts.yml"
      provides: "Alert rules for monitoring health"
      contains: "DCGMExporterDown"
    - path: "monitoring/docker-compose.yaml"
      provides: "Alertmanager v0.28.1+ image"
      contains: "alertmanager:v0.28"
  key_links:
    - from: "monitoring/prometheus/rules/ds01_alerts.yml"
      to: "monitoring/alertmanager/alertmanager.yml"
      via: "Prometheus evaluates rules, fires to Alertmanager"
      pattern: "severity.*critical"
    - from: "monitoring/alertmanager/alertmanager.yml"
      to: "SMTP server"
      via: "smtp_smarthost"
      pattern: "smtp_smarthost"
---

<objective>
Configure Alertmanager for dual-channel alerting (email + Teams webhook) and ensure alert rules cover monitoring stack health.

Purpose: Alerting is one of Phase 1's success criteria — "Alertmanager email configuration functional (test notification delivered)". This plan upgrades Alertmanager to v0.28.1+ (native Teams support), configures dual-channel routing, and ensures monitoring health alerts (DCGM down, scrape failures) are properly defined. SMTP credentials and Teams webhook URL require human input.

Output: Updated `alertmanager.yml` with dual-channel config, upgraded Alertmanager image in `docker-compose.yaml`, enhanced alert rules.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-observability/01-CONTEXT.md
@.planning/phases/01-foundation-observability/01-RESEARCH.md
@monitoring/alertmanager/alertmanager.yml
@monitoring/docker-compose.yaml
@monitoring/prometheus/rules/ds01_alerts.yml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Upgrade Alertmanager and configure dual-channel alerting</name>
  <files>monitoring/alertmanager/alertmanager.yml, monitoring/docker-compose.yaml</files>
  <action>
**Part A: Upgrade Alertmanager image in docker-compose.yaml**

Change the Alertmanager image version from `prom/alertmanager:v0.26.0` to `prom/alertmanager:v0.28.1` in `monitoring/docker-compose.yaml`. This version supports native Teams webhooks with adaptive cards. Change ONLY the image version — all other config (ports, volumes, labels, etc.) stays the same.

**Part B: Update alertmanager.yml for dual-channel alerting**

Rewrite `monitoring/alertmanager/alertmanager.yml` following the decisions in CONTEXT.md:

1. **Global settings** — keep existing SMTP config (hertie-school.org), keep smtp_auth_password commented out with instructions to set via .env or alertmanager-local.yml

2. **Route tree:**
   - Default receiver: `ds01-admin` (email + Teams)
   - group_by: `['alertname']` (simplify from current `['alertname', 'severity']`)
   - group_wait: `5m` (batch related alerts — CONTEXT.md says "aggressive grouping, 5 min wait")
   - group_interval: `5m`
   - repeat_interval: `4h`
   - Child route for critical: receiver `ds01-critical`, group_wait `30s`, repeat_interval `1h`

3. **Receivers:**
   - `ds01-admin`: email to `datasciencelab@hertie-school.org` + Teams webhook
   - `ds01-critical`: email with `[CRITICAL]` subject prefix + Teams webhook
   - Teams webhook URL: use placeholder `${TEAMS_WEBHOOK_URL}` with comment explaining how to set it

4. **Inhibit rules** — keep existing rules (critical suppresses warning for same alertname, DS01ExporterDown suppresses DS01.* alerts)

5. **Teams webhook config:**
   ```yaml
   webhook_configs:
     - url: '${TEAMS_WEBHOOK_URL}'
       send_resolved: true
   ```
   Add comment: "# Set TEAMS_WEBHOOK_URL in .env or use alertmanager-local.yml"

6. Keep the templates reference: `- '/etc/alertmanager/templates/*.tmpl'`

**Important:** The SMTP password and Teams webhook URL are placeholders. The checkpoint task will prompt the user to provide them.
  </action>
  <verify>
Check Alertmanager version: `grep "alertmanager:v0.28" /opt/ds01-infra/monitoring/docker-compose.yaml`
Check webhook config: `grep "webhook_configs" /opt/ds01-infra/monitoring/alertmanager/alertmanager.yml`
Check dual receivers: `grep -c "email_configs\|webhook_configs" /opt/ds01-infra/monitoring/alertmanager/alertmanager.yml` should be >= 4 (2 receivers x 2 channels).
YAML validity: `python3 -c "import yaml; yaml.safe_load(open('/opt/ds01-infra/monitoring/alertmanager/alertmanager.yml'))"`
  </verify>
  <done>
- Alertmanager image upgraded to v0.28.1 in docker-compose.yaml
- alertmanager.yml has dual-channel receivers (email + Teams webhook)
- Aggressive grouping configured (5m group_wait, group_by alertname)
- Critical alerts have shorter intervals (30s wait, 1h repeat)
- Placeholders for SMTP password and Teams webhook URL with clear instructions
  </done>
</task>

<task type="auto">
  <name>Task 2: Enhance monitoring health alert rules</name>
  <files>monitoring/prometheus/rules/ds01_alerts.yml</files>
  <action>
Review and enhance `monitoring/prometheus/rules/ds01_alerts.yml` to ensure monitoring stack health is fully covered.

**Existing rules already cover (DO NOT duplicate or remove):**
- DS01ExporterDown (critical, 2m)
- DS01NodeExporterDown (warning, 2m)
- DS01DCGMExporterDown (warning, 2m)
- GPU alerts (waste, temperature, memory)
- Container alerts (CPU, memory)
- System alerts (disk, load, memory)
- User alerts (GPU waste, MIG limits)
- Capacity alerts

**Changes needed:**
1. **Upgrade DCGMExporterDown severity** from `warning` to `critical` — DCGM stability is a Phase 1 success criterion, and GPU metrics are critical for the system. Change the existing rule's severity label.

2. **Add Grafana health alert** (currently missing):
   ```yaml
   - alert: DS01GrafanaDown
     expr: up{job="grafana"} == 0
     for: 5m
     labels:
       severity: warning
     annotations:
       summary: "Grafana is down"
       description: "Dashboard UI is not responding"
   ```
   Note: This requires Grafana to be scraped by Prometheus. Check if Grafana is in the Prometheus scrape config. If not, add a comment noting this alert will only fire once Grafana scrape is configured.

3. **Add Alertmanager health alert** (currently missing):
   ```yaml
   - alert: DS01AlertmanagerDown
     expr: up{job="alertmanager"} == 0
     for: 3m
     labels:
       severity: critical
     annotations:
       summary: "Alertmanager is down"
       description: "Alert routing and notifications are not functioning"
   ```

4. **Add GPU XID error alert** if not already present — check existing rules. From CONTEXT.md: GPU issues (XID errors, temp warnings) should trigger alerts.
   If missing, add:
   ```yaml
   - alert: DS01GPUXIDError
     expr: DCGM_FI_DEV_XID_ERRORS > 0
     for: 1m
     labels:
       severity: critical
     annotations:
       summary: "GPU XID error detected"
       description: "GPU {{ $labels.gpu }} reported XID error {{ $value }}"
   ```

Place new alerts in the `ds01_system_alerts` group since they're monitoring infrastructure health.

**Do not remove or modify** any existing alert rules beyond the DCGMExporterDown severity change.
  </action>
  <verify>
YAML validity: `python3 -c "import yaml; yaml.safe_load(open('/opt/ds01-infra/monitoring/prometheus/rules/ds01_alerts.yml'))"`
Check new alerts: `grep -c "DS01GrafanaDown\|DS01AlertmanagerDown\|DS01GPUXIDError" /opt/ds01-infra/monitoring/prometheus/rules/ds01_alerts.yml` should be >= 2 (at least Grafana + Alertmanager).
Check DCGM severity: `grep -A2 "DS01DCGMExporterDown" /opt/ds01-infra/monitoring/prometheus/rules/ds01_alerts.yml | grep "critical"`
  </verify>
  <done>
- DCGMExporterDown upgraded to critical severity
- GrafanaDown alert added (warning, 5m)
- AlertmanagerDown alert added (critical, 3m)
- GPU XID error alert present (added if missing)
- All existing alert rules preserved
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Alertmanager dual-channel config (email + Teams) with upgraded image and enhanced alert rules. SMTP password and Teams webhook URL are placeholders that need real values.</what-built>
  <how-to-verify>
1. Check if you have SMTP credentials for ds01-alerts@hertie-school.org. If yes, add the password to `monitoring/.env` as `SMTP_AUTH_PASSWORD=your_password` or update alertmanager.yml directly.
2. If you have a Teams webhook URL, add it to `monitoring/.env` as `TEAMS_WEBHOOK_URL=your_url` or update alertmanager.yml directly.
3. If credentials are not yet available, confirm that placeholder config is acceptable for now — the monitoring stack will function without email/Teams (alerts will still fire in Prometheus, just not route to external channels).
4. Review alert severity changes: DCGM down is now critical (was warning). Acceptable?
  </how-to-verify>
  <resume-signal>Type "approved" (with or without credentials), or provide SMTP password / Teams webhook URL to configure now, or describe issues.</resume-signal>
</task>

</tasks>

<verification>
1. Alertmanager image: `grep "alertmanager:v0.28" /opt/ds01-infra/monitoring/docker-compose.yaml`
2. Dual-channel: `grep -c "webhook_configs" /opt/ds01-infra/monitoring/alertmanager/alertmanager.yml` >= 2
3. Alert rules valid: `python3 -c "import yaml; yaml.safe_load(open('/opt/ds01-infra/monitoring/prometheus/rules/ds01_alerts.yml'))"`
4. DCGM critical: `grep -A3 "DS01DCGMExporterDown" /opt/ds01-infra/monitoring/prometheus/rules/ds01_alerts.yml | grep critical`
</verification>

<success_criteria>
- Alertmanager upgraded to v0.28.1 with native Teams support
- Dual-channel receivers configured (email + Teams webhook)
- Aggressive grouping: 5min batch, critical gets 30s fast-path
- Monitoring health alerts comprehensive: DCGM, Grafana, Alertmanager, Prometheus, GPU XID
- Placeholder credentials documented with clear setup instructions
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-observability/01-03-SUMMARY.md`
</output>
