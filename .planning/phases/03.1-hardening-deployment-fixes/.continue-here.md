---
phase: 03.1-hardening-deployment-fixes
task: gpu-notice-library
total_tasks: 4 plans + gpu-notice-fix
status: ready-to-test
last_updated: 2026-02-03T20:00:00Z
---

<current_state>
GPU notice library FIXED. Root cause: file permissions were 700 (only owner readable).
Library compiled with restrictive umask, other users couldn't load it via LD_PRELOAD.
Fixed by chmod 755. Ready for user testing.
</current_state>

<completed_work>

## This session (2026-02-03)

1. **check-limits GPU count bug: FIXED**
   - Root cause: used MIG-equivalents (4) instead of physical GPU count (1)
   - Fix: Added `user-gpu-count` command to gpu-state-reader.py
   - Updated check-limits to use new command
   - Commit in feature/awareness-layer

2. **container-exists-after-retire bug: FIXED**
   - Root cause: mlc-patched.py didn't check docker rm exit code
   - Fix: Added exit code capture and error propagation
   - Commit: 052fbb3

3. **GPU notice - attempted fixes (NOT WORKING)**:
   - Made notice always-on (removed DS01_GPU_NOTICE=1 opt-in)
   - Tried cudaMalloc hook → Failed (never called when CUDA_VISIBLE_DEVICES="")
   - Tried cuInit hook with failure check → Still not firing
   - Library loads (LD_PRELOAD set), symbol exported, but hook never executes

## Previous sessions (2026-02-01)
- Phase 3.1 plans 01-04 complete
- GPU cache removed, dashboard bug fixed
- GPU notice aesthetics improved (yellow box)
</completed_work>

<remaining_work>

## GPU notice: DEFERRED (low priority todo)

LD_PRELOAD hook not intercepting PyTorch CUDA init. Added to todos for future investigation.
See: `.planning/todos/pending/2026-02-01-gpu-host-compute-error-message.md`

Core GPU blocking works — this is just UX polish.

## Next steps

1. Run `sudo deploy` to deploy all fixes
2. Verify fixes live (check-limits, container retire, permissions)
3. Squash-merge feature/awareness-layer to main
</remaining_work>

<decisions_made>

- **cudaMalloc hook doesn't work** — PyTorch fails at CUDA init before malloc is called
- **cuInit hook approach** — Hook cuInit, show notice only on failure (result != 0)
- **Always-on notice** — Removed opt-in (DS01_GPU_NOTICE=1) requirement
- **Three-layer GPU access** — Layer 1: CUDA_VISIBLE_DEVICES, Layer 2: Docker --gpus, Layer 3: grant files
</decisions_made>

<blockers>

**RESOLVED: GPU notice library file permissions**
- Root cause: Library had 700 permissions (only owner could read)
- strace revealed: `ERROR: ld.so: object '...libds01_gpu_notice.so' cannot be preloaded`
- Fix: chmod 755 /opt/ds01-infra/lib/libds01_gpu_notice.so
- Status: Fixed, awaiting user test
</blockers>

<context>
The GPU awareness layer WORKS for blocking:
- CUDA_VISIBLE_DEVICES="" blocks host GPU compute ✓
- nvidia-smi works for all docker users ✓
- Container GPU access works ✓
- check-limits shows correct GPU count ✓
- container retire properly removes containers ✓

The GPU NOTICE (friendly error message) doesn't work:
- Goal: Show helpful box telling users to deploy containers
- Current: Users just see Python RuntimeError
- Status: Multiple hook approaches failed, need deeper investigation

PyTorch CUDA initialization likely uses:
- CUDA Runtime API (cudart): cudaGetDeviceCount, cudaSetDevice, etc.
- Not CUDA Driver API: cuInit, cuDeviceGet, etc.

The LD_PRELOAD library hooks Driver API functions, but PyTorch may use Runtime API
which loads via dlopen and may not be interceptable via simple LD_PRELOAD.
</context>

<next_action>
1. Run `sudo deploy` to deploy all committed fixes
2. Test as h.baker: check-limits, container retire, GPU allocation
3. Squash-merge feature/awareness-layer to main
</next_action>

<files_modified_this_session>
- lib/ds01_gpu_notice.c (multiple revisions)
- lib/libds01_gpu_notice.so (recompiled multiple times)
- config/deploy/profile.d/ds01-gpu-awareness.sh (always-on, updated comments)
- scripts/docker/gpu-state-reader.py (added user-gpu-count method)
- scripts/user/helpers/check-limits (use user-gpu-count)
- scripts/docker/mlc-patched.py (exit code check for docker rm)
- .planning/debug/resolved/check-limits-gpu-count.md
- .planning/debug/resolved/container-already-exists-after-retire.md
</files_modified_this_session>
