# Phase 3.1: Access Control Completion & Hardening - Research

**Researched:** 2026-02-01
**Domain:** Linux deployment automation, GPU access control, permissions management
**Confidence:** HIGH

## Summary

Phase 3.1 requires completing Phase 3 deployment with design alignment, fixing systemic permissions bugs, and hardening the GPU allocator chain. Research focused on five key areas: (1) Linux deployment permission manifests and deterministic enforcement, (2) self-updating script patterns and idempotency, (3) exception handling strategies for infrastructure code, (4) HPC/SLURM environment variable patterns for GPU isolation, and (5) profile.d execution mechanics and security implications.

**Key findings:**
- Standard Linux deployment uses 755 for executables, 644 for config files, with deterministic enforcement in deployment scripts (not umask-dependent)
- Idempotency requires "check before create" patterns; symlinks preferred over copies for deployment to /usr/local/bin
- Fail-open is appropriate for infrastructure code where availability > strict enforcement (OWASP 2025 guidance: fail-closed for security, fail-open for resilience)
- SLURM/HPC universally use CUDA_VISIBLE_DEVICES for GPU isolation; device permissions at defaults (0666) is the standard
- Profile.d scripts are sourced (not executed), require 644 permissions, and LD_PRELOAD must never block login

**Primary recommendation:** Follow HPC patterns (CUDA_VISIBLE_DEVICES + video group exemptions), implement deterministic permission manifest in deploy.sh, use fail-open for GPU allocator errors, remove udev rules entirely (align with 02.1-DESIGN.md).

## Standard Stack

The established tools and libraries for Linux deployment automation and GPU management:

### Core Components
| Component | Version/Pattern | Purpose | Why Standard |
|-----------|-----------------|---------|--------------|
| bash deployment scripts | 4.x+ (set -euo pipefail) | Idempotent system deployment | Universal Linux standard, POSIX-compliant |
| chmod deterministic perms | 755/644/711 | Permission manifest enforcement | Filesystem ACL standard, platform-independent |
| symlink deployment | ln -sf absolute_path | /usr/local/bin command deployment | Zero-downtime updates, atomic switches |
| CUDA_VISIBLE_DEVICES | CUDA Runtime API | GPU isolation (host-level) | HPC standard (SLURM, PBS, LSF, Kubernetes) |
| video group | Standard Linux group | Device access control (/dev/nvidia*) | NVIDIA driver convention (NVreg_DeviceFileGID) |
| at/atd scheduler | POSIX at(1) | Temporary grant auto-revocation | Universal POSIX scheduler, simpler than systemd timers |

### Supporting Tools
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| PyYAML | 5.x-6.x | Config parsing (resource-limits.yaml) | Already in use, standard Python YAML library |
| python3 (system) | 3.8+ | Infrastructure scripting | Avoid pip dependencies for core infra |
| nvidia-smi | NVIDIA driver bundled | GPU state query | Standard GPU management CLI |
| Docker labels | Docker Engine API | Container ownership/state tracking | Universal container metadata mechanism |
| fcntl file locking | Python stdlib | Race-safe GPU allocation | POSIX file locking, kernel-level atomicity |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| bash + chmod | Ansible/Puppet/Chef | Config mgmt tools overkill for single-server; adds deps |
| symlinks | install command (copy) | Copies lose atomic update capability, harder rollback |
| CUDA_VISIBLE_DEVICES | udev device permissions | Device perms break nvidia-smi, conflict with driver |
| at scheduler | systemd timers | Timers more complex; at is POSIX standard, simpler |
| fail-open | fail-closed (strict) | Fail-closed blocks users on infra bugs; fail-open maintains availability |

**Installation:**
```bash
# Core dependencies (already present on DS01)
apt-get install at cron python3 python3-yaml

# No pip dependencies for core infrastructure
# System python3 only (avoid virtualenv fragility)
```

## Architecture Patterns

### Recommended Deployment Structure
```
/opt/ds01-infra/
├── scripts/                    # Source of truth (755 for .sh/.py)
│   ├── user/                   # User-facing commands
│   ├── admin/                  # Admin commands
│   ├── docker/                 # GPU allocation, wrapper
│   ├── lib/                    # Shared libraries (755)
│   └── system/
│       └── deploy.sh           # Self-contained deployment script
├── config/                     # Configuration files (644)
│   ├── resource-limits.yaml    # Main config (644 - users must read)
│   ├── groups/*.members        # Group membership (644)
│   └── deploy/
│       └── profile.d/*.sh      # To be copied to /etc/profile.d/ (644)
├── lib/
│   └── libds01_gpu_notice.so   # LD_PRELOAD library (755)
└── /usr/local/bin/             # Symlinks to scripts/ (not copies)
    ├── container-deploy -> /opt/ds01-infra/scripts/user/orchestrators/container-deploy
    ├── gpu-allocator -> /opt/ds01-infra/scripts/docker/gpu_allocator_v2.py
    └── deploy -> /opt/ds01-infra/scripts/system/deploy.sh
```

### Pattern 1: Deterministic Permission Manifest (Idempotent Deployment)

**What:** Deployment script explicitly sets permissions on all deployed files, regardless of umask or git checkout state.

**When to use:** Always for production deployment scripts. Never rely on umask or git to preserve permissions.

**Example:**
```bash
# Source: Linux deployment best practices 2026
# https://www.ssdnodes.com/blog/linux-permissions-chmod-755-644-drwxrxrx-explained/

# BAD: Umask-dependent (non-deterministic)
cp scripts/user/atomic/* /usr/local/bin/

# GOOD: Deterministic permission enforcement
deploy_with_perms() {
    local src=$1 dest=$2 perms=$3

    # Check before create (idempotency)
    if [ -f "$dest" ] && [ "$src" -ot "$dest" ]; then
        return 0  # Already deployed, up-to-date
    fi

    ln -sf "$src" "$dest"
    chmod "$perms" "$src"  # Enforce on source (symlink preserves)
}

# Deterministic manifest
chmod 755 scripts/user/atomic/*
chmod 755 scripts/lib/*.sh
chmod 644 config/resource-limits.yaml
chmod 644 config/groups/*.members
chmod 755 lib/*.so

# Deploy with symlinks (atomic updates)
for cmd in scripts/user/atomic/*; do
    deploy_with_perms "$PWD/$cmd" "/usr/local/bin/$(basename $cmd)" 755
done
```

**Why deterministic:**
- Git checkouts reset permissions to umask defaults (typically 644 for files, 755 for dirs)
- umask varies by user (datasciencelab vs root may differ)
- Deployment must work identically regardless of who runs it or when

**Source:** [SSD Nodes: Linux File Permissions Guide](https://www.ssdnodes.com/blog/linux-permissions-chmod-755-644-drwxrxrx-explained/), [DigitalOcean: How to Set Permissions in Linux](https://www.digitalocean.com/community/tutorials/how-to-set-permissions-linux)

### Pattern 2: Self-Updating Deployment Script (Bootstrap Problem)

**What:** Deployment script that can update itself atomically without requiring two runs.

**Problem:** If deploy.sh is updated in git, the running copy executes old code. Naive symlink deployment requires running deploy.sh twice (first run updates symlink, second run executes new code).

**Solution:** Re-exec pattern with temp copy:
```bash
# Source: Idempotency patterns for infrastructure code 2026
# https://medium.com/@tiwari.sushil/idempotency-the-secret-to-seamless-devops-and-infrastructure-bf22e63e1be5

#!/bin/bash
# deploy.sh

INFRA_ROOT="/opt/ds01-infra"
SELF="$INFRA_ROOT/scripts/system/deploy.sh"

# Self-update check: if we're the deployed copy, re-exec from source
if [ "$0" = "/usr/local/bin/deploy" ] || [ "$0" = "deploy" ]; then
    # Running as deployed command, re-exec from source repo
    exec "$SELF" "$@"
fi

# Now we're running from source, continue with deployment
echo "Deploying from source: $SELF"

# Deploy commands (including self)
ln -sf "$SELF" /usr/local/bin/deploy
# ... rest of deployment
```

**Idempotency:** Re-exec is idempotent (running twice has same effect as once). Check `if [ "$0" = ... ]` prevents infinite loop.

**Alternative:** Temp copy pattern:
```bash
# If source updated, copy self to /tmp and re-exec
if [ "$0" != "/tmp/deploy.sh.$$" ]; then
    cp "$SELF" "/tmp/deploy.sh.$$"
    exec "/tmp/deploy.sh.$$" "$@"
fi
```

**Source:** [Idempotency: The Secret to Seamless DevOps](https://medium.com/@tiwari.sushil/idempotency-the-secret-to-seamless-devops-and-infrastructure-bf22e63e1be5), [Infrastructure as Code: Principles, Patterns, and Practices](https://shahadarsh.com/2020/07/12/principles-patterns-and-practices-for-effective-infrastructure-as-code/)

### Pattern 3: Symlink vs Copy for /usr/local/bin Deployment

**What:** Use symlinks (not copies) for deploying commands to /usr/local/bin, with absolute paths.

**Why symlinks:**
1. **Atomic updates:** `ln -sf` is atomic; new symlink created, then renamed over old
2. **Zero-downtime deployment:** Running commands continue with old code; new invocations get new code
3. **Easy rollback:** Change symlink target back to previous version
4. **SCRIPT_DIR resolution works:** `$(dirname "$0")` resolves to source directory, not /usr/local/bin

**Why absolute paths:**
```bash
# BAD: Relative symlink (breaks from different cwd)
ln -sf ../opt/ds01-infra/scripts/user/atomic/container-create /usr/local/bin/

# GOOD: Absolute symlink (works from any cwd)
ln -sf /opt/ds01-infra/scripts/user/atomic/container-create /usr/local/bin/
```

**When to copy instead:**
- Binary executables compiled for specific path
- Files that must persist if source directory deleted
- Environments where symlinks unsupported (rare on modern Linux)

**DS01 specific:** mlc-create MUST use symlink so SCRIPT_DIR resolves to /opt/ds01-infra/scripts/docker/, allowing imports of gpu_allocator_v2.py, gpu-state-reader.py, etc.

**Source:** [Race-condition-free deployment with symlink replacement](https://news.ycombinator.com/item?id=4682035), [Beginner's Guide to /usr/local/bin](https://dev.to/hbalenda/beginner-s-guide-to-usr-local-bin-4fe2), [Workflow: Using Symbolic Links in Linux](https://mangohost.net/blog/workflow-using-symbolic-links-in-linux/)

### Pattern 4: Fail-Open Exception Handling for Infrastructure Code

**What:** Infrastructure code (GPU allocator, checkers) should fail-open (log errors, allow operation) rather than fail-closed (block operation on error).

**Rationale:**
- **Availability > strict enforcement** for non-security operations
- Infrastructure bugs should not block users from working
- Container layer provides actual security boundary (Docker device mapping)
- Host layer (CUDA_VISIBLE_DEVICES) is UX deterrent, not enforcement

**Example: GPU Allocator Fail-Open**
```python
# Source: OWASP 2025 A10 - Mishandling of Exceptional Conditions
# https://authzed.com/blog/fail-open
# https://owasp.org/Top10/2025/A10_2025-Mishandling_of_Exceptional_Conditions/

def allocate_gpu(username, container):
    """Allocate GPU with fail-open error handling."""
    try:
        # Check user limits
        limits = get_resource_limits(username)
    except Exception as e:
        # Fail-open: Log error, use default limits
        log_event("gpu_allocator_error", {
            "error": str(e),
            "fallback": "default_limits"
        })
        limits = DEFAULT_LIMITS  # Don't block user

    try:
        # Allocate GPU
        gpu = _find_available_gpu(limits)
        return gpu
    except Exception as e:
        # Fail-open: Log error, allow container creation without GPU
        log_event("gpu_allocation_failed", {
            "error": str(e),
            "action": "proceeding_without_gpu"
        })
        return None  # Container creates without --gpus flag
```

**Fail-Closed vs Fail-Open Decision Matrix:**

| Operation | Strategy | Rationale |
|-----------|----------|-----------|
| GPU allocator errors | Fail-open | Availability > strict limits; container layer enforces access |
| Config file read errors | Fail-open (use defaults) | System must remain operational |
| Docker label read errors | Fail-open (skip ownership check) | Legacy/external containers must work |
| Authentication/Authorization | Fail-closed | Security-critical; deny by default |
| File permission checks | Fail-closed | Security boundary |
| LD_PRELOAD library load | Fail-open (silent) | Must never prevent login |

**OWASP 2025 Guidance:**
- A10: Mishandling of Exceptional Conditions includes CWE-636 (Not Failing Securely / Failing Open)
- **For security checks:** Fail-closed (deny by default on error)
- **For resilience/availability:** Fail-open with logging (infrastructure code)
- **Never:** Silent failures that hide bugs (always log exceptions)

**Source:** [AuthZed: Understanding Fail Open and Fail Closed](https://authzed.com/blog/fail-open), [OWASP Top 10 2025: A10 Mishandling of Exceptional Conditions](https://owasp.org/Top10/2025/A10_2025-Mishandling_of_Exceptional_Conditions/)

### Pattern 5: Profile.d Script Execution Mechanics

**What:** Scripts in /etc/profile.d/ are **sourced** (not executed) by login shells, requiring 644 permissions (not 755).

**Why 644 works:**
```bash
# /etc/profile sources all .sh files in profile.d/
for i in /etc/profile.d/*.sh; do
    if [ -r "$i" ]; then
        . "$i"  # Source (not execute), so read permission sufficient
    fi
done
```

**Permission requirements:**
- **644 (rw-r--r--):** Standard for profile.d scripts (read by all, sourced by shell)
- **755 (rwxr-xr-x):** NOT required (scripts sourced, not executed)
- **600 (rw-------):** BREAKS login (shell cannot read to source)

**DS01 current issue:**
- `ds01-home-enforce.sh`: 600 (broken - shell can't read)
- `ds01-warnings.sh`: 600 (broken)
- `ds01-gpu-awareness.sh`: 644 (correct)
- `ds01-motd.sh`: 644 (correct)

**Fix:**
```bash
chmod 644 /etc/profile.d/ds01-*.sh
```

**Interactive shell check:**
```bash
# Profile.d scripts should skip non-interactive shells
[[ $- == *i* ]] || return 0

# Why: cron jobs, systemd services, scp, rsync all trigger profile loading
# but shouldn't run interactive setup (MOTD, warnings, etc.)
```

**LD_PRELOAD in profile.d:**
```bash
# MUST fail silently if .so doesn't exist or can't load
_ds01_notice="/opt/ds01-infra/lib/libds01_gpu_notice.so"
if [ -f "$_ds01_notice" ]; then
    export LD_PRELOAD="${_ds01_notice}${LD_PRELOAD:+:$LD_PRELOAD}"
fi
unset _ds01_notice

# Why fail-open:
# - LD_PRELOAD errors must NEVER block login
# - Missing .so is non-fatal (users just don't get notice)
# - setuid protection: loader ignores LD_PRELOAD for suid binaries anyway
```

**LD_PRELOAD security:**
- Loader automatically disables for setuid/setgid executables (secure-execution mode)
- Library file should be 755 (world-readable, executable) for dynamic linker
- Ownership should prevent user modification (root:root or user:docker)

**Source:** [Linux permissions for shell scripts](https://ss64.com/bash/syntax-permissions.html), [LD_PRELOAD Security](https://medium.com/r3d-buck3t/overwriting-preload-libraries-to-gain-root-linux-privesc-77c87b5f3bf8), [UNIX Setuid & LD_PRELOAD](https://www.cs.uaf.edu/2017/fall/cs493/lecture/11_01_setuid.html)

### Anti-Patterns to Avoid

**1. Umask-dependent deployment**
- **Bad:** Rely on git checkout or cp preserving permissions
- **Why:** umask varies, git resets to defaults, non-deterministic
- **Fix:** Explicit chmod in deploy.sh

**2. Copy instead of symlink for scripts**
- **Bad:** `cp script.sh /usr/local/bin/`
- **Why:** SCRIPT_DIR resolves to /usr/local/bin/, not source repo; updates require re-copy
- **Fix:** `ln -sf /opt/ds01-infra/scripts/.../script.sh /usr/local/bin/`

**3. Profile.d scripts with 755 or 600 permissions**
- **Bad:** chmod 755 or 600 /etc/profile.d/script.sh
- **Why:** 600 breaks sourcing (shell can't read), 755 is unnecessarily permissive
- **Fix:** chmod 644 (read for all, sourced not executed)

**4. NVIDIA udev rules for device permissions**
- **Bad:** Set /dev/nvidia* to 0660 via udev rules
- **Why:** Driver overrides with NVreg_DeviceFileMode, breaks nvidia-smi, race conditions
- **Fix:** Leave at defaults (0666), use CUDA_VISIBLE_DEVICES for access control

**5. Fail-closed for infrastructure errors**
- **Bad:** Raise exception and block operation on config read error
- **Why:** Infrastructure bugs block all users from working
- **Fix:** Log error, use safe defaults, continue operation (fail-open)

**6. Broad exception handling without logging**
- **Bad:** `except Exception: pass`
- **Why:** Silences bugs, no observability, violates OWASP 2025 A10
- **Fix:** `except Exception as e: log_event("error", {"error": str(e)}); use_fallback()`

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| File locking for GPU allocation | Custom lock files with pid tracking | `fcntl.flock()` (Python stdlib) | Kernel-level atomicity, handles crashes, POSIX standard |
| Temporary grant scheduling | Custom cron job generator | `at` command (POSIX) | Standard scheduler, auto-cleanup, simpler than systemd timers |
| GPU isolation on host | Custom device permission management | CUDA_VISIBLE_DEVICES environment variable | HPC standard (SLURM, PBS), works with all CUDA apps |
| Config file merging | Custom YAML merger | PyYAML + dict merge | Group .members files pattern already works in get_resource_limits.py |
| Atomic symlink updates | Custom rename logic | `ln -sf` | Atomic by design, race-free |
| Idempotency framework | Custom state tracking | "Check before create" conditionals | POSIX test command, simple and reliable |

**Key insight:** For infrastructure code, prefer POSIX-standard tools and kernel mechanisms over custom abstractions. Deployment scripts should be readable bash, not framework magic.

## Common Pitfalls

### Pitfall 1: Permissions Inherited from umask/git

**What goes wrong:** Files deployed with 600 or 700 permissions because datasciencelab user has restrictive umask, or git checkout reset permissions.

**Why it happens:**
- Git stores execute bit only (not full permissions)
- umask applies to new files (cp, git checkout)
- Different users have different umask defaults

**How to avoid:**
- Deterministic permission manifest in deploy.sh
- Explicit chmod for every file type: scripts (755), config (644), state dirs (711 or 755)
- Never rely on git or umask to preserve permissions

**Warning signs:**
- Non-admin users report "Permission denied" for ds01 commands
- Config files (resource-limits.yaml) not readable by profile.d scripts
- State directories (/var/lib/ds01/bare-metal-grants/) inaccessible to users

**Fix applied 2026-02-01:**
```bash
# In deploy.sh: Deterministic permission pass
chmod 755 scripts/user/atomic/*
chmod 755 scripts/docker/*.py
chmod 644 config/resource-limits.yaml
chmod 644 config/groups/*.members
chmod 711 /var/lib/ds01/bare-metal-grants/  # Traverse without listing
```

### Pitfall 2: NVIDIA Udev Rules Fighting Driver Defaults

**What goes wrong:** udev rules set /dev/nvidia* to 0660, but NVIDIA driver recreates devices with 0666 (or configured NVreg_DeviceFileMode), causing permission flip-flop and breaking nvidia-smi.

**Why it happens:**
- NVIDIA driver has kernel module parameters (NVreg_DeviceFileMode, NVreg_DeviceFileGID)
- Driver recreates device nodes when X server starts or nvidia-persistenced runs
- udev rules fire on device creation, but driver wins the race
- Some HPC environments use nvidia-persistenced for faster CUDA init

**How to avoid:**
- Don't manipulate NVIDIA device permissions at all
- Use CUDA_VISIBLE_DEVICES for access control (HPC standard)
- Leave /dev/nvidia* at driver defaults (typically 0666)
- If device group control needed, use NVreg_DeviceFileGID kernel module param, not udev

**Warning signs:**
- nvidia-smi works for root but fails for users in video group
- Device permissions reset after reboot or X server start
- udevadm trigger has no lasting effect

**02.1-DESIGN.md decision:** Reject device permission manipulation entirely. CUDA_VISIBLE_DEVICES + video group exemption is the access control mechanism.

**Source:** [NVIDIA Developer Forums: Setting group/mode on /dev/nvidia*](https://forums.developer.nvidia.com/t/setting-group-mode-on-dev-nvidia/49418), [ArchLinux Bug: wrong permission of /dev/nvidia*](https://bugs.archlinux.org/task/5682)

### Pitfall 3: Profile.d Scripts That Exit Instead of Return

**What goes wrong:** Profile.d script contains `exit 0`, which closes user's SSH session immediately after login.

**Why it happens:**
- Scripts in /etc/profile.d/ are **sourced** (not executed in subshell)
- `exit` in sourced script exits the parent shell (user's login shell)
- Common mistake when converting standalone scripts to profile.d

**How to avoid:**
```bash
# BAD: Exits user's shell
if [ some_condition ]; then
    exit 0
fi

# GOOD: Returns from sourced script
if [ some_condition ]; then
    return 0
fi

# Also check for interactive shell first
[[ $- == *i* ]] || return 0
```

**Warning signs:**
- User logs in and immediately gets disconnected
- SSH session closes with no error message
- Works fine when script executed directly, fails when in profile.d

**DS01 status:** All profile.d scripts correctly use `return`, not `exit`.

### Pitfall 4: LD_PRELOAD Library Blocking Login

**What goes wrong:** LD_PRELOAD points to missing or unreadable .so file, causing shell to fail during login.

**Why it happens:**
- Profile.d script sets `export LD_PRELOAD=/path/to/lib.so`
- If lib doesn't exist or has wrong permissions, dynamic linker may fail
- Some shells abort on LD_PRELOAD errors

**How to avoid:**
```bash
# Check file exists before adding to LD_PRELOAD
_ds01_notice="/opt/ds01-infra/lib/libds01_gpu_notice.so"
if [ -f "$_ds01_notice" ]; then
    export LD_PRELOAD="${_ds01_notice}${LD_PRELOAD:+:$LD_PRELOAD}"
fi
unset _ds01_notice
```

**Library permissions:**
- **755 (rwxr-xr-x):** Standard for shared libraries (world-readable, executable by linker)
- Ownership: root:root or datasciencelab:docker (prevent user modification)

**Warning signs:**
- User can't login via SSH
- Login succeeds for root but fails for regular users
- Error messages about "cannot open shared object file"

**DS01 current state:**
- libds01_gpu_notice.so is 755 datasciencelab:ds-admin (correct)
- Profile.d script checks file existence before export (correct)

**Security note:** LD_PRELOAD automatically disabled for setuid/setgid binaries, so malicious user can't inject code into privileged processes.

**Source:** [LD_PRELOAD Security and Privilege Escalation](https://www.hackingarticles.in/linux-privilege-escalation-using-ld_preload/)

### Pitfall 5: GPU Allocator Loading Config Without Group Merge

**What goes wrong:** gpu_allocator_v2.py loads resource-limits.yaml but doesn't merge groups/*.members files, causing all users to fall back to default limits even if they're in a group .members file.

**Why it happens:**
- get_resource_limits.py has correct merge logic (_load_group_members)
- gpu_allocator_v2.py has simplified _load_config() without .members support
- Code duplication led to drift

**How to avoid:**
- DRY: Share config loading logic via library module
- Or: Port _load_external_files() from get_resource_limits.py to gpu_allocator_v2.py

**Warning signs:**
- User reports "I'm in researchers.members but get student limits"
- GPU allocator log shows "using defaults" for known group members
- get-limits command shows correct group, but allocator doesn't honor it

**Fix required:** Port _load_group_members and _load_external_files logic from get_resource_limits.py (lines 66-127) into gpu_allocator_v2.py _load_config().

### Pitfall 6: GPU Availability Checker Only Querying MIG

**What goes wrong:** gpu-availability-checker.py get_available_gpus() only calls _get_all_mig_instances(), skipping full GPUs. Reports 0 GPUs available when MIG disabled, even though 4x A100 GPUs present.

**Why it happens:**
- _get_full_gpus_available() method exists but not called in default code path
- MIG-first design assumed MIG always enabled
- Recent MIG disable exposed the bug

**How to avoid:**
```python
def get_available_gpus(self):
    """Get available GPUs (MIG instances + full GPUs)."""
    available = {}

    # Get MIG instances
    mig_instances = self._get_all_mig_instances()
    available.update(mig_instances)

    # Get full GPUs (if allow_full_gpu enabled for any user)
    full_gpus = self._get_full_gpus_available()
    available.update(full_gpus)

    return available
```

**Warning signs:**
- container-create reports "No GPUs available" on system with multiple GPUs
- nvidia-smi shows GPUs, but allocator sees 0
- Works with MIG enabled, broken with MIG disabled

**Fix required:** Update gpu-availability-checker.py to call both _get_all_mig_instances() and _get_full_gpus_available().

## Code Examples

Verified patterns from official sources and current DS01 implementation:

### Idempotent Directory Creation
```bash
# Source: Idempotency best practices for infrastructure automation
# https://skundunotes.com/2019/04/19/idempotency-in-infrastructure-as-code/

# Create state directories with correct permissions (idempotent)
ensure_dir() {
    local dir=$1 perms=$2 owner=$3

    if [ ! -d "$dir" ]; then
        mkdir -p "$dir"
    fi

    chmod "$perms" "$dir"
    chown "$owner" "$dir"
}

ensure_dir /var/lib/ds01/bare-metal-grants 711 root:root
ensure_dir /var/lib/ds01/rate-limits 1777 root:root  # World-writable with sticky bit
ensure_dir /var/log/ds01 755 root:docker
```

### Profile.d Script Template (Sourced, Not Executed)
```bash
#!/bin/bash
# /etc/profile.d/ds01-example.sh
# Purpose: Example profile.d script with correct structure
# Permissions: 644 (sourced, not executed)

# Skip non-interactive shells (cron, systemd, scp, etc.)
[[ $- == *i* ]] || return 0

# Check preconditions before proceeding
if ! command -v some_command &>/dev/null; then
    return 0  # Not exit! (sourced script)
fi

# Do work (export env vars, set aliases, etc.)
export SOME_VAR="value"

# Clean up temporary variables
unset _temp_var
```

### Video Group Exemption Check (CUDA_VISIBLE_DEVICES)
```bash
# Source: 02.1-DESIGN.md - Three-layer GPU access control
# Current: config/deploy/profile.d/ds01-gpu-awareness.sh

#!/bin/bash
# Skip for non-interactive shells
[[ $- == *i* ]] || return 0

# Check bare-metal access grant file
_ds01_grant_file="/var/lib/ds01/bare-metal-grants/$(whoami).json"
if [ -f "$_ds01_grant_file" ]; then
    unset _ds01_grant_file
    return 0  # Exempt: temporary/permanent grant exists
fi
unset _ds01_grant_file

# Check permanently exempt users from config
_ds01_user="$(whoami)"
if grep -qx "  - ${_ds01_user}" /opt/ds01-infra/config/resource-limits.yaml 2>/dev/null; then
    unset _ds01_user
    return 0  # Exempt: in exempt_users list
fi
unset _ds01_user

# Not exempt: hide GPUs from host CUDA applications
export CUDA_VISIBLE_DEVICES=""

# LD_PRELOAD notice (fail silently if missing)
_ds01_notice="/opt/ds01-infra/lib/libds01_gpu_notice.so"
if [ -f "$_ds01_notice" ]; then
    export LD_PRELOAD="${_ds01_notice}${LD_PRELOAD:+:$LD_PRELOAD}"
fi
unset _ds01_notice
```

**Alternative: Video Group Check** (simpler, group-based instead of grant file):
```bash
# Check if user is in video group
if groups | grep -qw video; then
    return 0  # Exempt: video group member has bare-metal access
fi

export CUDA_VISIBLE_DEVICES=""
```

**Design decision:** DS01 uses grant file + config grep (more flexible), not video group check. Video group membership is granted to ALL docker users (for nvidia-smi access), so it can't be the exemption mechanism.

### Fail-Open GPU Allocator with Logging
```python
# Source: OWASP 2025 A10 guidance on exception handling
# Current: scripts/docker/gpu_allocator_v2.py (to be updated)

import logging
from typing import Optional

def allocate_gpu(username: str, container: str) -> Optional[str]:
    """
    Allocate GPU with fail-open error handling.

    Returns GPU UUID on success, None on failure (container proceeds without GPU).
    Logs all errors for observability.
    """
    try:
        limits = get_resource_limits(username)
    except FileNotFoundError as e:
        # Fail-open: Config file missing, use defaults
        logging.error(f"Config not found: {e}, using defaults")
        log_event("gpu_allocator_config_error", {"error": str(e), "fallback": "defaults"})
        limits = DEFAULT_LIMITS
    except PermissionError as e:
        # Fail-open: Can't read config (permissions issue), use defaults
        logging.error(f"Config permission denied: {e}, using defaults")
        log_event("gpu_allocator_permission_error", {"error": str(e), "fallback": "defaults"})
        limits = DEFAULT_LIMITS
    except Exception as e:
        # Fail-open: Unexpected error, log and use defaults
        logging.error(f"Unexpected config error: {e}, using defaults")
        log_event("gpu_allocator_unexpected_error", {"error": str(e), "fallback": "defaults"})
        limits = DEFAULT_LIMITS

    try:
        gpu = _find_available_gpu(limits)
        log_event("gpu_allocated", {"username": username, "container": container, "gpu": gpu})
        return gpu
    except NoGPUAvailable:
        # Expected condition: No GPUs free, log and return None
        log_event("gpu_allocation_queued", {"username": username, "reason": "no_gpu_available"})
        return None
    except Exception as e:
        # Fail-open: Unexpected allocator error, log and return None
        logging.error(f"GPU allocation failed: {e}, proceeding without GPU")
        log_event("gpu_allocation_error", {"error": str(e), "action": "no_gpu"})
        return None
```

**Exception granularity:**
- Narrow exceptions (FileNotFoundError, PermissionError) for known failure modes
- Broad exception (Exception) as catch-all with detailed logging
- Always log to both Python logging and DS01 event log (dual observability)

### Symlink Deployment with Idempotency Check
```bash
# Source: Deploy script best practices, symlink atomic updates
# Pattern: Check before create, absolute paths, atomic ln -sf

deploy_cmd() {
    local src="$1"       # Absolute path to source file
    local name="$2"      # Command name in /usr/local/bin
    local perms="$3"     # Permissions (755 for executables)

    local dest="/usr/local/bin/$name"

    # Idempotency: Check if source file exists
    if [ ! -f "$src" ]; then
        echo "WARNING: Source not found: $src"
        return 1
    fi

    # Set permissions on source (symlink preserves)
    chmod "$perms" "$src"

    # Atomic symlink update
    ln -sf "$src" "$dest"

    return 0
}

# Usage
deploy_cmd "/opt/ds01-infra/scripts/user/atomic/container-create" "container-create" 755
deploy_cmd "/opt/ds01-infra/scripts/docker/gpu_allocator_v2.py" "gpu-allocator" 755
```

**Why ln -sf is atomic:**
- Creates new symlink with temporary name
- Renames (atomic operation) over old symlink
- No window where symlink is missing or invalid

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Device permissions (0660) for GPU access control | CUDA_VISIBLE_DEVICES environment variable | HPC standard since ~2012 | nvidia-smi works for all users, monitoring not restricted |
| Copy scripts to /usr/local/bin | Symlink scripts to /usr/local/bin | Modern deployment (2020+) | Atomic updates, easy rollback, SCRIPT_DIR resolution |
| Umask-dependent permissions | Deterministic chmod in deploy script | Config management best practice | Reliable across environments, no umask variation |
| Fail-closed infrastructure | Fail-open with logging (availability > strict) | OWASP 2025 A10 guidance | Infrastructure bugs don't block users |
| cgroups v1 devices controller | cgroups v2 + Docker device mapping | Linux kernel 4.5+, mainstream 2020+ | NVIDIA Container Toolkit handles complexity |
| nvidia-smi wrapper as security | nvidia-smi wrapper as UX (error messages) | Phase 2.1 research | Clear separation: UX vs security layers |

**Deprecated/outdated:**
- **udev rules for NVIDIA device permissions:** Conflicts with driver, race conditions, breaks monitoring tools
- **Manual cgroup hierarchy management:** Docker + NVIDIA Container Toolkit abstract this away
- **Video group as primary enforcement:** Video group now for nvidia-smi access only; CUDA_VISIBLE_DEVICES is deterrent
- **Copying scripts to /usr/local/bin:** Symlinks standard for deployment automation

**Current as of 2026:**
- SLURM 23.x+ continues using CUDA_VISIBLE_DEVICES for GPU isolation
- NVIDIA Container Toolkit 1.16.2+ (CVE-2025-23266 fixed)
- Kubernetes GPU Operator uses device plugin + CUDA_VISIBLE_DEVICES pattern
- OWASP Top 10 2025 includes A10 (Mishandling of Exceptional Conditions)

## Open Questions

### 1. Video Group Restriction Strategy

**What we know:**
- Video group currently has ~25 users (all docker users)
- 02.1-DESIGN.md assumes video group is restricted to exempt users only
- Current deploy.sh adds ALL docker users to video group (for nvidia-smi access)
- Profile.d exemption logic checks grant files + config, not video group membership

**What's unclear:**
- Should video group be restricted to exempt users only (breaking nvidia-smi for others)?
- Or should video group remain universal, with exemption checked separately?
- How does bare-metal-access grant/revoke interact with video group?

**Recommendation:**
- **Keep video group universal** (all docker users) for nvidia-smi access
- **Don't use video group as exemption mechanism** (too many members, not auditable)
- **Use grant files + config grep for exemptions** (current approach is correct)
- **Update 02.1-DESIGN.md** to clarify: video group = nvidia-smi access, NOT bare-metal exemption

**Confidence:** MEDIUM (design ambiguity, needs user decision)

### 2. nvidia-* Wrapper Value Proposition

**What we know:**
- nvidia-wrapper.sh exists (Phase 3 plan 03-01)
- Provides helpful error messages directing users to containers
- Does NOT provide security (users can bypass by calling CUDA libraries directly)
- deploy.sh currently REMOVES nvidia-* wrappers (lines 287-293)
- 03-03 planned to DEPLOY nvidia-* wrappers

**What's unclear:**
- Is UX value (error messages) worth the deployment complexity?
- Do users actually invoke nvidia-smi on host often enough to justify wrapper?
- Does wrapper cause confusion (looks like security boundary but isn't)?

**Recommendation:**
- **Deploy nvidia-smi wrapper** for UX (helpful error messages)
- **Document clearly:** Wrapper is UX, not security; CUDA_VISIBLE_DEVICES is deterrent; container device mapping is enforcement
- **Keep simple:** Wrapper just checks video group and grant files, then execs real nvidia-smi or shows error

**Alternative:** Remove wrapper entirely, rely on CUDA_VISIBLE_DEVICES + GPU notice library for UX.

**Confidence:** LOW (UX preference, needs user input)

### 3. Deploy.sh Idempotency Level

**What we know:**
- Current deploy.sh is partially idempotent (symlinks, directory creation)
- Some operations not idempotent (at package install, group membership sync)
- Re-running deploy.sh is safe but may show warnings

**What's unclear:**
- Should deploy.sh be fully idempotent (running 100 times = same result as once, no warnings)?
- Or is "safe to re-run" sufficient (warnings OK if no-op)?
- Should deploy.sh output "N commands deployed, M already up-to-date"?

**Recommendation:**
- **Aim for silent idempotency** where easy (suppress "already exists" messages)
- **Accept warnings** for complex operations (at install, systemd reload)
- **Add summary output:** "Deployed X commands, Y already current"

**Confidence:** MEDIUM (implementation detail, style preference)

### 4. Python Dependency Management (System vs pip)

**What we know:**
- DS01 uses system python3 for infrastructure scripts
- PyYAML, docker required (currently installed via pip)
- deploy.sh checks `python3 -c "import docker"` and installs if missing
- System python pip may not be available (Debian/Ubuntu trend towards python3-pip package)

**What's unclear:**
- Should infrastructure deps be installed via apt (python3-yaml, python3-docker)?
- Or continue using pip3 install (current approach)?
- How to handle missing pip on fresh systems?

**Recommendation:**
- **Prefer apt packages** for core dependencies: `apt-get install python3-yaml python3-docker`
- **Fallback to pip** if apt package unavailable or too old
- **Check pip availability** before using: `command -v pip3 || apt-get install python3-pip`

**Confidence:** HIGH (standard practice for system-level Python)

## Sources

### Primary (HIGH confidence)
- [SLURM GRES GPU Scheduling](https://slurm.schedmd.com/gres.html) - Official SLURM documentation on CUDA_VISIBLE_DEVICES usage
- [Official Python Exception Handling Docs](https://docs.python.org/3/tutorial/errors.html) - Python 3.14 documentation
- [OWASP Top 10 2025: A10 Mishandling of Exceptional Conditions](https://owasp.org/Top10/2025/A10_2025-Mishandling_of_Exceptional_Conditions/) - Security guidance on fail-open/fail-closed
- [Linux File Permissions Guide (SSD Nodes)](https://www.ssdnodes.com/blog/linux-permissions-chmod-755-644-drwxrxrx-explained/) - Standard Linux permission patterns
- [NVIDIA System Management Interface Manual](https://docs.nvidia.com/deploy/nvidia-smi/index.html) - Official nvidia-smi documentation

### Secondary (MEDIUM confidence)
- [How SLURM Assigns GPUs (Medium)](https://medium.com/the-owl/how-slurm-assigns-gpus-to-your-jobs-and-what-cuda-visible-devices-means-40037ef2356c) - Practical SLURM GPU allocation explanation
- [Understanding Fail Open and Fail Closed (AuthZed)](https://authzed.com/blog/fail-open) - Infrastructure resilience patterns
- [Idempotency in DevOps (Medium)](https://medium.com/@tiwari.sushil/idempotency-the-secret-to-seamless-devops-and-infrastructure-bf22e63e1be5) - Idempotent deployment patterns
- [Race-condition-free deployment with symlink (HN)](https://news.ycombinator.com/item?id=4682035) - Atomic symlink deployment discussion
- [LD_PRELOAD Security (Medium)](https://medium.com/r3d-buck3t/overwriting-preload-libraries-to-gain-root-linux-privesc-77c87b5f3bf8) - LD_PRELOAD privilege escalation vectors

### Tertiary (LOW confidence - community forums, marked for validation)
- [NVIDIA Developer Forums: Setting group/mode on /dev/nvidia*](https://forums.developer.nvidia.com/t/setting-group-mode-on-dev-nvidia/49418) - User reports of udev rule issues
- [ArchLinux Bug: wrong permission of /dev/nvidia*](https://bugs.archlinux.org/task/5682) - Historic device permission bugs
- [Video unix group discussion (NVIDIA Forums)](https://forums.developer.nvidia.com/t/video-unix-group/183294) - Video group purpose

### DS01 Internal Sources (HIGH confidence - system ground truth)
- `.planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md` - Three-layer architecture design decision
- `.planning/phases/cross-phase-audit/cross-phase-UAT.md` - System audit findings (8 issues)
- `/etc/profile.d/` permissions audit - Real system state (644 standard, 600 broken)
- `/etc/udev/rules.d/99-ds01-nvidia.rules` - Currently deployed (contradicts design)
- `scripts/docker/get_resource_limits.py` - Working .members file merge logic

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - HPC patterns well-documented, Linux permissions universal
- Architecture patterns: HIGH - SLURM, Kubernetes, OWASP guidance verified
- Pitfalls: HIGH - Derived from DS01 UAT audit (real system failures) + community reports
- Open questions: MEDIUM-LOW - Design ambiguities requiring user decisions

**Research date:** 2026-02-01
**Valid until:** 90 days (Linux deployment patterns stable; OWASP guidance current through 2025)

**Next steps:**
- User review of open questions (video group strategy, nvidia wrapper deployment)
- Planning phase: Wave 1 (permissions + allocator), Wave 2 (bare metal deployment), Wave 3 (validation)
