# Phase 3.2: Architecture Audit & Code Quality - Research

**Researched:** 2026-02-05
**Domain:** Architecture validation methodologies, code audit practices, config management patterns
**Confidence:** MEDIUM

## Summary

This research investigates how to structure a comprehensive architecture audit and code quality review for Phases 1–3.1 of DS01 infrastructure. Unlike typical software feature development, this is a retrospective validation phase with three workstreams: (1) validating design decisions against SLURM/Kubernetes/HPC industry standards, (2) identifying and fixing code quality issues through systematic audit, and (3) consolidating configuration files into a single source of truth with generative deployment patterns.

The Phase 2.1 research already validated GPU access control architecture against HPC/Kubernetes patterns (CUDA_VISIBLE_DEVICES + container device mapping + video group exemptions). This phase extends that validation methodology to all other architectural decisions: event logging, workload detection, container isolation, deployment patterns, and resource enforcement.

**Key findings:**
- Architecture validation requires comparing implementation against SLURM, Kubernetes, and HPC cluster patterns across 7 subsystems
- Code audit for infrastructure scripts uses different tooling than application code: ShellCheck for bash, vulture/deadcode for Python, manual review for logic
- Config consolidation best practice: hierarchical SSOT with template-based generation (deploy-time variable substitution) not declarative copies
- Planning document audits check: do PLAN.md files contain enough detail for independent execution? Do STATE.md and ROADMAP.md reflect current reality?
- HPC event logging validated against structured JSON, append-only, audit trail best practices (DS01 already aligned)

**Primary recommendation:** Use SLURM/K8s/HPC patterns as validation baseline. Audit code with severity tiers (Critical/High/Medium/Low). Consolidate config using deploy/runtime/state hierarchy with generative patterns. Fix Critical+High issues in-phase, catalogue rest for backlog.

## Standard Stack

The established patterns and tools for infrastructure audit:

### Core Validation References
| Reference | Version/Status | Purpose | Why Standard |
|-----------|----------------|---------|--------------|
| SLURM cgroup docs | 23.11+ | HPC resource isolation patterns | Production HPC standard, 20+ years mature, handles GPU/CPU/memory/IO |
| Kubernetes patterns | v1.35+ | Cloud-native container orchestration | De facto container platform, GPU multi-tenancy solutions well-documented |
| systemd resource control | 249+ | Modern Linux resource management | Upstream standard for cgroup v2, DeviceAllow, slice management |
| NVIDIA Container Toolkit | 1.17.8+ | GPU container allocation | Official NVIDIA standard, integrates Docker + cgroups + GPU access |
| Docker authorization plugins | 19.03+ | Multi-user container isolation | Standard extension point for access control, used by major platforms |

### Code Analysis Tools
| Tool | Version | Purpose | When to Use |
|------|---------|---------|-------------|
| ShellCheck | 0.10+ | Static analysis for shell scripts | All bash scripts, detects portability issues, dangerous patterns |
| vulture | 2.13+ | Dead code detection for Python | Find unused functions, variables, imports |
| ruff | 0.8+ | Python linting and formatting | Already in project, fast, replaces multiple tools |
| deadcode | Latest | Python dead code with imports analysis | More aggressive than vulture, finds transitively unused code |
| Manual review | N/A | Logic validation, architecture alignment | Complex logic, error handling, race conditions |

### Configuration Management Patterns
| Pattern | Status | Purpose | Why Standard |
|---------|--------|---------|--------------|
| Hierarchical SSOT | Established | Organise by lifecycle (deploy/runtime/state) | Separates concerns, clear ownership, deploy-time generation |
| Template-based generation | Established | Fill variables at deploy time | Reduces duplication, per-environment config without copies |
| Generative not declarative | Established | Compute derived config vs store copies | Single source in git, deployed configs are artifacts |
| Permissions manifest | DS01 pattern | Deterministic file permissions on deploy | Eliminates umask/checkout drift, explicit chmod/chown |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Manual architecture audit | Automated conformance tools | No mature tools for HPC/infrastructure pattern validation, manual more thorough |
| ShellCheck | Manual review only | ShellCheck catches 80% of common issues, manual for remaining 20% |
| Config templates | Declarative copies (Ansible/Chef) | Templates lighter weight, no orchestration layer needed |
| SLURM/K8s patterns | Custom designs | Industry patterns proven at scale, avoid reinventing, easier to hire/onboard |

**Installation:**
```bash
# Code analysis tools
pip install vulture deadcode ruff
sudo apt-get install shellcheck

# Already in project: ruff via pre-commit hooks
```

## Architecture Patterns

### Pattern 1: Tiered Validation Against Industry Standards

**What:** Compare each DS01 architectural decision against how SLURM, Kubernetes, and HPC clusters solve the same problem. Pass/fail verdict with evidence.

**When to use:** Architecture audit of infrastructure system where industry patterns exist.

**Subsystems to validate:**
1. **Event logging:** Structured JSON, append-only, audit trail, never-block pattern
2. **Workload detection:** Polling vs event-driven, transient filtering, classification
3. **Container isolation:** User filtering, ownership verification, fail-open for unowned
4. **GPU access control:** Layer 1 (CUDA_VISIBLE_DEVICES) + Layer 2 (Docker devices) + Layer 3 (video group)
5. **Deployment:** Self-bootstrap, deterministic permissions, idempotent deployment
6. **GPU allocation:** Fail-open exception handling, MIG slot management, file locking
7. **Config management:** SSOT hierarchy, generative patterns, lifecycle separation

**Validation template per subsystem:**
```markdown
### [Subsystem]: [Decision]

**DS01 Approach:** [What we do]

**Industry Pattern:**
- SLURM: [How SLURM does it]
- Kubernetes: [How K8s does it]
- HPC clusters: [Common bare-metal pattern]

**Verdict:** [PASS / PASS-WITH-NOTES / FAIL]

**Evidence:**
- [Reference 1 URL]
- [Reference 2 URL]

**Notes:** [Deviations justified, concerns, improvements]
```

**Example (Event Logging):**
```markdown
### Event Logging: Structured JSON with Never-Block Pattern

**DS01 Approach:**
- JSON Lines format (/var/log/ds01/events.jsonl)
- Never-block: returns False on error, never raises
- Atomic writes (<4KB PIPE_BUF guarantee)
- Bash-via-CLI bridge for shell script logging

**Industry Pattern:**
- SLURM: syslog for job events, JSON optional, blocking writes acceptable
- Kubernetes: Structured logging (klog), audit webhooks for compliance
- HPC clusters: Append-only audit logs standard, JSON increasingly adopted

**Verdict:** PASS

**Evidence:**
- Structured logging best practices: https://www.joyfulprogramming.com/p/structured-logging-best-practices
- JSONL for append-only: https://jsonlines.org/
- Audit trail requirements: https://moss.sh/devops-monitoring/devops-audit-logging-best-practices/

**Notes:** Never-block pattern is stronger than typical HPC (where job submission can wait on logging). JSONL format industry-standard for streaming logs. No concerns.
```

**Source:** Derived from Phase 2.1 GPU access control validation methodology

### Pattern 1b: Layer Elimination Challenge

**What:** For multi-layer architectures (like GPU access control), systematically test whether each layer can be removed. Forces evidence-based justification for complexity.

**When to use:** Validating that an N-layer design isn't over-engineered. Each layer must justify its existence.

**Method:** For each layer, ask: "What do we lose if we remove this?"

**Example (three-layer GPU access control):**

| Remove | What's Lost | Verdict |
|--------|------------|---------|
| Layer 1 (CUDA_VISIBLE_DEVICES) | No UX deterrent on host — silent GPU usage, detection burden shifts entirely to Phase 2 scanner | Keep — cheap deterrent, reduces monitoring load |
| Layer 2 (Docker device mapping) | No kernel-level isolation — any container gets all GPUs regardless of allocation | Keep — this IS the security boundary |
| Layer 3 (video group exemptions) | No bare-metal access for anyone — admins also containerised, no emergency debugging | Keep — operational flexibility required |
| Consolidate L1+L3 | Profile.d already does this — conditionally sets CUDA_VISIBLE_DEVICES based on exemption status | Already done (Phase 02.1-02) — logical consolidation achieved |

**Conclusion template:**
- If removing any single layer creates an unacceptable gap → architecture is minimal
- If two layers serve the same purpose → consolidate
- If a layer exists "just in case" with no concrete scenario → remove

**Source:** Validated by independent audit (haiku-3.2-work branch reached same conclusion via elimination analysis)

### Pattern 2: Severity-Tiered Code Audit

**What:** Systematically review all scripts/modules with findings categorised by severity. Fix Critical+High in-phase, catalogue rest.

**When to use:** Code quality audit of existing codebase before next phase.

**Severity definitions:**
| Severity | Definition | Examples | Action |
|----------|------------|----------|--------|
| **CRITICAL** | Security issue, data loss, system unavailability | Race condition in GPU allocator causing double-allocation; uncaught exception blocking all containers | Fix immediately |
| **HIGH** | Reliability issue, incorrect behaviour, significant UX problem | Silent failure in event logging losing audit trail; file permission check missing | Fix in-phase |
| **MEDIUM** | Maintainability issue, minor UX problem, code smell | Duplicate logic across 3 scripts; inconsistent error messages; missing type hints | Catalogue, schedule |
| **LOW** | Style issue, documentation gap, minor optimisation | Variable naming inconsistency; missing docstring; redundant variable | Catalogue, low priority |

**Security checkpoint (include in audit):**
- CVE-2025-23266 (NVIDIA Container Toolkit privilege escalation) — verify nvidia-ctk >= 1.17.8 or config.toml workaround applied. Already flagged in STATE.md but must be verified during audit.

**Implicit assumptions to probe per subsystem** (common sources of hidden bugs):
- Naming conflicts: Can a username collide with a reserved value? (e.g., user named "default" in GPU allocator config)
- Filesystem assumptions: Will paths work on NFS with root_squash? Are parent directories guaranteed writable?
- Group/user existence: Does the script assume `docker` group, `datasciencelab` user, or `video` group exist without checking?
- Command availability: Does `groups`, `jq`, `envsubst` exist on all target systems?
- Format fragility: Does parsing rely on specific output format (e.g., MIG detection via `'.' in slot`, nvidia-smi output columns)?

**Audit checklist per script:**
```markdown
### File: scripts/[path]/[filename]

**Purpose:** [1-line description]

**Criticality:** [CRITICAL / HIGH / MEDIUM / LOW]
- CRITICAL: GPU allocation, container creation, deploy.sh
- HIGH: Event logging, wrapper authorization, cleanup jobs
- MEDIUM: Query tools, dashboards, helpers
- LOW: Utilities, formatters, one-off scripts

**Issues Found:**

#### [SEVERITY] [Issue Title]
**What:** [Description]
**Impact:** [User/system impact]
**Fix:** [Proposed solution]
**Effort:** [minutes/hours estimate]

**Dead Code:** [List of unused functions/variables]

**Duplicates:** [Similar logic in other files]

**Dependencies:** [External commands, libraries checked for availability]
```

**Example audit entry:**
```markdown
### File: scripts/docker/gpu_allocator_v2.py

**Purpose:** Stateless GPU allocation with file locking

**Criticality:** CRITICAL

**Issues Found:**

#### HIGH: File lock timeout missing
**What:** File lock acquisition blocks indefinitely if lock file stuck
**Impact:** Container creation hangs forever, requires manual intervention
**Fix:** Add timeout parameter to fcntl.flock, fail-open after 5 seconds
**Effort:** 15 minutes

#### MEDIUM: MIG slot representation fragile
**What:** Uses string check `'.' in gpu` to detect MIG, breaks if UUID contains dot
**Impact:** Incorrect classification if NVIDIA changes UUID format
**Fix:** Parse via nvidia-smi output or explicit MIG flag
**Effort:** 30 minutes

**Dead Code:** None found

**Duplicates:** get_current_allocations() similar to gpu-state-reader.py, consider consolidation
```

**Sources:**
- Code audit tools: [Jit.io Python Analysis Tools](https://www.jit.io/resources/appsec-tools/top-python-code-analysis-tools-to-improve-code-quality), [CodeAnt Audit Tools](https://www.codeant.ai/blogs/10-best-code-audit-tools-to-improve-code-quality-security-in-2025)
- Dead code detection: [vulture](https://github.com/jendrikseipp/vulture), [deadcode](https://github.com/albertas/deadcode)

### Pattern 3: Config Consolidation with Generative Patterns

**What:** Merge config/deploy/ and config/etc-mirrors/ into hierarchical SSOT organised by lifecycle (deploy/runtime/state), use template substitution for environment-specific values.

**When to use:** Config sprawl across multiple directories with duplication.

**Target structure:**
```
config/
├── deploy/              # Install-time configs (deployed TO /etc/)
│   ├── systemd/         # Service units, timers
│   ├── profile.d/       # Environment setup scripts
│   ├── logrotate.d/     # Log rotation rules
│   └── cron.d/          # Scheduled jobs
├── runtime/             # Per-operation configs (read during execution)
│   ├── resource-limits.yaml      # Resource quotas
│   ├── exemptions.yaml           # Bare-metal access grants
│   └── container-policies.yaml   # Lifecycle enforcement rules
├── state/               # Persistent data directories (created at deploy)
│   ├── grants/          # Temporary access grants (drwx--x--x root:root 711)
│   ├── rate-limits/     # Per-user rate limit state (drwxrwxrwt root:root 1777)
│   └── manifests/       # Container/GPU state (drwxrwxr-x root:docker 775)
└── variables.env        # Deploy-time variables (usernames, paths, environment)
```

**Current state (before consolidation):**
- `config/deploy/`: Files to copy TO /etc/ (incomplete, missing some profile.d)
- `config/etc-mirrors/`: Reference copies FROM /etc/ (duplication, out-of-sync risk)
- Overlap: profile.d scripts in both directories
- No clear lifecycle separation

**Generative pattern (template example):**
```bash
# config/deploy/profile.d/ds01-gpu-awareness.sh.template
#!/bin/bash
# DS01 GPU Awareness - Hide GPUs from host processes

# Exempt users (filled at deploy time)
EXEMPT_USERS="${EXEMPT_USERS}"

# Check if current user exempt
if id "$USER" | grep -qE "(${EXEMPT_USERS})"; then
    # User has bare-metal access
    export DS01_GPU_ACCESS="bare-metal"
else
    # Block host GPU access
    export CUDA_VISIBLE_DEVICES=""
    export DS01_GPU_ACCESS="container-only"
fi
```

**Deploy-time generation:**
```bash
# deploy.sh
source config/variables.env

# Generate from template
fill_config_template() {
    local template="$1"
    local output="$2"

    # Substitute variables
    envsubst < "$template" > "$output"

    # Set permissions from manifest
    apply_permissions "$output"
}

fill_config_template \
    config/deploy/profile.d/ds01-gpu-awareness.sh.template \
    /etc/profile.d/ds01-gpu-awareness.sh
```

**Variables file:**
```bash
# config/variables.env
# Deploy-time variables (not tracked in git - generated or provided)

EXEMPT_USERS="datasciencelab|h\.baker|j\.smith"
STATE_DIR="/var/lib/ds01"
LOG_DIR="/var/log/ds01"
INFRA_ROOT="/opt/ds01-infra"
```

**Benefits:**
- Single source: Template in git, deployed file is artifact
- Per-environment: Different variables.env per server
- No duplication: User list in one place
- Clear lifecycle: deploy/ = install, runtime/ = execution, state/ = persistence
- Testable: Can generate to temp dir for validation

**Sources:**
- Template deployment: [Google Cloud Deployment Manager Templates](https://cloud.google.com/deployment-manager/docs/configuration/templates/create-basic-template)
- Config management: [F5 Configuration Management with Templates](https://techdocs.f5.com/kb/en-us/products/big-iq-centralized-mgmt/manuals/product/big-iq-centralized-management-device-6-0-0/8.html)

### Pattern 4: Planning Document Quality Audit

**What:** Review PLAN.md files for executability, STATE.md/ROADMAP.md for accuracy, research alignment check.

**When to use:** Before next phase to ensure planning quality supports execution.

**Audit dimensions:**

**1. PLAN.md Executability:**
Can someone else execute this plan without questions?
```markdown
### Plan: [phase]-[number]-PLAN.md

**Clarity Check:**
- [ ] Goal clear in 1 sentence
- [ ] Tasks have acceptance criteria
- [ ] Prerequisites/dependencies listed
- [ ] File paths absolute not relative
- [ ] Commands copy-pasteable
- [ ] Verification steps concrete

**Completeness Check:**
- [ ] All config files specified
- [ ] All affected scripts listed
- [ ] Error cases handled
- [ ] Rollback procedure if applicable
- [ ] Manual steps flagged as checkpoints

**Score:** [1-5] (5 = perfect, 1 = needs major revision)
**Issues:** [List gaps]
```

**2. STATE.md / ROADMAP.md Accuracy:**
Do planning docs reflect current reality?
```markdown
### Document: [filename]

**Drift Check:**
- [ ] Current position accurate
- [ ] Completed phases marked
- [ ] Pending todos up-to-date
- [ ] Blockers still blocking
- [ ] Decisions reflect implementation
- [ ] Velocity metrics current

**Orphaned Items:**
- [List todos that are done but not marked]
- [List completed phases not checked off]
- [List stale blockers]

**Missing Items:**
- [Implementation decisions not documented]
- [New blockers not captured]
```

**3. Research Alignment:**
Do implementations match research conclusions?
```markdown
### Subsystem: [name]

**Research Conclusion:** [From phase X.X-RESEARCH.md]

**Implementation:** [What was built]

**Alignment:** [MATCH / JUSTIFIED-DEVIATION / DRIFT]

**Evidence:** [File paths, commit references]

**Notes:** [If deviated, why? If drifted, intentional or bug?]
```

**Example research alignment check:**
```markdown
### Subsystem: Event Logging

**Research Conclusion:** (Phase 01-RESEARCH.md)
- Use JSONL format for append-only logs
- Never-block pattern: return False on error
- Bash-via-CLI bridge for shell scripts
- Copytruncate for logrotate

**Implementation:**
- ✓ JSONL: /var/log/ds01/events.jsonl
- ✓ Never-block: ds01_events.py log_event() returns bool
- ✓ Bash bridge: ds01_events.sh sources Python CLI
- ✓ Logrotate: config/deploy/logrotate.d/ds01 uses copytruncate

**Alignment:** MATCH

**Evidence:**
- scripts/lib/ds01_events.py lines 40-60
- scripts/lib/ds01_events.sh
- config/deploy/logrotate.d/ds01

**Notes:** Implementation fully aligned with research. No drift.
```

**Planning quality baseline (from independent audit):** Prior independent review of all 18 Phase 1–3.1 plans found 13/18 fully clear, 5/18 with minor gaps, 0/18 unclear. Expect the planning document audit to be lightweight — focus on the known gap areas: credential placeholder ambiguity (01-03), subjective verification steps (02.1-02), outdated plans not updated after research revisions (03-03), misleading titles (03.1-03). Quality improves from Phase 01 → 03.1.

**Sources:** Architecture Decision Records (ADR) validation practices from [Microsoft ADR guidance](https://learn.microsoft.com/en-us/azure/well-architected/architect-role/architecture-decision-record), [ADR best practices](https://www.techtarget.com/searchapparchitecture/tip/4-best-practices-for-creating-architecture-decision-records)

### Anti-Patterns to Avoid

1. **Architecture audit without baseline:**
   - Why it's bad: "This looks wrong" without evidence is opinion not audit. Need SLURM/K8s reference.
   - What to do instead: Every verdict cites specific industry pattern, documentation, or reference implementation.

2. **Fixing everything in audit phase:**
   - Why it's bad: Scope creep, delays next phase, mixing validation with improvement.
   - What to do instead: Fix Critical+High only. Catalogue Medium+Low for future phases. Audit is about identifying issues, not solving all of them.

3. **Config consolidation preserving duplication:**
   - Why it's bad: Moving deploy/ + etc-mirrors/ into subdirectories doesn't solve duplication problem.
   - What to do instead: Identify duplicated data (user lists, paths), extract to variables, generate at deploy time.

4. **Manual review without tools:**
   - Why it's bad: ShellCheck catches 80% of bash issues automatically. Wastes manual review time on mechanical issues.
   - What to do instead: Run ShellCheck first, manual review for logic/architecture after tools pass.

5. **Planning document audit focusing on format:**
   - Why it's bad: Well-formatted plan that's missing critical info is still bad.
   - What to do instead: Executability test - could someone else run this plan successfully? Missing dependencies, unclear acceptance criteria, vague tasks all fail this test.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Shell script static analysis | Manual review only | ShellCheck | Catches quoting issues, undefined variables, portability problems, dangerous patterns |
| Dead code detection | Manual review only | vulture or deadcode | Finds unused functions, imports, variables automatically; manual review misses transitive unused code |
| Config template substitution | Custom Python script | envsubst or Python's string.Template | Standard tools, handles escaping, well-tested |
| Architecture validation | Bespoke checklist | Industry reference patterns (SLURM, K8s docs) | Proven at scale, community-validated, easier to justify decisions |
| Severity classification | Subjective assessment | Standard severity model (Critical/High/Medium/Low with definitions) | Consistent prioritisation, clear for stakeholders |

**Key insight:** Infrastructure audit has established patterns (ShellCheck for bash, industry references for architecture, severity models for prioritisation). Don't invent validation frameworks when proven ones exist.

## Common Pitfalls

### Pitfall 1: Audit Scope Creep

**What goes wrong:** Start with "validate architecture", end with "rewrite everything", phase never completes.

**Why it happens:** Finding issues triggers desire to fix immediately. Refactoring reveals more issues. No clear stop condition.

**How to avoid:**
- Define severity tiers upfront (Critical/High fix now, Medium/Low catalogue)
- Separate audit (identify issues) from remediation (fix issues)
- Critical+High fixes only in audit phase
- Time-box audit: 2 hours research, 1 hour per subsystem validation, 3 hours code review
- Defer improvements to appropriate future phase

**Warning signs:**
- "While we're at it, let's also..."
- Audit phase stretching beyond 1 week
- More code written than reviewed
- Backlog items getting implemented instead of catalogued

**DS01 context:** Phase 4 (Resource Enforcement) is next. Audit shouldn't delay it. Fix blockers only.

### Pitfall 2: Validation Without Evidence

**What goes wrong:** "This doesn't match Kubernetes" without citing what Kubernetes actually does. Verdict based on assumption not research.

**Why it happens:** Familiarity with technology creates false confidence. Memory of "best practices" without source.

**How to avoid:**
- Every "FAIL" verdict requires URL evidence from official docs
- "Industry pattern" must cite specific implementation (SLURM docs, K8s GitHub, HPC centre documentation)
- If can't find evidence, verdict is "UNCERTAIN" not "FAIL"
- Phase 2.1 research already covers GPU access control - reuse those findings

**Warning signs:**
- Verdicts without source citations
- "Everyone knows X" statements
- "Best practice is Y" without reference
- Comparing to remembered pattern not documented pattern

**Example fix:**
- ❌ "FAIL: Docker wrapper is non-standard, Kubernetes uses OPA"
- ✓ "PASS-WITH-NOTES: Docker wrapper provides authorization without OPA complexity. K8s has admission controllers but bare Docker has authorization plugins as extension point (docs.docker.com/engine/extend/plugins_authorization). Wrapper approach is within Docker's design. Note: OPA adoption path exists if needed."

**Sources:** [ADR review process requirements](https://github.com/joelparkerhenderson/architecture-decision-record)

### Pitfall 3: Config Consolidation as File Reorganisation

**What goes wrong:** Move config/deploy/ → config/deploy-time/, config/etc-mirrors/ → config/runtime/, declare success. Duplication remains.

**Why it happens:** Confusing file organisation with data organisation. Directories are surface, duplication is structure.

**How to avoid:**
- Identify what's duplicated: user lists, paths, settings
- Extract variables: create config/variables.env with single-source data
- Convert to templates: replace hardcoded values with ${VARIABLE} placeholders
- Generate at deploy: deploy.sh fills templates from variables.env
- Test: Can you change user list in one place and have it propagate everywhere?

**Warning signs:**
- Just moved files between directories
- Same user list appears in multiple files
- Environment-specific values hardcoded in templates
- Config "consolidation" doesn't reduce line count

**Example of real consolidation:**

Before:
```bash
# config/deploy/profile.d/ds01-gpu-awareness.sh
if id "$USER" | grep -qE "(datasciencelab|h\.baker)"; then

# scripts/admin/bare-metal-access
EXEMPT_USERS="datasciencelab h.baker"

# deploy.sh
usermod -aG video datasciencelab
usermod -aG video h.baker
```

After:
```bash
# config/variables.env
EXEMPT_USERS="datasciencelab|h\.baker"

# config/deploy/profile.d/ds01-gpu-awareness.sh.template
if id "$USER" | grep -qE "(${EXEMPT_USERS})"; then

# scripts/admin/bare-metal-access
source /opt/ds01-infra/config/variables.env
for user in $(echo "$EXEMPT_USERS" | tr '|' ' '); do
```

Single source, three consumers.

**Sources:** Template-based configuration management patterns from [Google Cloud Deployment Manager](https://cloud.google.com/deployment-manager/docs/configuration/templates/create-basic-template)

### Pitfall 4: Fail-Open vs Fail-Closed Misapplication

**What goes wrong:** Apply wrong failure mode to wrong layer. User operations fail-closed (block containers), system operations fail-open (ignore errors), both wrong.

**Why it happens:** Tiered error handling decision (02.1-02) requires context-specific application. Easy to reverse.

**How to avoid:**
- User-facing operations (container create, GPU allocation): Fail-open
  - Rationale: Availability over restriction. GPU allocator error shouldn't block non-GPU container.
  - Implementation: Try-except returns safe default (empty GPU list), container succeeds
- System operations (deploy.sh, service startup): Fail-closed
  - Rationale: System integrity over availability. Partially-deployed system is dangerous.
  - Implementation: set -euo pipefail, exit on any error, requires manual fix
- Logging/monitoring: Best-effort (never block)
  - Rationale: Observability failure shouldn't cause operation failure
  - Implementation: log_event || true, catch exceptions and warn

**Example validation:**
```bash
# CORRECT: User operation fail-open
python3 gpu_allocator.py allocate "$USER" "$CONTAINER" || {
    # Allocation failed, but don't block container creation
    echo "Warning: GPU allocation failed, creating without GPU" >&2
    GPU_FLAG=""
}

# CORRECT: System operation fail-closed
if ! apply_permissions_manifest; then
    echo "ERROR: Permission manifest application failed" >&2
    exit 1
fi

# CORRECT: Logging best-effort
log_event "container.create" "$USER" "$CONTAINER" || true
```

**Warning signs in audit:**
- Container creation blocked by logging failure
- deploy.sh continues after permission error
- GPU allocation error causes script exit instead of graceful degradation

**Sources:**
- Fail-open vs fail-closed patterns: [AuthZed fail open blog](https://authzed.com/blog/fail-open), [Cisco fail-open explanation](https://community.cisco.com/t5/security-knowledge-base/fail-open-amp-fail-close-explanation/ta-p/5012930)
- DS01 context: STATE.md line 91 "Fail-open exception handling for GPU allocation chain"

### Pitfall 5: Dead Code vs Failed Approaches

**What goes wrong:** Remove all unused code including experiments and failed approaches, lose architectural context.

**Why it happens:** Dead code detection tools flag everything unused. Tempting to delete it all.

**How to avoid:**
- Confirmed dead code → Remove (e.g., `ORIGINAL_MLC` variable never referenced)
- Failed approaches → Document in commit message, then remove
- Incomplete implementations (stubs, TODOs) → Catalogue not implement
- Alternative implementations kept "just in case" → Remove (git history preserves)

**Example decision tree:**
```
Is code referenced?
├─ Yes → Keep
└─ No → Is it a documented alternative approach?
    ├─ Yes → Document why not used, then remove
    └─ No → Is it an incomplete implementation?
        ├─ Yes → Catalogue in backlog, remove stub
        └─ No → Remove immediately (dead code)
```

**Documentation pattern for failed approaches:**
```bash
git commit -m "refactor: remove device permission manipulation approach

Device permissions (0660, video group) approach removed in favour of
CUDA_VISIBLE_DEVICES environment variable hiding. Device permission
approach failed because:
- nvidia-smi requires device access for all queries
- Monitoring tools (DCGM) broken by restricted permissions
- Race conditions with nvidia-persistenced

Replaced with three-layer architecture (Phase 2.1-02):
- Layer 1: CUDA_VISIBLE_DEVICES=\"\" (host deterrent)
- Layer 2: Docker device mapping (container security)
- Layer 3: Video group exemption (opt-in bare-metal)

Research: .planning/phases/02.1-gpu-access-control-research/02.1-RESEARCH.md
Design: .planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md"
```

Version control has the code. Commit message has the why.

**Sources:** Vulture dead code detection: [vulture GitHub](https://github.com/jendrikseipp/vulture), [deadcode tool](https://github.com/albertas/deadcode)

## Code Examples

Verified patterns from research and existing DS01 implementation:

### ShellCheck Static Analysis (Bash Scripts)

```bash
# Run on all scripts
find scripts -name "*.sh" -exec shellcheck {} \;

# Run with severity filter (error and warning only, skip style)
shellcheck -S error -S warning scripts/docker/docker-wrapper.sh

# Check for specific issues
shellcheck -s bash -e SC2086,SC2162 script.sh  # Disable specific checks if justified

# Integration with CI (exit code non-zero on issues)
shellcheck scripts/**/*.sh || echo "ShellCheck found issues"
```

**Common issues ShellCheck catches:**
- SC2086: Unquoted variable expansion (word splitting risk)
- SC2046: Quote command substitution to prevent word splitting
- SC2162: Read without -r mangles backslashes
- SC2181: Check exit status directly: `if cmd; then` not `if [ $? -eq 0 ]`

**Source:** [ShellCheck documentation](https://www.shellcheck.net/)

### Vulture Dead Code Detection (Python)

```bash
# Run on Python codebase
vulture scripts/

# With minimum confidence (0-100, higher = more certain it's dead)
vulture --min-confidence 80 scripts/

# Generate allowlist for false positives
vulture scripts/ --make-whitelist > vulture_allowlist.py
vulture scripts/ vulture_allowlist.py  # Subsequent runs ignore allowlist items

# Example output:
# scripts/docker/gpu_allocator_v2.py:145: unused function 'legacy_allocate' (90% confidence)
# scripts/lib/ds01_core.py:67: unused variable 'TIMEOUT' (100% confidence)
```

**Integration with audit:**
1. Run vulture with min-confidence 80
2. For each finding: Is this actually unused?
   - Yes, confirmed dead → Remove
   - No, false positive → Add to allowlist with comment explaining usage
   - Uncertain → Mark for manual review

**Source:** [Vulture documentation](https://github.com/jendrikseipp/vulture)

### Config Template Generation (deploy.sh pattern)

```bash
#!/bin/bash
# deploy.sh config generation section

# Source variables
source config/variables.env

# Template substitution function
fill_config_template() {
    local template_file="$1"
    local output_file="$2"

    # Validate template exists
    if [ ! -f "$template_file" ]; then
        echo "ERROR: Template $template_file not found" >&2
        exit 1
    fi

    # Generate config
    envsubst < "$template_file" > "$output_file"

    # Verify substitution worked (no ${VAR} remaining)
    if grep -q '\${[A-Z_]*}' "$output_file"; then
        echo "ERROR: Template $template_file has unsubstituted variables" >&2
        cat "$output_file" | grep '\${[A-Z_]*}'
        exit 1
    fi

    echo "Generated: $output_file"
}

# Generate systemd service
fill_config_template \
    config/deploy/systemd/ds01-workload-detector.service.template \
    /etc/systemd/system/ds01-workload-detector.service

# Set permissions from manifest
apply_permissions /etc/systemd/system/ds01-workload-detector.service
```

**Template example:**
```ini
# config/deploy/systemd/ds01-workload-detector.service.template
[Unit]
Description=DS01 Workload Detector

[Service]
Type=simple
ExecStart=${INFRA_ROOT}/scripts/monitoring/detect-workloads.py
User=root
Environment="STATE_DIR=${STATE_DIR}"
Environment="LOG_DIR=${LOG_DIR}"
Restart=always

[Install]
WantedBy=multi-user.target
```

**Variables file:**
```bash
# config/variables.env
INFRA_ROOT="/opt/ds01-infra"
STATE_DIR="/var/lib/ds01"
LOG_DIR="/var/log/ds01"
EXEMPT_USERS="datasciencelab|h\.baker"
```

**Source:** Google Cloud Deployment Manager template patterns

### Severity-Based Audit Report

```markdown
# Code Audit Report: Phase 1-3.1

**Scope:** 21 Python scripts, 91 shell scripts, 65 config files
**Audited:** 2026-02-05
**Duration:** 4 hours

## Summary

| Severity | Count | Fixed | Deferred |
|----------|-------|-------|----------|
| Critical | 2     | 2     | 0        |
| High     | 7     | 7     | 0        |
| Medium   | 12    | 0     | 12       |
| Low      | 8     | 0     | 8        |
| **Total** | **29** | **9** | **20** |

## Critical Issues (All Fixed)

### CRITICAL-01: GPU Allocator File Lock Timeout Missing
**File:** `scripts/docker/gpu_allocator_v2.py:234`
**Issue:** File lock blocks indefinitely if lockfile stuck
**Impact:** Container creation hangs forever, requires kill -9 and manual lockfile removal
**Fix:** Added 5-second timeout with SIGALRM, fail-open on timeout
**Status:** ✅ Fixed (commit abc1234)

### CRITICAL-02: Deploy.sh Permission Manifest Not Applied
**File:** `scripts/system/deploy.sh:456`
**Issue:** Permissions manifest sourced but not executed
**Impact:** Scripts deployed with 644 permissions, non-admin users cannot execute
**Fix:** Call apply_permissions_manifest after file deployment
**Status:** ✅ Fixed (commit def5678)

## High Severity Issues (All Fixed)

### HIGH-01: Event Log Size Limit Not Enforced
**File:** `scripts/lib/ds01_events.py:89`
**Issue:** No check for event size before write
**Impact:** Events >4KB break atomic write guarantee, corrupt log file
**Fix:** Added MAX_EVENT_SIZE check, truncate details if needed
**Status:** ✅ Fixed (commit 789abcd)

[... 6 more HIGH issues ...]

## Medium Severity Issues (Deferred)

### MEDIUM-01: MIG Slot Representation Fragile
**File:** `scripts/docker/gpu_allocator_v2.py:123`
**Issue:** Uses string check `'.' in gpu` to detect MIG
**Impact:** Breaks if UUID format changes, incorrect classification
**Recommendation:** Parse via nvidia-smi or explicit MIG flag
**Deferred to:** Phase 3.3 (GPU allocation refactoring)

[... 11 more MEDIUM issues ...]

## Deferred Items Backlog

See: `.planning/backlog/phase-3.2-deferred-items.md`

Total: 20 items (12 MEDIUM, 8 LOW)
Estimated effort: 12 hours
Suggested phases: 3.3 (GPU refactor), 4.1 (cgroup integration), 5.1 (cleanup refactor)
```

**Source:** Infrastructure code audit methodology from [STX Next Python code audit guide](https://www.stxnext.com/blog/how-to-audit-the-quality-of-your-python-code)

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Manual architecture validation | Industry pattern comparison (SLURM, K8s, HPC) | Established 2020+ | Objective baselines, evidence-based decisions, easier justification |
| Bash scripts without static analysis | ShellCheck in CI pipeline | 2015+ widely adopted | 80% fewer quoting bugs, portability issues caught automatically |
| Config duplication (deploy + etc-mirrors) | Template-based generation with variables | Google GCP pattern 2018+ | Single source of truth, per-environment configs without copies |
| Fix all issues in audit phase | Severity tiers (Critical/High fix now, Medium/Low defer) | Standard project management | Audit phase bounded, critical issues addressed, backlog managed |
| Subjective code review | Tooling + manual review for logic | Modern DevOps 2020+ | Tools handle mechanical issues, humans focus on architecture |

**Deprecated/outdated:**
- **Config file copies in version control:** Modern approach uses templates + generation (deployed files are artifacts). Copies get out of sync.
- **Bash without ShellCheck:** Static analysis for bash is mature and fast. No excuse for skipping it.
- **Architecture decisions without ADRs:** Modern practice documents decisions with context and alternatives. Makes audits possible.
- **All-or-nothing audit:** Fixing everything delays next phase. Severity tiers + backlog management is standard.

**Recent developments (2025-2026):**
- **AI-assisted code review:** Tools like GitHub Copilot can suggest refactorings, but still need human validation for architecture alignment
- **Infrastructure-as-Code quality frameworks:** New frameworks for measuring IaC quality (arXiv:2502.03127) emerging but not yet standardised
- **SLURM + Kubernetes convergence:** Project Slinky, Soperator bringing SLURM scheduling to K8s, validating that DS01's hybrid approach (HPC + container patterns) is industry direction

## Open Questions

Things that couldn't be fully resolved:

1. **Question: How deep should code review go for low-criticality scripts?**
   - What we know: Critical scripts (GPU allocator, deploy.sh) need deep review. Query tools less critical.
   - What's unclear: Optimal time allocation across 112 scripts. Diminishing returns point?
   - Recommendation: Adaptive depth per criticality (1 hour for critical, 15 min for low). Manual judgment call.

2. **Question: When is config "consolidated enough"?**
   - What we know: User lists, paths, environment-specific values should be in variables.env. Templates should reference variables.
   - What's unclear: Every hardcoded value extractable to variable, but some not worth it (e.g., systemd target "multi-user.target" unlikely to change).
   - Recommendation: Extract if value appears in 2+ files OR is environment-specific. Don't extract immutable standards.

3. **Question: Should architecture validation extend to future phases?**
   - What we know: Phases 1-3.1 implemented, can validate. Phase 4+ planned but not built.
   - What's unclear: Value of validating Phase 4 design (cgroup v2 resource enforcement) before implementation vs during.
   - Recommendation: Validate Phase 4 high-level approach only (cgroup v2 vs v1, systemd integration). Detailed validation during implementation.

4. **Question: How to handle justified deviations from industry patterns?**
   - What we know: DS01 has some intentional deviations (fail-open GPU allocator vs typical fail-closed HPC job submission).
   - What's unclear: Verdict category for "different but justified" - is this PASS or PASS-WITH-NOTES?
   - Recommendation: PASS-WITH-NOTES with rationale. Distinguishes from problems (FAIL) while capturing design choice.

5. **Question: Should dead code removal happen in audit phase or dedicated refactoring phase?**
   - What we know: Dead code removal is low-risk but time-consuming. Audit phase already busy with validation + critical fixes.
   - What's unclear: Whether to defer all dead code removal or handle confirmed-dead cases immediately.
   - Recommendation: Remove confirmed dead code in audit (quick wins). Defer refactoring of live-but-redundant code to Phase 3.3.

## Sources

### Primary (HIGH confidence)
- [SLURM cgroup v2 support](https://slurm.schedmd.com/cgroup_v2.html) - Official SLURM documentation on resource isolation
- [Kubernetes GPU multi-tenancy](https://www.vcluster.com/blog/gpu-multitenancy-kubernetes-strategies) - K8s GPU sharing patterns
- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html) - Official GPU container allocation
- [systemd resource control](https://docs.kernel.org/admin-guide/cgroup-v2.html) - Linux cgroup v2 and systemd integration
- [ShellCheck documentation](https://www.shellcheck.net/) - Static analysis tool for bash
- [Phase 2.1 GPU Access Control Research](/opt/ds01-infra/.planning/phases/02.1-gpu-access-control-research/02.1-RESEARCH.md) - Already validated GPU architecture

### Secondary (MEDIUM confidence)
- [Code audit best practices](https://www.stxnext.com/blog/how-to-audit-the-quality-of-your-python-code) - Infrastructure code audit methodology
- [Python code analysis tools 2026](https://www.jit.io/resources/appsec-tools/top-python-code-analysis-tools-to-improve-code-quality) - Current tooling landscape
- [Structured logging best practices](https://www.joyfulprogramming.com/p/structured-logging-best-practices) - JSON logging patterns
- [Google Cloud Deployment Manager templates](https://cloud.google.com/deployment-manager/docs/configuration/templates/create-basic-template) - Template-based config generation
- [Architecture Decision Records](https://learn.microsoft.com/en-us/azure/well-architected/architect-role/architecture-decision-record) - ADR validation practices
- [Fail-open vs fail-closed patterns](https://authzed.com/blog/fail-open) - Error handling design

### Tertiary (LOW confidence)
- [Code quality tools 2026](https://www.aikido.dev/blog/code-quality-tools) - General survey, not infrastructure-specific
- [IaC quality framework](https://arxiv.org/abs/2502.03127) - Very new (Feb 2025), not yet validated
- [SLURM vs Kubernetes comparison](https://www.whitefiber.com/blog/slurm-vs-kubernetes) - Third-party analysis, not official docs

## Metadata

**Confidence breakdown:**
- Architecture validation approach: **HIGH** - Phase 2.1 established pattern, SLURM/K8s docs authoritative
- Code audit methodology: **MEDIUM** - Tooling well-established (ShellCheck, vulture), severity model standard, but infrastructure audit guidance less common than application code
- Config consolidation pattern: **MEDIUM** - Template-based generation is standard, but specific hierarchy (deploy/runtime/state) is DS01 design
- Planning document audit: **HIGH** - ADR best practices well-documented, executability concept straightforward

**Research date:** 2026-02-05
**Valid until:** 2026-08-05 (6 months - stable domain, tooling evolves slowly)

**Critical follow-ups:**
- Run ShellCheck on all bash scripts before audit to identify mechanical issues
- Confirm vulture min-confidence threshold (80 recommended, may need tuning)
- Verify Phase 2.1 research covers all GPU access control validation (reuse findings)
- Check if any Phases 1-3.1 plans missing or incomplete (planning document audit prerequisite)
