---
phase: 03.2-architecture-audit-code-quality
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/03.2-architecture-audit-code-quality/03.2-AUDIT-REPORT.md
autonomous: true

must_haves:
  truths:
    - "Every architectural decision from Phases 1-3.1 has a pass/fail/pass-with-notes verdict against SLURM/K8s/HPC patterns"
    - "All Critical and High severity code issues are identified with proposed fixes"
    - "Planning documents (ROADMAP.md, STATE.md, PLAN.md files) assessed for accuracy and executability"
    - "Structured backlog of deferred items (Medium/Low) produced with severity and suggested phase"
  artifacts:
    - path: ".planning/phases/03.2-architecture-audit-code-quality/03.2-AUDIT-REPORT.md"
      provides: "Comprehensive audit findings: architecture verdicts, code issues, planning quality, deferred backlog"
      contains: "PASS\\|FAIL\\|PASS-WITH-NOTES"
  key_links:
    - from: "03.2-AUDIT-REPORT.md"
      to: "03.2-02-PLAN.md, 03.2-03-PLAN.md"
      via: "Audit findings drive remediation plans"
      pattern: "CRITICAL|HIGH"
---

<objective>
Comprehensive audit of Phases 1-3.1 across three dimensions: (1) architecture validation against SLURM/K8s/HPC industry standards, (2) code quality review with severity-tiered findings, (3) planning document accuracy check. Produces a single audit report that drives Plans 02-04.

Purpose: Validate that the DS01 infrastructure is built on sound architectural foundations before adding Phase 4 resource enforcement. Architecture validation is the headline deliverable; code quality is secondary.

Output: `.planning/phases/03.2-architecture-audit-code-quality/03.2-AUDIT-REPORT.md` containing architecture verdicts, code issues by severity, planning quality scores, and deferred backlog.
</objective>

<execution_context>
@/home/datasciencelab/.claude/get-shit-done/workflows/execute-plan.md
@/home/datasciencelab/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03.2-architecture-audit-code-quality/03.2-CONTEXT.md
@.planning/phases/03.2-architecture-audit-code-quality/03.2-RESEARCH.md
@.planning/phases/02.1-gpu-access-control-research/02.1-RESEARCH.md
@.planning/phases/02.1-gpu-access-control-research/02.1-DESIGN.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Architecture Validation Against Industry Standards</name>
  <files>.planning/phases/03.2-architecture-audit-code-quality/03.2-AUDIT-REPORT.md (started)</files>
  <action>
Validate every major architectural decision from Phases 1-3.1 against how SLURM, Kubernetes, and HPC clusters solve the same problem. Use the validation template from 03.2-RESEARCH.md (DS01 Approach / Industry Pattern / Verdict / Evidence / Notes).

**7 subsystems to validate:**

1. **Event logging** (Phase 1): Structured JSON, append-only, never-block pattern, Bash-via-CLI bridge, copytruncate logrotate
   - Compare: SLURM syslog/accounting, K8s audit webhooks, HPC append-only audit logs
   - Check: Is JSONL + never-block pattern aligned with industry practice?

2. **Workload detection** (Phase 2): Polling-based scanner, transient filtering (2-scan threshold), container classification, host GPU process detection
   - Compare: K8s kubelet watch events, SLURM prolog/epilog hooks, systemd unit tracking
   - Check: Is 30s polling interval acceptable? Is transient filtering standard?

3. **Container isolation** (Phase 3): Docker wrapper authorization, user filtering via labels, ownership verification, fail-open for unowned containers
   - Compare: K8s admission controllers, SLURM job ownership, Docker authorization plugins
   - Check: Is wrapper-based auth a valid alternative to Docker authorization plugins?

4. **GPU access control** (Phases 2.1, 3, 3.1): Three-layer architecture (CUDA_VISIBLE_DEVICES deterrent + Docker device mapping security + video group exemption)
   - Compare: SLURM GRES + cgroup devices, K8s device plugins, bare-metal GPU clusters
   - Apply layer elimination challenge from research: can any layer be removed?
   - Reuse Phase 2.1 research conclusions but verify implementation matches

5. **Deployment pipeline** (Phase 3.1): Self-bootstrap re-exec, deterministic permissions manifest, idempotent deploy.sh
   - Compare: Ansible idempotency, Puppet convergent state, SLURM node configuration
   - Check: Is self-bootstrap re-exec pattern safe? Is permissions manifest comprehensive?

6. **GPU allocation** (Phases 1, 3.1): Stateless file-locking allocator, fail-open exception handling, MIG slot management
   - Compare: SLURM GRES scheduling, K8s device plugin allocation, NVIDIA Container Toolkit patterns
   - Check: Is file-locking race-safe? Is fail-open correct for GPU allocation?

7. **Config management** (current state): deploy/ + etc-mirrors/ + runtime configs + state dirs
   - Compare: Ansible roles, Puppet modules, SLURM configless mode
   - Check: Is current structure adequate or does it need consolidation? (feeds Plan 03)

**Also include:**
- Security check: CVE-2025-23266 (NVIDIA Container Toolkit privilege escalation) -- verify nvidia-ctk version or config.toml workaround
- Error handling philosophy check: Review fail-open vs fail-closed application per subsystem (user ops fail-open, system ops fail-closed, logging best-effort)
- Research alignment: Verify Phases 1-3.1 implementation matches Phase 2.1 research conclusions

**Verdict scale:**
- PASS: Aligned with industry patterns, no concerns
- PASS-WITH-NOTES: Different approach but justified, or minor improvement possible
- FAIL: Violates industry patterns, creates real risk, needs remediation
- IMPROVE: Current approach works but a superior alternative exists in industry practice — document what it is, why it's better, and what migration would involve

**CRITICAL: Do not just validate — actively search for better approaches.** For each subsystem, after comparing DS01's approach to SLURM/K8s/HPC patterns, explicitly ask: "Is there a superior architecture for this problem that DS01 should adopt?" If yes, document it as an IMPROVE verdict with a concrete migration path. The goal is not to rubber-stamp existing decisions but to find genuinely better alternatives where they exist.

Every verdict MUST cite specific evidence (URL to SLURM/K8s docs, HPC reference, or Phase 2.1 research).
  </action>
  <verify>
The audit report section on architecture validation contains:
- 7 subsystem validations with DS01 approach, industry pattern comparison, and verdict
- Every verdict has at least one evidence citation
- For each subsystem: explicit statement on whether a superior alternative exists (even if answer is "no, current approach is optimal")
- CVE-2025-23266 security check completed
- Error handling philosophy validated per subsystem
- Research alignment check completed (Phase 2.1 conclusions vs implementation)
  </verify>
  <done>All 7 subsystems have clear PASS/FAIL/PASS-WITH-NOTES/IMPROVE verdicts with industry evidence. Superior alternatives documented where they exist. No unsubstantiated claims.</done>
</task>

<task type="auto">
  <name>Task 2: Code Quality Audit + Planning Document Review</name>
  <files>.planning/phases/03.2-architecture-audit-code-quality/03.2-AUDIT-REPORT.md (continued)</files>
  <action>
**Part A: Code Quality Audit**

Review all critical and high-criticality scripts from Phases 1-3.1. Use severity tiers from research (Critical/High/Medium/Low).

**CRITICAL scripts (deep review):**
- `scripts/docker/gpu_allocator_v2.py` -- file locking, MIG detection, .members loading, fail-open exceptions
- `scripts/docker/docker-wrapper.sh` (or `mlc-create-wrapper.sh`) -- isolation logic, user filtering, error handling
- `scripts/system/deploy.sh` -- permissions manifest, self-bootstrap, profile.d deployment
- `scripts/docker/mlc-patched.py` -- container creation patching

**HIGH scripts (focused review):**
- `scripts/lib/ds01_events.py` -- event logging, never-block pattern, group permissions
- `scripts/lib/ds01_events.sh` -- Bash bridge
- `scripts/monitoring/detect-workloads.py` -- scanner logic, transient filtering
- `scripts/docker/gpu-availability-checker.py` / `gpu_availability_checker.py` -- GPU detection, MIG handling
- `config/deploy/profile.d/` scripts -- exemption logic, CUDA_VISIBLE_DEVICES
- `config/resource-limits.yaml` -- config structure, validation

**Per-script checklist:**
1. Error handling: fail-open or fail-closed as appropriate?
2. Race conditions: file locking correct? Atomic operations?
3. Dead code: unused functions, variables, imports?
4. Logic correctness: edge cases handled? Assumptions documented?
5. Implicit assumptions: group existence, command availability, path existence?
6. Format fragility: parsing relies on specific output format?

**Severity classification for each finding:**
- CRITICAL: Security issue, data loss, system unavailability
- HIGH: Reliability issue, incorrect behaviour, significant UX problem
- MEDIUM: Maintainability, code smell, minor UX
- LOW: Style, documentation gap, minor optimisation

Run ShellCheck on critical bash scripts if available (`shellcheck scripts/docker/docker-wrapper.sh scripts/system/deploy.sh`). Run ruff check on Python files. Use findings to supplement manual review.

**Part B: Planning Document Quality Audit**

Review all PLAN.md files from Phases 1-3.1 (approximately 17 plans). For each plan:
- Can someone else execute this without questions? (clarity score 1-5)
- Are acceptance criteria testable?
- Are dependencies and file paths explicit?

Also check STATE.md and ROADMAP.md:
- Does current position match reality?
- Are completed phases correctly marked?
- Are pending todos accurate?
- Are decisions list up-to-date?

Research baseline from context: Independent review found 13/18 plans fully clear, 5/18 with minor gaps. Focus on known gap areas: credential placeholder ambiguity (01-03), subjective verification steps (02.1-02), outdated plans (03-03), misleading titles (03.1-03).

**Output:** Append to audit report:
- Code findings table: severity, file, issue, proposed fix, effort
- Dead code list
- Planning quality scores per plan
- STATE.md/ROADMAP.md accuracy assessment
  </action>
  <verify>
Audit report contains:
- Code findings with severity classification for all reviewed scripts
- At least Critical and High findings listed with proposed fixes
- Dead code identified (if any)
- Planning quality scores for all reviewed PLAN.md files
- STATE.md/ROADMAP.md accuracy assessment
- ShellCheck / ruff output referenced where run
  </verify>
  <done>Code audit identifies all Critical+High issues with proposed fixes. Planning docs assessed. Medium+Low issues catalogued in deferred backlog section. Single comprehensive audit report ready for Plans 02-04 to consume.</done>
</task>

</tasks>

<verification>
- [ ] Audit report exists at `.planning/phases/03.2-architecture-audit-code-quality/03.2-AUDIT-REPORT.md`
- [ ] 7 architecture subsystems have verdicts with evidence
- [ ] CVE-2025-23266 security check documented
- [ ] Code findings categorised by severity (Critical/High/Medium/Low)
- [ ] All Critical+High issues have proposed fixes with effort estimates
- [ ] Deferred backlog (Medium/Low) produced with suggested phase
- [ ] Planning document quality scores assigned
- [ ] STATE.md/ROADMAP.md accuracy verified
</verification>

<success_criteria>
1. Architecture validation: 7 subsystems validated with pass/fail/pass-with-notes/improve verdicts, every verdict evidence-backed, superior alternatives documented where they exist
2. Code quality: All Critical and High issues identified with actionable fix descriptions
3. Planning quality: All PLAN.md files scored, known gap areas confirmed or resolved
4. Deferred backlog: Structured list of Medium+Low items with severity and suggested phase
5. Single audit report that Plans 02, 03, 04 can consume directly
</success_criteria>

<output>
After completion, create `.planning/phases/03.2-architecture-audit-code-quality/03.2-01-SUMMARY.md`
</output>
