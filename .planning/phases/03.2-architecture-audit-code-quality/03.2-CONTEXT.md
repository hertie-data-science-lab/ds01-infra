# Phase 3.2: Architecture Audit & Code Quality - Context

**Gathered:** 2026-02-05
**Status:** Ready for planning

<domain>
## Phase Boundary

Full audit of Phases 1–3.1 work across three interconnected workstreams:

1. **Architecture Validation** (primary) — Validate all design decisions against SLURM/K8s/HPC industry standards. Pass/fail verdicts. This is the headline deliverable.
2. **Code Quality & Refactoring** (secondary) — Dead code removal, error handling improvements, logic simplification, Occam's Razor applied throughout.
3. **Config Consolidation** — Merge `config/deploy/` and `config/etc-mirrors/` into single SSOT. Organise by lifecycle. Implement generative config patterns (templates + variables at deploy time).

This phase does NOT add user-facing features or new capabilities. It prepares the system for Phase 4 (Resource Enforcement) with a solid, audited foundation.

</domain>

<decisions>
## Implementation Decisions

### Audit scope & depth
- Full sweep — every script/module from Phases 1–3.1 gets reviewed
- Full stack — scripts, config files (YAML, systemd units, logrotate, profile.d), and deployment pipeline
- Adaptive depth per module — Claude adjusts based on script criticality (deeper for GPU allocator, lighter for simple query tools)
- Severity-based prioritisation: Critical (security/data loss) > High (reliability) > Medium (maintainability) > Low (style) — fix critical+high, catalogue rest
- Planning document quality audit — check ROADMAP.md, STATE.md still reflect reality; verify PLAN.md files are clear enough for someone else to execute without questions
- Research alignment check — verify Phases 1–3.1 implementation aligns with Phase 2.1 research conclusions; identify where we drifted and whether drift is justified
- Clean up orphaned debug/research files — archive or remove
- Flag cross-script inconsistencies (error handling patterns, logging approach, config loading)
- No preconceptions about what needs attention — fresh eyes, let audit reveal

### Validation standards
- Validate against SLURM, Kubernetes, and HPC best practices specifically
- Pass/fail verdict per architectural decision — clear, actionable
- Build on Phase 2.1 research conclusions (GPU access control already validated) — extend validation to areas it didn't cover (event logging, workload detection, container isolation, deployment patterns)
- Explicitly validate DS01 against how SLURM, Kubernetes, and bare-metal GPU clusters handle: host vs container GPU access control, multi-user isolation, resource enforcement patterns, audit trail / event logging
- Challenge the three-layer GPU access architecture (CUDA_VISIBLE_DEVICES + Docker device mapping + video group exemption) — is it the simplest/strongest approach? Be willing to recommend simplification if HPC/K8s patterns suggest better
- If validation reveals real problems (not just different approach), fix in this phase

### Error handling philosophy
- Tiered approach: fail-open for user operations (container create always succeeds), fail-closed for system operations (deploy.sh and system scripts fail loudly if state is wrong)
- Review all scripts against this philosophy and flag violations

### Refactoring threshold
- **Architecture & approach validation is the primary concern** — code quality is secondary
- Remove all confirmed dead code — version control has the history. Failed approaches documented in commit messages, not left as zombie code
- Comprehensive cleanup of logic, error handling, structure, naming
- Python: add type hints to public function signatures, brief docstrings to non-obvious functions
- Apply ruff formatting across all reviewed Python files (per ~/.claude/rules/python-standards.md)
- Consolidate duplicate logic into shared utilities in `scripts/lib/` when 3+ scripts do the same thing
- Interfaces can change if all callers updated in the same atomic commit
- Stubs/incomplete implementations: catalogue only, don't implement

### Config consolidation strategy
- **Target:** Merge `config/deploy/` and `config/etc-mirrors/` into single SSOT
- **Organisation:** Hierarchical by lifecycle:
  - `config/deploy/` — Install-time configs (systemd services, profile.d scripts, initial state dirs)
  - `config/runtime/` — Per-operation configs (resource limits, exemption rules, wrapper settings)
  - `config/state/` — Persistent data directories (grants, rate limits, state manifests)
- **Pattern:** Generative (templates + variables at deploy time) rather than declarative
  - Reduces duplication: user lists, paths, environment-specific settings filled in at deploy time
  - Enables per-environment config without multiple copies
  - Single source in git, deployed configs are derived artifacts
- Config structure (deploy/runtime/state) is the target — ensure all scripts use it correctly after migration

### Output & deliverables
- **Report first, then code** — produce audit findings for review, execute fixes as separate step after approval
- **Atomic commits** — each fix gets its own commit with clear message; clean history of what was cleaned up
- Update existing architecture documentation (CLAUDE.md files, subsystem docs) to reflect post-audit state
- Produce structured backlog of deferred items: categorised with severity, effort estimate, and suggested phase
- Sync STATE.md and ROADMAP.md to reflect audit results

### Claude's Discretion
- Exact depth of review per individual script (based on criticality assessment)
- Which industry references are most relevant per subsystem
- Whether a specific inconsistency is worth consolidating vs leaving as-is
- Grouping and ordering of audit findings
- Exact refactor approach per file — which functions to extract, how to restructure complex logic
- Config variable naming & template system design
- Audit report structure and format

</decisions>

<specifics>
## Specific Ideas

- "I'm less interested in quality of code, more in the architecture & approaches — these need to be carefully validated with research / industry best" — architecture validation is the headline, refactoring is supporting work
- Phase 2.1 research (GPU access control design doc) already covers GPU subsystem — extend the same rigour to event logging, workload detection, container isolation, and deployment patterns
- Code style consistency: apply ruff formatting and Python standards across all reviewed files

</specifics>

<process>
## Validation Process: Haiku Branch Reference

A previous execution of this phase exists on the `haiku-3.2-work` branch (completed by Haiku model). This serves as a **validation resource only**, used with the following strict protocol:

1. **Do all work blind first** — research, write, plan independently without consulting the Haiku branch
2. **Then validate** — after completing each artefact (context, research, plans, implementations), check the corresponding Haiku branch artefact as a second opinion / comprehensiveness check
3. **Critically assess** — the Haiku work may contain useful ideas we missed, or it may be lower quality. Incorporate genuinely good ideas, ignore the rest
4. **Never let it lead** — the Haiku branch is a validation check, not a template. Our work stands on its own

This applies at every stage: context > research > plans > implementation.

</process>

<deferred>
## Deferred Ideas

None — discussion stayed within phase scope

</deferred>

---

*Phase: 03.2-architecture-audit-code-quality*
*Context gathered: 2026-02-05*
*Updated: 2026-02-05 (consolidated with haiku-3.2-work context)*
